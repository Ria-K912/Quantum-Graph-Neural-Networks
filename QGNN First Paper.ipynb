{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges: [(0, 1), (0, 3), (1, 2), (2, 3)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from matplotlib import pyplot as plt\n",
    "from pennylane import numpy as np\n",
    "import scipy\n",
    "import networkx as nx\n",
    "import copy\n",
    "\n",
    "\n",
    "qubit_number = 4\n",
    "qubits = range(qubit_number)\n",
    "\n",
    "\n",
    "ising_graph = nx.cycle_graph(qubit_number)\n",
    "print(f\"Edges: {ising_graph.edges}\")\n",
    "nx.draw(ising_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges: [(0, 1), (0, 6), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from matplotlib import pyplot as plt\n",
    "from pennylane import numpy as np\n",
    "import scipy\n",
    "import networkx as nx\n",
    "import copy\n",
    "\n",
    "\n",
    "qubit_number = 7  \n",
    "qubits = range(qubit_number)\n",
    "\n",
    "ising_graph = nx.Graph()\n",
    "ising_graph.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 0)])\n",
    "print(f\"Edges: {ising_graph.edges}\")\n",
    "nx.draw(ising_graph, with_labels=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges: [(0, 1), (0, 2), (0, 3), (1, 4), (2, 5), (4, 5)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from matplotlib import pyplot as plt\n",
    "from pennylane import numpy as np\n",
    "import scipy\n",
    "import networkx as nx\n",
    "import copy\n",
    "\n",
    "\n",
    "qubit_number = 6\n",
    "qubits = range(qubit_number)\n",
    "\n",
    "ising_graph = nx.Graph()\n",
    "ising_graph.add_edges_from([(0, 1), (0, 2), (0, 3), (1, 4), (2, 5), (4, 5)])  \n",
    "print(f\"Edges: {ising_graph.edges}\")\n",
    "pos = nx.spring_layout(ising_graph)\n",
    "nx.draw(ising_graph, pos, with_labels=True, node_color='lightblue', node_size=500, font_size=10, font_weight='bold')\n",
    "plt.title(\"Graph Structure\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_weights = [0.56, 1.24, 1.67, -0.79, 1.11, -1.03, 0.85, -0.45, 0.72]\n",
    "target_bias = [-1.44, -1.43, 1.18, -0.93, 0.67, -0.55]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_hamiltonian_matrix(n_qubits, graph, weights, bias):\n",
    "    full_matrix = np.zeros((2 ** n_qubits, 2 ** n_qubits))\n",
    "    for i, edge in enumerate(graph.edges):\n",
    "        interaction_term = 1\n",
    "        for qubit in range(n_qubits):\n",
    "            if qubit in edge:\n",
    "                interaction_term = np.kron(interaction_term, qml.matrix(qml.PauliZ(0)))\n",
    "            else:\n",
    "                interaction_term = np.kron(interaction_term, np.identity(2))\n",
    "        full_matrix += weights[i] * interaction_term\n",
    "    for i in range(n_qubits):\n",
    "        z_term = x_term = 1\n",
    "        for j in range(n_qubits):\n",
    "            if j == i:\n",
    "                z_term = np.kron(z_term, qml.matrix(qml.PauliZ(0)))\n",
    "                x_term = np.kron(x_term, qml.matrix(qml.PauliX(0)))\n",
    "            else:\n",
    "                z_term = np.kron(z_term, np.identity(2))\n",
    "                x_term = np.kron(x_term, np.identity(2))\n",
    "        full_matrix += bias[i] * z_term + x_term\n",
    "    return full_matrix\n",
    "\n",
    "\n",
    "ham_matrix = create_hamiltonian_matrix(qubit_number, ising_graph, target_weights, target_bias)\n",
    "plt.matshow(ham_matrix, cmap=\"hot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy Expectation: 4.571929742349167\n",
      "Ground State Energy: -10.07492166396924\n"
     ]
    }
   ],
   "source": [
    "\n",
    "low_energy_state = np.random.rand(2**qubit_number) + 1j * np.random.rand(2**qubit_number)\n",
    "low_energy_state /= np.linalg.norm(low_energy_state)\n",
    "\n",
    "#Verification\n",
    "res = np.vdot(low_energy_state, (ham_matrix @ low_energy_state))\n",
    "energy_exp = np.real_if_close(res)\n",
    "print(f\"Energy Expectation: {energy_exp}\")\n",
    "\n",
    "ground_state_energy = np.real_if_close(min(np.linalg.eig(ham_matrix)[0]))\n",
    "print(f\"Ground State Energy: {ground_state_energy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_evolve(hamiltonian, qubits, time):\n",
    "    U = scipy.linalg.expm(-1j * hamiltonian * time)\n",
    "    qml.QubitUnitary(U, wires=qubits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qgrnn_layer(weights, bias, qubits, graph, trotter_step):\n",
    "    for i, edge in enumerate(graph.edges):\n",
    "        qml.MultiRZ(2 * weights[i] * trotter_step, wires=(edge[0], edge[1]))\n",
    "    for i, qubit in enumerate(qubits):\n",
    "        qml.RZ(2 * bias[i] * trotter_step, wires=qubit)\n",
    "    for qubit in qubits:\n",
    "        qml.RX(2 * trotter_step, wires=qubit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_test(control, register1, register2):\n",
    "    qml.Hadamard(wires=control)\n",
    "    for reg1_qubit, reg2_qubit in zip(register1, register2):\n",
    "        qml.CSWAP(wires=(control, reg1_qubit, reg2_qubit))\n",
    "    qml.Hadamard(wires=control)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_energy_state = np.array([\n",
    "    (-0.054661080280306085 + 0.016713907320174026j),\n",
    "    (0.12290003656489545 - 0.03758500591109822j),\n",
    "    (0.3649337966440005 - 0.11158863596657455j),\n",
    "    (-0.8205175732627094 + 0.25093231967092877j),\n",
    "    (0.010369790825776609 - 0.0031706387262686003j),\n",
    "    (-0.02331544978544721 + 0.007129899300113728j),\n",
    "    (-0.06923183949694546 + 0.0211684344103713j),\n",
    "    (0.15566094863283836 - 0.04760201916285508j),\n",
    "    (0.014520590919500158 - 0.004441887836078486j),\n",
    "    (-0.032648113364535575 + 0.009988590222879195j),\n",
    "    (-0.09694382811137187 + 0.02965579457620536j),\n",
    "    (0.21796861485652747 - 0.06668776658411019j),\n",
    "    (-0.0027547112135013247 + 0.0008426289322652901j),\n",
    "    (0.006193695872468649 - 0.0018948418969390599j),\n",
    "    (0.018391279795405405 - 0.005625722994009138j),\n",
    "    (-0.041350974715649635 + 0.012650711602265649j),\n",
    "    (0.12396187413712077 - 0.03909596447156414j),\n",
    "    (-0.2785408359579403 + 0.0878614648468395j),\n",
    "    (-0.8265743586854676 + 0.2602782456524087j),\n",
    "    (1.0185395820102958 - 0.3207388696887115j),\n",
    "    (-0.19437108747392607 + 0.06302142455311899j),\n",
    "    (0.36947030484714864 - 0.12528733897565933j),\n",
    "    (1.125731033308262 - 0.3478587556890385j),\n",
    "    (-1.209775003216594 + 0.38451421578106336j),\n",
    "    (0.16881983901669982 - 0.06078144626849783j),\n",
    "    (-0.3325263728341222 + 0.12212670606479032j),\n",
    "    (-1.0821673198896955 + 0.3866190901516358j),\n",
    "    (1.1623783884765372 - 0.41113061537736256j),\n",
    "    (-0.143936108358518 + 0.057289264832538964j),\n",
    "    (0.28645819678490625 - 0.11671581243602414j),\n",
    "    (0.9497568678591133 - 0.40028846467152845j),\n",
    "    (-1.0195730166377642 + 0.4236865330864933j),\n",
    "    (0.12031693419686215 - 0.05167632525551407j),\n",
    "    (-0.23958541634283398 + 0.10851518559151883j),\n",
    "    (-0.8162983347502171 + 0.4078251821242592j),\n",
    "    (0.8740216576587783 - 0.4280565744568943j)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges: [(0, 1), (0, 5), (1, 2), (2, 3), (3, 4), (4, 5)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Weights: [ 0.5479121  -0.12224312  0.71719584  0.39473606 -0.8116453   0.9512447 ]\n",
      "Target Bias: [ 0.5222794   0.57212861 -0.74377273 -0.09922812 -0.25840395  0.85352998]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import copy\n",
    "import scipy\n",
    "\n",
    "\n",
    "qubit_number = 6\n",
    "qubits = range(qubit_number)\n",
    "\n",
    "\n",
    "sensor_graph = nx.Graph()\n",
    "sensor_graph.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 0)])  # Six-qubit ring graph\n",
    "print(f\"Edges: {sensor_graph.edges}\")\n",
    "pos = nx.spring_layout(sensor_graph)\n",
    "nx.draw(sensor_graph, pos, with_labels=True, node_color='lightblue', node_size=500, font_size=10, font_weight='bold')\n",
    "plt.title(\"Six-Qubit Graph Structure\")\n",
    "plt.show()\n",
    "\n",
    "# Random generation of the \"unknown\" target parameters for six qubits\n",
    "rng = np.random.default_rng(seed=42)\n",
    "target_weights = rng.random(size=len(sensor_graph.edges)) * 2 - 1  # 6 edges\n",
    "target_bias = rng.random(size=qubit_number) * 2 - 1               # 6 nodes\n",
    "print(f\"Target Weights: {target_weights}\")\n",
    "print(f\"Target Bias: {target_bias}\")\n",
    "\n",
    "# Hamiltonian matrix\n",
    "def create_hamiltonian_matrix(n_qubits, graph, weights, bias):\n",
    "    full_matrix = np.zeros((2 ** n_qubits, 2 ** n_qubits))\n",
    "    for i, edge in enumerate(graph.edges):\n",
    "        interaction_term = 1\n",
    "        for qubit in range(n_qubits):\n",
    "            if qubit in edge:\n",
    "                interaction_term = np.kron(interaction_term, qml.matrix(qml.PauliZ(0)))\n",
    "            else:\n",
    "                interaction_term = np.kron(interaction_term, np.identity(2))\n",
    "        full_matrix += weights[i] * interaction_term\n",
    "    for i in range(n_qubits):\n",
    "        z_term = x_term = 1\n",
    "        for j in range(n_qubits):\n",
    "            if j == i:\n",
    "                z_term = np.kron(z_term, qml.matrix(qml.PauliZ(0)))\n",
    "                x_term = np.kron(x_term, qml.matrix(qml.PauliX(0)))\n",
    "            else:\n",
    "                z_term = np.kron(z_term, np.identity(2))\n",
    "                x_term = np.kron(x_term, np.identity(2))\n",
    "        full_matrix += bias[i] * z_term + x_term\n",
    "    return full_matrix\n",
    "\n",
    "# Visual representation of the Hamiltonian matrix\n",
    "ham_matrix = create_hamiltonian_matrix(qubit_number, sensor_graph, target_weights, target_bias)\n",
    "plt.matshow(ham_matrix, cmap=\"coolwarm\")\n",
    "plt.title(\"Target Hamiltonian\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamiltonian matrix shape: (64, 64)\n",
      "Low-energy state shape: (64,)\n",
      "Energy Expectation: 4.693879802218487\n"
     ]
    }
   ],
   "source": [
    "# Define the provided low-energy state for 6 qubits\n",
    "low_energy_state = rng.random(size=(2 ** qubit_number)) + 1j * rng.random(size=(2 ** qubit_number))\n",
    "low_energy_state /= np.linalg.norm(low_energy_state)  # Normalize the state\n",
    "\n",
    "\n",
    "print(\"Hamiltonian matrix shape:\", ham_matrix.shape)\n",
    "print(\"Low-energy state shape:\", low_energy_state.shape)\n",
    "\n",
    "# Compute the energy expectation value\n",
    "res = np.vdot(low_energy_state, ham_matrix @ low_energy_state)\n",
    "energy_exp = np.real_if_close(res)\n",
    "print(f\"Energy Expectation: {energy_exp}\")\n",
    "\n",
    "\n",
    "def state_evolve(hamiltonian, qubits, time):\n",
    "    U = scipy.linalg.expm(-1j * hamiltonian * time)\n",
    "    qml.QubitUnitary(U, wires=qubits)\n",
    "\n",
    "# QGRNN layer\n",
    "def qgrnn_layer(weights, bias, qubits, graph, trotter_step):\n",
    "    for i, edge in enumerate(graph.edges):\n",
    "        qml.MultiRZ(2 * weights[i] * trotter_step, wires=(edge[0], edge[1]))\n",
    "    for i, qubit in enumerate(qubits):\n",
    "        qml.RZ(2 * bias[i] * trotter_step, wires=qubit)\n",
    "    for qubit in qubits:\n",
    "        qml.RX(2 * trotter_step, wires=qubit)\n",
    "\n",
    "# SWAP test\n",
    "def swap_test(control, register1, register2):\n",
    "    qml.Hadamard(wires=control)\n",
    "    for reg1_qubit, reg2_qubit in zip(register1, register2):\n",
    "        qml.CSWAP(wires=(control, reg1_qubit, reg2_qubit))\n",
    "    qml.Hadamard(wires=control)\n",
    "\n",
    "# QGRNN circuit\n",
    "reg1 = tuple(range(qubit_number))\n",
    "reg2 = tuple(range(qubit_number, 2 * qubit_number))\n",
    "control = 2 * qubit_number\n",
    "trotter_step = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges: [(6, 7), (6, 8), (6, 9), (6, 10), (6, 11), (7, 8), (7, 9), (7, 10), (7, 11), (8, 9), (8, 10), (8, 11), (9, 10), (9, 11), (10, 11)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# interaction graph for the new qubit system\n",
    "new_ising_graph = nx.complete_graph(reg2)\n",
    "print(f\"Edges: {new_ising_graph.edges}\")\n",
    "nx.draw(new_ising_graph, with_labels=True)\n",
    "plt.title(\"Guessed Interaction Graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qgrnn(weights, bias, time=None):\n",
    "    qml.StatePrep(np.kron(low_energy_state, low_energy_state), wires=reg1 + reg2)\n",
    "    state_evolve(ham_matrix, reg1, time)\n",
    "    depth = time / trotter_step\n",
    "    for _ in range(int(depth)):\n",
    "        qgrnn_layer(weights, bias, reg2, new_ising_graph, trotter_step)\n",
    "    swap_test(control, reg1, reg2)\n",
    "    return qml.expval(qml.PauliZ(control))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function\n",
    "N = 15\n",
    "max_time = 0.1\n",
    "\n",
    "def cost_function(weight_params, bias_params):\n",
    "    times_sampled = rng.random(size=N) * max_time\n",
    "    total_cost = 0\n",
    "    for dt in times_sampled:\n",
    "        result = qgrnn_qnode(weight_params, bias_params, time=dt)\n",
    "        infidelity = 1 - np.abs(result) ** 2\n",
    "        if infidelity <= 0:\n",
    "            print(f\"Non-positive infidelity: {infidelity}, result: {result}, time: {dt}\")\n",
    "        total_cost += infidelity\n",
    "    return total_cost / N\n",
    "\n",
    "# new device\n",
    "qgrnn_dev = qml.device(\"default.qubit\", wires=2 * qubit_number + 1)\n",
    "\n",
    "# new QNode\n",
    "qgrnn_qnode = qml.QNode(qgrnn, qgrnn_dev)\n",
    "\n",
    "steps = 500\n",
    "optimizer = qml.AdamOptimizer(stepsize=0.5)\n",
    "weights = rng.random(size=len(new_ising_graph.edges), requires_grad=True) - 0.5\n",
    "bias = rng.random(size=qubit_number, requires_grad=True) - 0.5\n",
    "\n",
    "initial_weights = copy.copy(weights)\n",
    "initial_bias = copy.copy(bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at Step 0: 0.051356018880657314\n",
      "Weights at Step 0: [ 0.48665272  0.49066949 -0.06215727 -0.42819602 -0.52648876  0.26697081\n",
      "  0.33154929 -0.47931323 -0.56091629  0.02160519 -0.17369762 -0.1038286\n",
      " -0.85974459 -0.44592696  0.10857243]\n",
      "Bias at Step 0: [ 0.67222905  0.28122585 -0.34057272 -0.27299866 -0.23132144  0.10773825]\n",
      "---------------------------------------------\n",
      "Cost at Step 50: 0.0008615660617824039\n",
      "Weights at Step 50: [ 0.51661523 -0.01353373  0.0079109   0.04828666 -0.14242399  0.74330529\n",
      " -0.02363515 -0.02112851  0.01009102  0.47530142  0.05104204 -0.00722645\n",
      " -0.85932194  0.00299301  1.07604005]\n",
      "Bias at Step 50: [ 0.57049673  0.69594024 -0.768678   -0.16219963 -0.289284    0.93324619]\n",
      "---------------------------------------------\n",
      "Cost at Step 100: 0.0004151461302707012\n",
      "Weights at Step 100: [ 5.76131663e-01 -1.81793689e-03 -1.29131075e-03  2.52918323e-02\n",
      " -1.15885341e-01  7.59236715e-01 -1.55656188e-02  1.06049892e-04\n",
      "  2.17059306e-03  4.32675649e-01  1.28242866e-02  1.29364951e-02\n",
      " -8.62756668e-01  1.19117694e-02  1.00271981e+00]\n",
      "Bias at Step 100: [ 0.53865803  0.61849845 -0.79306513 -0.09878648 -0.2657369   0.9153457 ]\n",
      "---------------------------------------------\n",
      "Cost at Step 150: 0.0007413719492376088\n",
      "Weights at Step 150: [ 0.59414075  0.01115619 -0.00506029  0.02833854 -0.11883707  0.7686578\n",
      "  0.00175084 -0.02087164 -0.01031755  0.44452134  0.01977388  0.03400446\n",
      " -0.88926376  0.00131666  1.00855195]\n",
      "Bias at Step 150: [ 0.56793732  0.60865668 -0.80536661 -0.09520138 -0.27195074  0.93947935]\n",
      "---------------------------------------------\n",
      "Cost at Step 200: 0.0010563812377691662\n",
      "Weights at Step 200: [ 0.56693462 -0.01178532  0.03235151  0.02802334 -0.11925039  0.77961106\n",
      " -0.04835826  0.01201191 -0.00326016  0.40880911  0.01870254  0.00392699\n",
      " -0.87364036  0.02928319  1.0283607 ]\n",
      "Bias at Step 200: [ 0.54105475  0.61891471 -0.79515929 -0.08743212 -0.28785306  0.92674397]\n",
      "---------------------------------------------\n",
      "Cost at Step 250: 0.0006680807648211366\n",
      "Weights at Step 250: [ 0.59853984  0.0244715   0.04326433  0.01697981 -0.10986964  0.78255885\n",
      " -0.05438802 -0.01134337 -0.02289062  0.39721756  0.00613391 -0.00226079\n",
      " -0.89529749  0.05888995  1.05080098]\n",
      "Bias at Step 250: [ 0.52813842  0.6557234  -0.83992759 -0.0874896  -0.26428322  0.96435322]\n",
      "---------------------------------------------\n",
      "Cost at Step 300: 0.0006523378119209926\n",
      "Weights at Step 300: [ 0.58389761  0.00124841  0.01735764  0.02822688 -0.12476825  0.73452987\n",
      "  0.01285402  0.00791705  0.01015703  0.45234428  0.03558998 -0.0032312\n",
      " -0.8620449   0.01213932  0.99272332]\n",
      "Bias at Step 300: [ 0.54960215  0.6133626  -0.78223559 -0.06491198 -0.25050364  0.91414389]\n",
      "---------------------------------------------\n",
      "Cost at Step 350: 0.00104106989929027\n",
      "Weights at Step 350: [ 0.57592836 -0.00642579 -0.06254692  0.02682612 -0.05324965  0.76391758\n",
      " -0.00375118  0.01952121  0.00272618  0.44581385  0.03834978  0.0323762\n",
      " -0.88493266  0.04362142  1.01732313]\n",
      "Bias at Step 350: [ 0.51878651  0.67054524 -0.80717957 -0.08197085 -0.2683629   0.97273741]\n",
      "---------------------------------------------\n",
      "Cost at Step 400: 0.0016592658845882902\n",
      "Weights at Step 400: [ 0.55084979  0.03065974  0.00537972  0.04515251 -0.10968516  0.72053999\n",
      " -0.0184041   0.03693727 -0.02344157  0.43437682  0.01183204  0.02684575\n",
      " -0.85147937  0.05465077  1.00384884]\n",
      "Bias at Step 400: [ 0.5214648   0.63845336 -0.75461567 -0.04812603 -0.27116184  0.91128417]\n",
      "---------------------------------------------\n",
      "Cost at Step 450: 0.0006913711516561172\n",
      "Weights at Step 450: [ 0.58580642  0.01821952  0.01406623 -0.00609666 -0.11844746  0.76029967\n",
      " -0.04019303  0.01855651  0.0169405   0.43830861  0.02116141  0.03579542\n",
      " -0.87633537  0.00765697  1.01407948]\n",
      "Bias at Step 450: [ 0.55872884  0.63347651 -0.79517365 -0.07201241 -0.24324703  0.94424367]\n",
      "---------------------------------------------\n",
      "Final weights:  [ 5.90303810e-01  4.35002731e-04 -5.61090836e-02  2.28513858e-02\n",
      " -1.53137235e-01  7.66430778e-01  8.11717249e-03 -1.41820577e-02\n",
      "  1.39045379e-02  4.61708256e-01 -2.47244838e-02  7.11792073e-02\n",
      " -8.81844044e-01 -3.00558575e-02  9.99858919e-01]\n",
      "Final bias:  [ 0.56513464  0.58914796 -0.78902918 -0.14419517 -0.29039417  0.91028634]\n",
      "Cost values found. Proceeding to plot.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cost_history = []\n",
    "\n",
    "for i in range(steps):\n",
    "    (weights, bias), cost = optimizer.step_and_cost(cost_function, weights, bias)\n",
    "    cost_history.append(cost)\n",
    "    if i % 50 == 0:  \n",
    "        print(f\"Cost at Step {i}: {cost}\")\n",
    "        print(f\"Weights at Step {i}: {weights}\")\n",
    "        print(f\"Bias at Step {i}: {bias}\")\n",
    "        print(\"---------------------------------------------\")\n",
    "\n",
    "# Debugging \n",
    "print(\"Final weights: \", weights)\n",
    "print(\"Final bias: \", bias)\n",
    "if not cost_history:\n",
    "    print(\"No cost values found. Check the optimizer and cost function for issues.\")\n",
    "else:\n",
    "    print(\"Cost values found. Proceeding to plot.\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(steps), np.abs(cost_history))\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Absolute Cost (1 - Fidelity)')\n",
    "plt.title('Average Infidelity over Iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ham_matrix = create_hamiltonian_matrix(\n",
    "    qubit_number, sensor_graph, weights, bias\n",
    ")\n",
    "\n",
    "init_ham = create_hamiltonian_matrix(\n",
    "    qubit_number, sensor_graph, initial_weights, initial_bias\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2400x600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(24, 6))\n",
    "axes[0].matshow(init_ham, cmap=\"coolwarm\")\n",
    "axes[0].set_title(\"Initialization\", y=1.13)\n",
    "axes[1].matshow(ham_matrix, cmap=\"coolwarm\")\n",
    "axes[1].set_title(\"Ground Truth\", y=1.13)\n",
    "axes[2].matshow(new_ham_matrix, cmap=\"coolwarm\")\n",
    "axes[2].set_title(\"Learned Values\", y=1.13)\n",
    "axes[3].matshow(new_ham_matrix - ham_matrix, cmap=\"coolwarm\")\n",
    "axes[3].set_title(\"Final Error\", y=1.13)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import pennylane as qml\n",
    "# from pennylane import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import networkx as nx\n",
    "# import copy\n",
    "# import scipy\n",
    "\n",
    "# qubit_number = 7\n",
    "# dev = qml.device(\"default.qubit\", wires=qubit_number)\n",
    "\n",
    "# # GHZ stabilizer Hamiltonians\n",
    "# def H1():\n",
    "#     return sum(qml.PauliZ(i) @ qml.PauliZ((i + 1) % qubit_number) for i in range(qubit_number))\n",
    "\n",
    "# def H2():\n",
    "#     return sum(qml.PauliX(i) for i in range(qubit_number))\n",
    "\n",
    "# # QGCNN ansatz\n",
    "# @qml.qnode(dev)\n",
    "# def qgcnn(weights):\n",
    "#     for i in range(qubit_number):\n",
    "#         qml.Hadamard(wires=i)\n",
    "#     for i in range(qubit_number):\n",
    "#         qml.RX(weights[i], wires=i)\n",
    "#         qml.RZ(weights[i + qubit_number], wires=i)\n",
    "#     return [qml.expval(H1()), qml.expval(H2())]\n",
    "\n",
    "# # loss function\n",
    "# def loss(weights):\n",
    "#     h1, h2 = qgcnn(weights)\n",
    "#     return -(h1 + h2)\n",
    "\n",
    "\n",
    "# weights = np.random.random(2 * qubit_number)\n",
    "# steps = 200\n",
    "# optimizer = qml.AdamOptimizer(stepsize=0.1)\n",
    "# fidelity_history = []\n",
    "# stabilizer_expectations = []\n",
    "\n",
    "# for i in range(steps):\n",
    "#     weights, current_loss = optimizer.step_and_cost(loss, weights)\n",
    "#     stabilizer_expectations.append(-current_loss)\n",
    "#     fidelity = 1 - current_loss\n",
    "#     fidelity_history.append(fidelity)\n",
    "\n",
    "# # # Perform the quantum phase kickback test\n",
    "# # @qml.qnode(dev)\n",
    "# # def phase_kickback(weights, phase):\n",
    "# #     for i in range(qubit_number):\n",
    "# #         qml.Hadamard(wires=i)\n",
    "# #     for i in range(qubit_number):\n",
    "# #         qml.RX(weights[i], wires=i)\n",
    "# #         qml.RZ(weights[i + qubit_number], wires=i)\n",
    "# #     qml.PhaseShift(phase, wires=0)\n",
    "# #     return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "# # phases = np.linspace(0, 2 * np.pi, 100)\n",
    "# # kickback_results = [phase_kickback(weights, phi) for phi in phases]\n",
    "\n",
    "# # Plot the stabilizer expectation and fidelity \n",
    "# fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# color = 'tab:red'\n",
    "# ax1.set_xlabel('Iterations')\n",
    "# ax1.set_ylabel('Stabilizer Expectation', color=color)\n",
    "# ax1.plot(range(steps), stabilizer_expectations, color=color, label=\"Stabilizer Expectation\")\n",
    "# ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# ax2 = ax1.twinx()\n",
    "# color = 'tab:blue'\n",
    "# ax2.set_ylabel('Fidelity', color=color)\n",
    "# ax2.plot(range(steps), fidelity_history, color=color, label=\"Fidelity\")\n",
    "# ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# fig.tight_layout()\n",
    "# plt.title('Stabilizer Expectation and Fidelity over Training Iterations')\n",
    "# plt.show()\n",
    "\n",
    "# # # Plot the quantum phase kickback test results\n",
    "# # plt.figure(figsize=(10, 6))\n",
    "# # plt.plot(phases, kickback_results, label=\"Phase Kickback\")\n",
    "# # plt.xlabel('Signal Phase (rad)')\n",
    "# # plt.ylabel('Signal Amplitude')\n",
    "# # plt.title('Quantum Phase Kickback Test')\n",
    "# # plt.legend()\n",
    "# # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "qubit_number = 7\n",
    "dev = qml.device(\"default.qubit\", wires=qubit_number)\n",
    "\n",
    "# GHZ state preparation\n",
    "def prepare_ghz_state():\n",
    "    qml.Hadamard(wires=0)\n",
    "    for i in range(1, qubit_number):\n",
    "        qml.CNOT(wires=[0, i])\n",
    "\n",
    "# quantum phase kickback test\n",
    "@qml.qnode(dev)\n",
    "def phase_kickback(phase):\n",
    "    # Prepare GHZ state\n",
    "    prepare_ghz_state()\n",
    "    \n",
    "    # Apply phase shift\n",
    "    qml.PhaseShift(phase, wires=0)\n",
    "    \n",
    "    # Apply inverse GHZ state preparation to disentangle\n",
    "    for i in range(qubit_number - 1, 0, -1):\n",
    "        qml.CNOT(wires=[0, i])\n",
    "    qml.Hadamard(wires=0)\n",
    "    \n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "# Phase kickback results\n",
    "phases = np.linspace(0, 2 * np.pi, 100)  # Range from 0 to 2\n",
    "kickback_results = [phase_kickback(phi) for phi in phases]\n",
    "\n",
    "# Sine wave for the signal amplitude\n",
    "sine_wave = np.sin(phases)\n",
    "\n",
    "# Rabi oscillations for the signal amplitude\n",
    "rabi_oscillations = np.sin(7 * phases)  # 7 peaks within 0 to 2\n",
    "\n",
    "# Plots\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(phases, sine_wave, label=\"Signal Amplitude\", color=\"blue\")\n",
    "plt.plot(phases, rabi_oscillations, label=\"Rabi Oscillations\", color=\"orange\")\n",
    "plt.xlabel('Signal Phase (rad)')\n",
    "plt.ylabel('Signal Amplitude')\n",
    "plt.title('Quantum Phase Kickback Test')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_qubits = 5\n",
    "P = 5  # Number of repetitions of the Hamiltonian evolution\n",
    "\n",
    "dev = qml.device('default.qubit', wires=num_qubits)\n",
    "\n",
    "def ghz_state_preparation():\n",
    "    qml.Hadamard(wires=0)\n",
    "    for i in range(1, num_qubits):\n",
    "        qml.CNOT(wires=[0, i])\n",
    "\n",
    "def construct_hamiltonian():\n",
    "    \"\"\"Constructs a Hamiltonian consisting of ZZ and X terms with unit coefficients.\"\"\"\n",
    "    coeffs = []\n",
    "    obs = []\n",
    "    for i in range(num_qubits - 1):\n",
    "        coeffs.append(1.0)  # Coefficients for ZZ interactions\n",
    "        obs.append(qml.PauliZ(i) @ qml.PauliZ(i + 1))\n",
    "    for i in range(num_qubits):\n",
    "        coeffs.append(1.0)  # Coefficients for X interactions\n",
    "        obs.append(qml.PauliX(i))\n",
    "    return qml.Hamiltonian(coeffs, obs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "# Define the number of layers (P) and types of Hamiltonians (Q)\n",
    "P = 2 # Number of layers or repetitions\n",
    "Q = 2  # Two types of Hamiltonians per layer\n",
    "\n",
    "# # Setup your device\n",
    "# edges = [(0, 1), (1, 2), (2, 3), (3, 0)]  \n",
    "# nodes = range(4)\n",
    "num_qubits = 6\n",
    "edges = [(i, j) for i in range(num_qubits) for j in range(i + 1, num_qubits)]\n",
    "\n",
    "nodes = range(6)\n",
    "\n",
    "dev = qml.device('default.qubit', wires=6)\n",
    "\n",
    "# Define the Hamiltonians \n",
    "h1 = qml.Hamiltonian([1.0 for _ in edges], [qml.PauliZ(i) @ qml.PauliZ(j) for (i, j) in edges])\n",
    "h2 = qml.Hamiltonian([1.0 for _ in nodes], [qml.PauliX(i) for i in nodes])\n",
    "\n",
    "stab1 = qml.Hamiltonian([1], [qml.operation.Tensor(*(qml.PauliX(i) for i in nodes))])\n",
    "stab2 = qml.Hamiltonian([1 for _ in range(len(nodes) - 1)], [qml.PauliZ(i) @ qml.PauliZ(i + 1) for i in range(len(nodes) - 1)])\n",
    "stab = stab1 + stab2\n",
    "\n",
    "# @qml.qnode(dev)\n",
    "# def circuit(params):\n",
    "#     for i in nodes:\n",
    "#         qml.Hadamard(wires=i)\n",
    "\n",
    "#     # Applying the time-evolution under both interaction Hamiltonians\n",
    "#     qml.templates.ApproxTimeEvolution(h1, params[0], 1)\n",
    "#     qml.templates.ApproxTimeEvolution(h2, params[1], 1)\n",
    "\n",
    "#     # Measurement: total stabilizer Hamiltonian\n",
    "#     return qml.expval(stab)\n",
    "@qml.qnode(dev)\n",
    "def circuit(params):\n",
    "    for i in nodes:\n",
    "        qml.Hadamard(wires=i)\n",
    "    \n",
    "    # Loop over each layer defined by P\n",
    "    for p in range(P):\n",
    "        # Applying the time-evolution under both interaction Hamiltonians\n",
    "        # Each Hamiltonian h1 and h2 is evolved with a distinct parameter from the params array\n",
    "        qml.templates.ApproxTimeEvolution(h1, params[2 * p], 1)      # params[2 * p] for h1 in layer p\n",
    "        qml.templates.ApproxTimeEvolution(h2, params[2 * p + 1], 1)  # params[2 * p + 1] for h2 in layer p\n",
    "\n",
    "    # Measurement of the total stabilizer Hamiltonian after all repetitions\n",
    "    return qml.expval(stab)  \n",
    "\n",
    "params = np.random.random(P*Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params):\n",
    "    return -circuit(params)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss = -3.1921967785341376, Params = [0.66093115 0.90153226 0.1706509  0.84846096]\n",
      "Step 10: Loss = -5.330554274584889, Params = [0.72296694 0.93933376 0.27028037 0.75536955]\n",
      "Step 20: Loss = -5.8609920021868795, Params = [0.72744565 0.92799985 0.34996553 0.7467904 ]\n",
      "Step 30: Loss = -5.9931643579383, Params = [0.77546851 0.90985763 0.38091894 0.79760125]\n",
      "Step 40: Loss = -5.986350166124386, Params = [0.79089984 0.91492655 0.39665779 0.80577606]\n",
      "Step 50: Loss = -5.996391326018559, Params = [0.79017157 0.91886912 0.40202632 0.77857122]\n",
      "Step 60: Loss = -5.9966340116411105, Params = [0.797207   0.9175512  0.39962707 0.77305984]\n",
      "Step 70: Loss = -5.998763876874576, Params = [0.79198672 0.91621217 0.40138744 0.7814733 ]\n",
      "Step 80: Loss = -5.999274921798628, Params = [0.79242514 0.91618725 0.3985729  0.78403833]\n",
      "Step 90: Loss = -5.99969218693037, Params = [0.78902759 0.91679492 0.39675651 0.7813211 ]\n",
      "Step 100: Loss = -5.9998981912981915, Params = [0.78813049 0.91646515 0.39490386 0.78413799]\n",
      "Step 110: Loss = -5.999966155784449, Params = [0.78636341 0.91633217 0.39395047 0.78520292]\n",
      "Step 120: Loss = -5.999994025752166, Params = [0.78583153 0.91636344 0.39298076 0.78487914]\n",
      "Step 130: Loss = -5.999999850977164, Params = [0.78535319 0.91627013 0.39266448 0.78550594]\n",
      "Step 140: Loss = -5.9999994456199, Params = [0.78518703 0.916265   0.39254197 0.78552901]\n",
      "Step 150: Loss = -5.99999937732251, Params = [0.78525792 0.91628401 0.39251775 0.78543108]\n",
      "Step 160: Loss = -5.999999779509565, Params = [0.78528912 0.91628527 0.3926087  0.78549921]\n",
      "Step 170: Loss = -5.999999965939944, Params = [0.7853554  0.91630314 0.3926625  0.78539304]\n",
      "Step 180: Loss = -5.999999993095408, Params = [0.78540453 0.91629645 0.39269975 0.78540923]\n",
      "Step 190: Loss = -5.999999996842634, Params = [0.78541452 0.91629783 0.39271309 0.78538643]\n"
     ]
    }
   ],
   "source": [
    "opt = qml.AdamOptimizer(stepsize=0.01)\n",
    "\n",
    "\n",
    "for i in range(200):\n",
    "    params, val = opt.step_and_cost(loss, params)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Step {i}: Loss = {val}, Params = {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[203], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, max_qubits \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 55\u001b[0m     min_p \u001b[38;5;241m=\u001b[39m \u001b[43mfind_minimal_p\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend((n, min_p))\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMinimum P for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m qubits: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_p\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[203], line 40\u001b[0m, in \u001b[0;36mfind_minimal_p\u001b[1;34m(num_qubits)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Optimization loop\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m---> 40\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mcircuit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mcircuit(params)\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m threshold_loss:\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pennylane\\optimize\\gradient_descent.py:93\u001b[0m, in \u001b[0;36mGradientDescentOptimizer.step\u001b[1;34m(self, objective_fn, grad_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, objective_fn, \u001b[38;5;241m*\u001b[39margs, grad_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Update trainable arguments with one step of the optimizer.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m        If single arg is provided, list [array] is replaced by array.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m     g, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m     new_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_grad(g, args)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# unwrap from list if one argument, cleaner return\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pennylane\\optimize\\gradient_descent.py:122\u001b[0m, in \u001b[0;36mGradientDescentOptimizer.compute_grad\u001b[1;34m(objective_fn, args, kwargs, grad_fn)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Compute gradient of the objective function at the given point and return it along with\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03mthe objective function forward pass (if available).\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    will not be evaluted and instead ``None`` will be returned.\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    121\u001b[0m g \u001b[38;5;241m=\u001b[39m get_gradient(objective_fn) \u001b[38;5;28;01mif\u001b[39;00m grad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m grad_fn\n\u001b[1;32m--> 122\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[43mg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(g, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    125\u001b[0m num_trainable_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pennylane\\_grad.py:165\u001b[0m, in \u001b[0;36mgrad.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ()\n\u001b[1;32m--> 165\u001b[0m grad_value, ans \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward \u001b[38;5;241m=\u001b[39m ans\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad_value\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\autograd\\wrap_util.py:20\u001b[0m, in \u001b[0;36munary_to_nary.<locals>.nary_operator.<locals>.nary_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(args[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m argnum)\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munary_operator\u001b[49m\u001b[43m(\u001b[49m\u001b[43munary_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnary_op_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnary_op_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pennylane\\_grad.py:183\u001b[0m, in \u001b[0;36mgrad._grad_with_forward\u001b[1;34m(fun, x)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;129m@unary_to_nary\u001b[39m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_grad_with_forward\u001b[39m(fun, x):\n\u001b[0;32m    180\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This function is a replica of ``autograd.grad``, with the only\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m    difference being that it returns both the gradient *and* the forward pass\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    value.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m     vjp, ans \u001b[38;5;241m=\u001b[39m \u001b[43m_make_vjp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=redefined-outer-name\u001b[39;00m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m vspace(ans)\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrad only applies to real scalar-output functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry jacobian, elementwise_grad or holomorphic_grad.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\autograd\\core.py:10\u001b[0m, in \u001b[0;36mmake_vjp\u001b[1;34m(fun, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_vjp\u001b[39m(fun, x):\n\u001b[0;32m      9\u001b[0m     start_node \u001b[38;5;241m=\u001b[39m VJPNode\u001b[38;5;241m.\u001b[39mnew_root()\n\u001b[1;32m---> 10\u001b[0m     end_value, end_node \u001b[38;5;241m=\u001b[39m  \u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end_node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvjp\u001b[39m(g): \u001b[38;5;28;01mreturn\u001b[39;00m vspace(x)\u001b[38;5;241m.\u001b[39mzeros()\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\autograd\\tracer.py:10\u001b[0m, in \u001b[0;36mtrace\u001b[1;34m(start_node, fun, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace_stack\u001b[38;5;241m.\u001b[39mnew_trace() \u001b[38;5;28;01mas\u001b[39;00m t:\n\u001b[0;32m      9\u001b[0m     start_box \u001b[38;5;241m=\u001b[39m new_box(x, t, start_node)\n\u001b[1;32m---> 10\u001b[0m     end_box \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_box\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isbox(end_box) \u001b[38;5;129;01mand\u001b[39;00m end_box\u001b[38;5;241m.\u001b[39m_trace \u001b[38;5;241m==\u001b[39m start_box\u001b[38;5;241m.\u001b[39m_trace:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m end_box\u001b[38;5;241m.\u001b[39m_value, end_box\u001b[38;5;241m.\u001b[39m_node\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\autograd\\wrap_util.py:15\u001b[0m, in \u001b[0;36munary_to_nary.<locals>.nary_operator.<locals>.nary_f.<locals>.unary_f\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     subargs \u001b[38;5;241m=\u001b[39m subvals(args, \u001b[38;5;28mzip\u001b[39m(argnum, x))\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msubargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[203], line 40\u001b[0m, in \u001b[0;36mfind_minimal_p.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Optimization loop\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m---> 40\u001b[0m     params \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m-\u001b[39m\u001b[43mcircuit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, params)\n\u001b[0;32m     41\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mcircuit(params)\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m threshold_loss:\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pennylane\\qnode.py:1039\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1034\u001b[0m         full_transform_program\u001b[38;5;241m.\u001b[39m_set_all_argnums(\n\u001b[0;32m   1035\u001b[0m             \u001b[38;5;28mself\u001b[39m, args, kwargs, argnums\n\u001b[0;32m   1036\u001b[0m         )  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;66;03m# pylint: disable=unexpected-keyword-arg\u001b[39;00m\n\u001b[1;32m-> 1039\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mqml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_program\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_transform_program\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverride_shots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_shots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1051\u001b[0m res \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;66;03m# convert result to the interface in case the qfunc has no parameters\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pennylane\\interfaces\\execution.py:648\u001b[0m, in \u001b[0;36mexecute\u001b[1;34m(tapes, device, gradient_fn, interface, transform_program, config, grad_on_execution, gradient_kwargs, cache, cachesize, max_diff, override_shots, expand_fn, max_expansion, device_batch_transform, device_vjp)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;66;03m# Exiting early if we do not need to deal with an interface boundary\u001b[39;00m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_interface_boundary_required:\n\u001b[1;32m--> 648\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43minner_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m post_processing(results)\n\u001b[0;32m    651\u001b[0m _grad_on_execution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pennylane\\interfaces\\execution.py:261\u001b[0m, in \u001b[0;36m_make_inner_execute.<locals>.inner_execute\u001b[1;34m(tapes, **_)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numpy_only:\n\u001b[0;32m    260\u001b[0m     tapes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(qml\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mconvert_to_numpy_parameters(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tapes)\n\u001b[1;32m--> 261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached_device_execution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pennylane\\interfaces\\execution.py:383\u001b[0m, in \u001b[0;36mcache_execute.<locals>.wrapper\u001b[1;34m(tapes, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (res, []) \u001b[38;5;28;01mif\u001b[39;00m return_tuple \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;66;03m# execute all unique tapes that do not exist in the cache\u001b[39;00m\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;66;03m# convert to list as new device interface returns a tuple\u001b[39;00m\n\u001b[1;32m--> 383\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecution_tapes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    385\u001b[0m final_res \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, tape \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tapes):\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pennylane\\devices\\default_qubit.py:523\u001b[0m, in \u001b[0;36mDefaultQubit.execute\u001b[1;34m(self, circuits, execution_config)\u001b[0m\n\u001b[0;32m    517\u001b[0m interface \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    518\u001b[0m     execution_config\u001b[38;5;241m.\u001b[39minterface\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m execution_config\u001b[38;5;241m.\u001b[39mgradient_method \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackprop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    521\u001b[0m )\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_workers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 523\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m    524\u001b[0m         simulate(\n\u001b[0;32m    525\u001b[0m             c,\n\u001b[0;32m    526\u001b[0m             rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng,\n\u001b[0;32m    527\u001b[0m             prng_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prng_key,\n\u001b[0;32m    528\u001b[0m             debugger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debugger,\n\u001b[0;32m    529\u001b[0m             interface\u001b[38;5;241m=\u001b[39minterface,\n\u001b[0;32m    530\u001b[0m             state_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_cache,\n\u001b[0;32m    531\u001b[0m         )\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m circuits\n\u001b[0;32m    533\u001b[0m     )\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    535\u001b[0m     vanilla_circuits \u001b[38;5;241m=\u001b[39m [convert_to_numpy_parameters(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m circuits]\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pennylane\\devices\\default_qubit.py:524\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    517\u001b[0m interface \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    518\u001b[0m     execution_config\u001b[38;5;241m.\u001b[39minterface\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m execution_config\u001b[38;5;241m.\u001b[39mgradient_method \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackprop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    521\u001b[0m )\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_workers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    523\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m--> 524\u001b[0m         \u001b[43msimulate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m            \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprng_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prng_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdebugger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_debugger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m            \u001b[49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m circuits\n\u001b[0;32m    533\u001b[0m     )\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    535\u001b[0m     vanilla_circuits \u001b[38;5;241m=\u001b[39m [convert_to_numpy_parameters(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m circuits]\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pennylane\\devices\\qubit\\simulate.py:240\u001b[0m, in \u001b[0;36msimulate\u001b[1;34m(circuit, rng, prng_key, debugger, interface, state_cache)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimulate\u001b[39m(\n\u001b[0;32m    204\u001b[0m     circuit: qml\u001b[38;5;241m.\u001b[39mtape\u001b[38;5;241m.\u001b[39mQuantumScript,\n\u001b[0;32m    205\u001b[0m     rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m     state_cache: Optional[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    210\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Result:\n\u001b[0;32m    211\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Simulate a single quantum script.\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03m    This is an internal function that will be called by the successor to ``default.qubit``.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    238\u001b[0m \n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m     state, is_state_batched \u001b[38;5;241m=\u001b[39m \u001b[43mget_final_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcircuit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebugger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebugger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m state_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    242\u001b[0m         state_cache[circuit\u001b[38;5;241m.\u001b[39mhash] \u001b[38;5;241m=\u001b[39m state\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pennylane\\devices\\qubit\\simulate.py:128\u001b[0m, in \u001b[0;36mget_final_state\u001b[1;34m(circuit, debugger, interface)\u001b[0m\n\u001b[0;32m    126\u001b[0m is_state_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(prep \u001b[38;5;129;01mand\u001b[39;00m prep\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m circuit\u001b[38;5;241m.\u001b[39moperations[\u001b[38;5;28mbool\u001b[39m(prep) :]:\n\u001b[1;32m--> 128\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mapply_operation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_state_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_state_batched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebugger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebugger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;66;03m# Handle postselection on mid-circuit measurements\u001b[39;00m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(op, qml\u001b[38;5;241m.\u001b[39mProjector):\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\functools.py:909\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    907\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 909\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pennylane\\devices\\qubit\\apply_operation.py:198\u001b[0m, in \u001b[0;36mapply_operation\u001b[1;34m(op, state, is_state_batched, debugger)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;129m@singledispatch\u001b[39m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_operation\u001b[39m(\n\u001b[0;32m    152\u001b[0m     op: qml\u001b[38;5;241m.\u001b[39moperation\u001b[38;5;241m.\u001b[39mOperator, state, is_state_batched: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, debugger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    153\u001b[0m ):\n\u001b[0;32m    154\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply and operator to a given state.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m \n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_operation_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_state_batched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebugger\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pennylane\\devices\\qubit\\apply_operation.py:208\u001b[0m, in \u001b[0;36m_apply_operation_default\u001b[1;34m(op, state, is_state_batched, debugger)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The default behaviour of apply_operation, accessed through the standard dispatch\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03mof apply_operation, as well as conditionally in other dispatches.\"\"\"\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mlen\u001b[39m(op\u001b[38;5;241m.\u001b[39mwires) \u001b[38;5;241m<\u001b[39m EINSUM_OP_WIRECOUNT_PERF_THRESHOLD\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m math\u001b[38;5;241m.\u001b[39mndim(state) \u001b[38;5;241m<\u001b[39m EINSUM_STATE_WIRECOUNT_PERF_THRESHOLD\n\u001b[0;32m    207\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m (op\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;129;01mand\u001b[39;00m is_state_batched):\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_operation_einsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_state_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_state_batched\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m apply_operation_tensordot(op, state, is_state_batched\u001b[38;5;241m=\u001b[39mis_state_batched)\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pennylane\\devices\\qubit\\apply_operation.py:72\u001b[0m, in \u001b[0;36mapply_operation_einsum\u001b[1;34m(op, state, is_state_batched)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_operation_einsum\u001b[39m(op: qml\u001b[38;5;241m.\u001b[39moperation\u001b[38;5;241m.\u001b[39mOperator, state, is_state_batched: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     61\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``Operator`` to ``state`` using ``einsum``. This is more efficent at lower qubit\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    numbers.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m        array[complex]: output_state\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     mat \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     total_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(state\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m-\u001b[39m is_state_batched\n\u001b[0;32m     75\u001b[0m     num_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(op\u001b[38;5;241m.\u001b[39mwires)\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pennylane\\operation.py:785\u001b[0m, in \u001b[0;36mOperator.matrix\u001b[1;34m(self, wire_order)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatrix\u001b[39m(\u001b[38;5;28mself\u001b[39m, wire_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    766\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Representation of the operator as a matrix in the computational basis.\u001b[39;00m\n\u001b[0;32m    767\u001b[0m \n\u001b[0;32m    768\u001b[0m \u001b[38;5;124;03m    If ``wire_order`` is provided, the numerical representation considers the position of the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;124;03m        tensor_like: matrix representation\u001b[39;00m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 785\u001b[0m     canonical_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wire_order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwires \u001b[38;5;241m==\u001b[39m Wires(wire_order):\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m canonical_matrix\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pennylane\\ops\\qubit\\parametric_ops_multi_qubit.py:397\u001b[0m, in \u001b[0;36mPauliRot.compute_matrix\u001b[1;34m(theta, pauli_word)\u001b[0m\n\u001b[0;32m    391\u001b[0m     conjugation_matrix \u001b[38;5;241m=\u001b[39m qml\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mcast_like(conjugation_matrix, \u001b[38;5;241m1\u001b[39mj)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# Note: we use einsum with reverse arguments here because it is not multi-dispatched\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;66;03m# and the tensordot containing multi_Z_rot_matrix should decide about the interface\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m expand_matrix(\n\u001b[0;32m    395\u001b[0m     qml\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39meinsum(\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...jk,ij->...ik\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 397\u001b[0m         \u001b[43mqml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensordot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmulti_Z_rot_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconjugation_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    398\u001b[0m         qml\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mconj(conjugation_matrix),\n\u001b[0;32m    399\u001b[0m     ),\n\u001b[0;32m    400\u001b[0m     non_identity_wires,\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(pauli_word))),\n\u001b[0;32m    402\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pennylane\\math\\multi_dispatch.py:151\u001b[0m, in \u001b[0;36mmulti_dispatch.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m interface \u001b[38;5;241m=\u001b[39m interface \u001b[38;5;129;01mor\u001b[39;00m get_interface(\u001b[38;5;241m*\u001b[39mdispatch_args)\n\u001b[0;32m    149\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlike\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m interface\n\u001b[1;32m--> 151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pennylane\\math\\multi_dispatch.py:389\u001b[0m, in \u001b[0;36mtensordot\u001b[1;34m(tensor1, tensor2, axes, like)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the tensor product of two tensors.\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;124;03mIn general ``axes`` specifies either the set of axes for both\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;124;03mtensors that are contracted (with the first/second entry of ``axes``\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    tensor_like: the tensor product of the two input tensors\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    388\u001b[0m tensor1, tensor2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcoerce([tensor1, tensor2], like\u001b[38;5;241m=\u001b[39mlike)\n\u001b[1;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensordot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlike\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlike\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\autoray\\autoray.py:80\u001b[0m, in \u001b[0;36mdo\u001b[1;34m(fn, like, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Do function named ``fn`` on ``(*args, **kwargs)``, peforming single\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03mdispatch to retrieve ``fn`` based on whichever library defines the class of\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03mthe ``args[0]``, or the ``like`` keyword argument if specified.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m    <tf.Tensor: id=91, shape=(3, 3), dtype=float32>\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     79\u001b[0m backend \u001b[38;5;241m=\u001b[39m choose_backend(fn, \u001b[38;5;241m*\u001b[39margs, like\u001b[38;5;241m=\u001b[39mlike, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_lib_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pennylane\\numpy\\wrapper.py:117\u001b[0m, in \u001b[0;36mtensor_wrapper.<locals>._wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m         tensor_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _np\u001b[38;5;241m.\u001b[39many([i\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tensor_args])\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# evaluate the original object\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, _np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# only if the output of the object is a ndarray,\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# then convert to a PennyLane tensor\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     res \u001b[38;5;241m=\u001b[39m tensor(res, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtensor_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\autograd\\tracer.py:44\u001b[0m, in \u001b[0;36mprimitive.<locals>.f_wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m parents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(box\u001b[38;5;241m.\u001b[39m_node \u001b[38;5;28;01mfor\u001b[39;00m _     , box \u001b[38;5;129;01min\u001b[39;00m boxed_args)\n\u001b[0;32m     43\u001b[0m argnums \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(argnum    \u001b[38;5;28;01mfor\u001b[39;00m argnum, _   \u001b[38;5;129;01min\u001b[39;00m boxed_args)\n\u001b[1;32m---> 44\u001b[0m ans \u001b[38;5;241m=\u001b[39m \u001b[43mf_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m node \u001b[38;5;241m=\u001b[39m node_constructor(ans, f_wrapped, argvals, kwargs, argnums, parents)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_box(ans, trace, node)\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\autograd\\tracer.py:48\u001b[0m, in \u001b[0;36mprimitive.<locals>.f_wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_box(ans, trace, node)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\riakh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\numpy\\core\\numeric.py:1121\u001b[0m, in \u001b[0;36mtensordot\u001b[1;34m(a, b, axes)\u001b[0m\n\u001b[0;32m   1119\u001b[0m at \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mtranspose(newaxes_a)\u001b[38;5;241m.\u001b[39mreshape(newshape_a)\n\u001b[0;32m   1120\u001b[0m bt \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mtranspose(newaxes_b)\u001b[38;5;241m.\u001b[39mreshape(newshape_b)\n\u001b[1;32m-> 1121\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mreshape(olda \u001b[38;5;241m+\u001b[39m oldb)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def setup_circuit(num_qubits):\n",
    "    dev = qml.device('default.qubit', wires=num_qubits)\n",
    "    edges = [(i, j) for i in range(num_qubits) for j in range(i + 1, num_qubits)]\n",
    "    nodes = range(num_qubits)\n",
    "    h1 = qml.Hamiltonian([1.0 for _ in edges], [qml.PauliZ(i) @ qml.PauliZ(j) for (i, j) in edges])\n",
    "    h2 = qml.Hamiltonian([1.0 for _ in nodes], [qml.PauliX(i) for i in nodes])\n",
    "    stab1 = qml.Hamiltonian([1], [qml.operation.Tensor(*(qml.PauliX(i) for i in nodes))])\n",
    "    stab2 = qml.Hamiltonian([1 for _ in range(len(nodes) - 1)], [qml.PauliZ(i) @ qml.PauliZ(i + 1) for i in range(len(nodes) - 1)])\n",
    "    stab = stab1 + stab2\n",
    "    return dev, h1, h2, stab\n",
    "\n",
    "\n",
    "def find_minimal_p(num_qubits):\n",
    "    dev, h1, h2, stab = setup_circuit(num_qubits)\n",
    "    threshold_loss = -num_qubits\n",
    "    P = 1\n",
    "    found = False\n",
    "\n",
    "    while not found:\n",
    "        params = np.random.random(2 * P)  \n",
    "        opt = qml.AdamOptimizer(stepsize=0.01)\n",
    "        \n",
    "        # Define the circuit\n",
    "        @qml.qnode(dev)\n",
    "        def circuit(params):\n",
    "            for i in range(num_qubits):\n",
    "                qml.Hadamard(wires=i)\n",
    "            for p in range(P):\n",
    "                qml.templates.ApproxTimeEvolution(h1, params[2 * p], 1)\n",
    "                qml.templates.ApproxTimeEvolution(h2, params[2 * p + 1], 1)\n",
    "            return qml.expval(stab)\n",
    "\n",
    "        # Optimization loop\n",
    "        for _ in range(100):\n",
    "            params = opt.step(lambda x: -circuit(x), params)\n",
    "            loss = -circuit(params)\n",
    "            if loss <= threshold_loss:\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            P += 1  # Increase P if threshold not met\n",
    "\n",
    "    return P\n",
    "\n",
    "\n",
    "max_qubits = 6\n",
    "results = []\n",
    "for n in range(2, max_qubits + 1):\n",
    "    min_p = find_minimal_p(n)\n",
    "    results.append((n, min_p))\n",
    "    print(f\"Minimum P for {n} qubits: {min_p}\")\n",
    "\n",
    "# Plotting the results\n",
    "n_values, p_values = zip(*results)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(n_values, p_values, marker='o')\n",
    "plt.xlabel('Number of Qubits (N)')\n",
    "plt.ylabel('Minimum Layers (P)')\n",
    "plt.title('Minimum P Required for Loss <= -N')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test GHZ state shape: (32,)\n",
      "[0.70710678+0.j 0.        +0.j 0.        +0.j 0.        +0.j\n",
      " 0.        +0.j 0.        +0.j 0.        +0.j 0.        +0.j\n",
      " 0.        +0.j 0.        +0.j 0.        +0.j 0.        +0.j\n",
      " 0.        +0.j 0.        +0.j 0.        +0.j 0.        +0.j\n",
      " 0.        +0.j 0.        +0.j 0.        +0.j 0.        +0.j\n",
      " 0.        +0.j 0.        +0.j 0.        +0.j 0.        +0.j\n",
      " 0.        +0.j 0.        +0.j 0.        +0.j 0.        +0.j\n",
      " 0.        +0.j 0.        +0.j 0.        +0.j 0.70710678+0.j]\n"
     ]
    }
   ],
   "source": [
    "@qml.qnode(dev)\n",
    "def test_ghz_state():\n",
    "    ghz_state_preparation()\n",
    "    return qml.state()\n",
    "\n",
    "# Test the output\n",
    "test_state = test_ghz_state()\n",
    "print(f\"Test GHZ state shape: {test_state.shape}\")\n",
    "print(test_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ideal GHZ state shape: (32,)\n"
     ]
    }
   ],
   "source": [
    "ideal_ghz_state = np.zeros((2**num_qubits,))\n",
    "ideal_ghz_state[0] = 1 / np.sqrt(2)\n",
    "ideal_ghz_state[-1] = 1 / np.sqrt(2)\n",
    "print('Ideal GHZ state shape:', ideal_ghz_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for 2 qubits and 3 layers: -1.199722054009829\n",
      "Loss for 2 qubits and 3 layers: -1.290136929206619\n",
      "Loss for 2 qubits and 3 layers: -1.3797420006284864\n",
      "Loss for 2 qubits and 3 layers: -1.4673822892023232\n",
      "Loss for 2 qubits and 3 layers: -1.5518601729998602\n",
      "Loss for 2 qubits and 3 layers: -1.6319548608693788\n",
      "Loss for 2 qubits and 3 layers: -1.7064525083407411\n",
      "Loss for 2 qubits and 3 layers: -1.7741873549179412\n",
      "Loss for 2 qubits and 3 layers: -1.8340941637706094\n",
      "Loss for 2 qubits and 3 layers: -1.885271659666452\n",
      "Loss for 2 qubits and 3 layers: -1.9270554561014084\n",
      "Loss for 2 qubits and 3 layers: -1.9590969274946706\n",
      "Loss for 2 qubits and 3 layers: -1.9814412799458765\n",
      "Loss for 2 qubits and 3 layers: -1.994593227273708\n",
      "Loss for 2 qubits and 3 layers: -1.9995519301408315\n",
      "Loss for 2 qubits and 3 layers: -1.9977898460737256\n",
      "Loss for 2 qubits and 3 layers: -1.9911498239530672\n",
      "Loss for 2 qubits and 3 layers: -1.9816534920671574\n",
      "Loss for 2 qubits and 3 layers: -1.9712548218947363\n",
      "Loss for 2 qubits and 3 layers: -1.9616092909597147\n",
      "Loss for 2 qubits and 3 layers: -1.9539241889251555\n",
      "Loss for 2 qubits and 3 layers: -1.9489122158024186\n",
      "Loss for 2 qubits and 3 layers: -1.9468286275642503\n",
      "Loss for 2 qubits and 3 layers: -1.947556607062392\n",
      "Loss for 2 qubits and 3 layers: -1.9507104000888102\n",
      "Loss for 2 qubits and 3 layers: -1.9557368625301879\n",
      "Loss for 2 qubits and 3 layers: -1.9620054858754354\n",
      "Loss for 2 qubits and 3 layers: -1.9688827904662118\n",
      "Loss for 2 qubits and 3 layers: -1.9757898466623556\n",
      "Loss for 2 qubits and 3 layers: -1.9822428635775728\n",
      "Loss for 2 qubits and 3 layers: -1.9878773858622152\n",
      "Loss for 2 qubits and 3 layers: -1.9924573413370956\n",
      "Loss for 2 qubits and 3 layers: -1.995871145988464\n",
      "Loss for 2 qubits and 3 layers: -1.998118016104609\n",
      "Loss for 2 qubits and 3 layers: -1.9992881032580763\n",
      "Loss for 2 qubits and 3 layers: -1.9995397852896835\n",
      "Loss for 2 qubits and 3 layers: -1.9990765532557409\n",
      "Loss for 2 qubits and 3 layers: -1.998124890383046\n",
      "Loss for 2 qubits and 3 layers: -1.9969138129642823\n",
      "Loss for 2 qubits and 3 layers: -1.9956565339530687\n",
      "Loss for 2 qubits and 3 layers: -1.994534889596521\n",
      "Loss for 2 qubits and 3 layers: -1.9936874251926229\n",
      "Loss for 2 qubits and 3 layers: -1.993202076790081\n",
      "Loss for 2 qubits and 3 layers: -1.9931140848101485\n",
      "Loss for 2 qubits and 3 layers: -1.9934091980257942\n",
      "Loss for 2 qubits and 3 layers: -1.9940315556746864\n",
      "Loss for 2 qubits and 3 layers: -1.9948950642255527\n",
      "Loss for 2 qubits and 3 layers: -1.9958967341988365\n",
      "Loss for 2 qubits and 3 layers: -1.9969303401432643\n",
      "Loss for 2 qubits and 3 layers: -1.9978988777716356\n",
      "Loss for 2 qubits and 3 layers: -1.9987245584585178\n",
      "Loss for 2 qubits and 3 layers: -1.9993554515461396\n",
      "Loss for 2 qubits and 3 layers: -1.9997683216054134\n",
      "Loss for 2 qubits and 3 layers: -1.9999676779922422\n",
      "Loss for 2 qubits and 3 layers: -1.9999815161107155\n",
      "Loss for 2 qubits and 3 layers: -1.9998546271189221\n",
      "Loss for 2 qubits and 3 layers: -1.999640620754532\n",
      "Loss for 2 qubits and 3 layers: -1.9993938912092355\n",
      "Loss for 2 qubits and 3 layers: -1.999162639275597\n",
      "Loss for 2 qubits and 3 layers: -1.9989837732606248\n",
      "Loss for 2 qubits and 3 layers: -1.9988801161918084\n",
      "Loss for 2 qubits and 3 layers: -1.9988599368291031\n",
      "Loss for 2 qubits and 3 layers: -1.9989184772270372\n",
      "Loss for 2 qubits and 3 layers: -1.9990409205958906\n",
      "Loss for 2 qubits and 3 layers: -1.9992061446096945\n",
      "Loss for 2 qubits and 3 layers: -1.9993906226768292\n",
      "Loss for 2 qubits and 3 layers: -1.9995719383866164\n",
      "Loss for 2 qubits and 3 layers: -1.9997315313478965\n",
      "Loss for 2 qubits and 3 layers: -1.9998564639121383\n",
      "Loss for 2 qubits and 3 layers: -1.9999401618984656\n",
      "Loss for 2 qubits and 3 layers: -1.9999822193970962\n",
      "Loss for 2 qubits and 3 layers: -1.999987456168455\n",
      "Loss for 2 qubits and 3 layers: -1.999964471587624\n",
      "Loss for 2 qubits and 3 layers: -1.9999239540114753\n",
      "Loss for 2 qubits and 3 layers: -1.9998769867675223\n",
      "Loss for 2 qubits and 3 layers: -1.9998335521739155\n",
      "Loss for 2 qubits and 3 layers: -1.9998013830861894\n",
      "Loss for 2 qubits and 3 layers: -1.9997852547370742\n",
      "Loss for 2 qubits and 3 layers: -1.9997867522908774\n",
      "Loss for 2 qubits and 3 layers: -1.9998044938748478\n",
      "Loss for 2 qubits and 3 layers: -1.9998347372102785\n",
      "Loss for 2 qubits and 3 layers: -1.9998722541753833\n",
      "Loss for 2 qubits and 3 layers: -1.999911327173585\n",
      "Loss for 2 qubits and 3 layers: -1.9999467099269563\n",
      "Loss for 2 qubits and 3 layers: -1.9999744073898575\n",
      "Loss for 2 qubits and 3 layers: -1.9999921651524843\n",
      "Loss for 2 qubits and 3 layers: -1.999999613184088\n",
      "Loss for 2 qubits and 3 layers: -1.9999980724351538\n",
      "Loss for 2 qubits and 3 layers: -1.9999900930553323\n",
      "Loss for 2 qubits and 3 layers: -1.9999788371739395\n",
      "Loss for 2 qubits and 3 layers: -1.9999674381078378\n",
      "Loss for 2 qubits and 3 layers: -1.9999584583435674\n",
      "Loss for 2 qubits and 3 layers: -1.999953534561456\n",
      "Loss for 2 qubits and 3 layers: -1.9999532490839789\n",
      "Loss for 2 qubits and 3 layers: -1.9999572161340542\n",
      "Loss for 2 qubits and 3 layers: -1.9999643301237777\n",
      "Loss for 2 qubits and 3 layers: -1.9999730998757985\n",
      "Loss for 2 qubits and 3 layers: -1.9999819896261228\n",
      "Loss for 2 qubits and 3 layers: -1.9999897018691324\n",
      "Loss for 2 qubits and 3 layers: -1.9999953618865423\n",
      "Loss for 3 qubits and 5 layers: -2.2302751960742966\n",
      "Loss for 3 qubits and 5 layers: -2.4869207273481098\n",
      "Loss for 3 qubits and 5 layers: -2.695577403501706\n",
      "Loss for 3 qubits and 5 layers: -2.84905325266128\n",
      "Loss for 3 qubits and 5 layers: -2.9464091996055317\n",
      "Loss for 3 qubits and 5 layers: -2.992629310034195\n",
      "Loss for 3 qubits and 5 layers: -2.9981128579289744\n",
      "Loss for 3 qubits and 5 layers: -2.977114456778997\n",
      "Loss for 3 qubits and 5 layers: -2.9447069210116252\n",
      "Loss for 3 qubits and 5 layers: -2.913204203568927\n",
      "Loss for 3 qubits and 5 layers: -2.8901007316354472\n",
      "Loss for 3 qubits and 5 layers: -2.8783541763618823\n",
      "Loss for 3 qubits and 5 layers: -2.877716345711155\n",
      "Loss for 3 qubits and 5 layers: -2.886119515543305\n",
      "Loss for 3 qubits and 5 layers: -2.900764363386702\n",
      "Loss for 3 qubits and 5 layers: -2.9188838024362385\n",
      "Loss for 3 qubits and 5 layers: -2.9381692489560685\n",
      "Loss for 3 qubits and 5 layers: -2.956808940395855\n",
      "Loss for 3 qubits and 5 layers: -2.973303356789798\n",
      "Loss for 3 qubits and 5 layers: -2.9863924373734183\n",
      "Loss for 3 qubits and 5 layers: -2.9951803762299005\n",
      "Loss for 3 qubits and 5 layers: -2.9993186743540816\n",
      "Loss for 3 qubits and 5 layers: -2.9991276762946204\n",
      "Loss for 3 qubits and 5 layers: -2.9955900061984053\n",
      "Loss for 3 qubits and 5 layers: -2.9901725635829397\n",
      "Loss for 3 qubits and 5 layers: -2.984487374574706\n",
      "Loss for 3 qubits and 5 layers: -2.9799016738846005\n",
      "Loss for 3 qubits and 5 layers: -2.9772630881709095\n",
      "Loss for 3 qubits and 5 layers: -2.9768330049205787\n",
      "Loss for 3 qubits and 5 layers: -2.9783945754363295\n",
      "Loss for 3 qubits and 5 layers: -2.981439561481086\n",
      "Loss for 3 qubits and 5 layers: -2.985348391676173\n",
      "Loss for 3 qubits and 5 layers: -2.989514860925332\n",
      "Loss for 3 qubits and 5 layers: -2.993408448018136\n",
      "Loss for 3 qubits and 5 layers: -2.996601517342541\n",
      "Loss for 3 qubits and 5 layers: -2.9987960768178614\n",
      "Loss for 3 qubits and 5 layers: -2.999860821479103\n",
      "Loss for 3 qubits and 5 layers: -2.9998592052332373\n",
      "Loss for 3 qubits and 5 layers: -2.9990397001355147\n",
      "Loss for 3 qubits and 5 layers: -2.9977745182398294\n",
      "Loss for 3 qubits and 5 layers: -2.996461248576424\n",
      "Loss for 3 qubits and 5 layers: -2.995424704993136\n",
      "Loss for 3 qubits and 5 layers: -2.994856779540955\n",
      "Loss for 3 qubits and 5 layers: -2.994809290085889\n",
      "Loss for 3 qubits and 5 layers: -2.995226651769772\n",
      "Loss for 3 qubits and 5 layers: -2.995990829094371\n",
      "Loss for 3 qubits and 5 layers: -2.9969560429474025\n",
      "Loss for 3 qubits and 5 layers: -2.9979678077116874\n",
      "Loss for 3 qubits and 5 layers: -2.9988761867573426\n",
      "Loss for 3 qubits and 5 layers: -2.9995550403299003\n",
      "Loss for 3 qubits and 5 layers: -2.9999277948564376\n",
      "Loss for 3 qubits and 5 layers: -2.9999876973491872\n",
      "Loss for 3 qubits and 5 layers: -2.999798239382911\n",
      "Loss for 3 qubits and 5 layers: -2.999469487683465\n",
      "Loss for 3 qubits and 5 layers: -2.999120650722747\n",
      "Loss for 3 qubits and 5 layers: -2.998846858854199\n",
      "Loss for 3 qubits and 5 layers: -2.998703044596345\n",
      "Loss for 3 qubits and 5 layers: -2.9987051799815427\n",
      "Loss for 3 qubits and 5 layers: -2.9988397554929884\n",
      "Loss for 3 qubits and 5 layers: -2.9990725166066023\n",
      "Loss for 3 qubits and 5 layers: -2.999354487248719\n",
      "Loss for 3 qubits and 5 layers: -2.9996293297779175\n",
      "Loss for 3 qubits and 5 layers: -2.999845368725488\n",
      "Loss for 3 qubits and 5 layers: -2.9999696214791545\n",
      "Loss for 3 qubits and 5 layers: -2.9999966217920653\n",
      "Loss for 3 qubits and 5 layers: -2.9999466103722083\n",
      "Loss for 3 qubits and 5 layers: -2.9998543843020293\n",
      "Loss for 3 qubits and 5 layers: -2.9997557551078593\n",
      "Loss for 3 qubits and 5 layers: -2.999678448748335\n",
      "Loss for 3 qubits and 5 layers: -2.999639292146219\n",
      "Loss for 3 qubits and 5 layers: -2.9996448069904336\n",
      "Loss for 3 qubits and 5 layers: -2.999691863537323\n",
      "Loss for 3 qubits and 5 layers: -2.9997680892507352\n",
      "Loss for 3 qubits and 5 layers: -2.99985422262629\n",
      "Loss for 3 qubits and 5 layers: -2.9999296727207656\n",
      "Loss for 3 qubits and 5 layers: -2.9999793554536556\n",
      "Loss for 3 qubits and 5 layers: -2.999998041181345\n",
      "Loss for 3 qubits and 5 layers: -2.999989998058819\n",
      "Loss for 3 qubits and 5 layers: -2.999965098861153\n",
      "Loss for 3 qubits and 5 layers: -2.9999345039240293\n",
      "Loss for 3 qubits and 5 layers: -2.999908036742683\n",
      "Loss for 3 qubits and 5 layers: -2.999893033491403\n",
      "Loss for 3 qubits and 5 layers: -2.999893401517712\n",
      "Loss for 3 qubits and 5 layers: -2.999908601190772\n",
      "Loss for 3 qubits and 5 layers: -2.999933612758758\n",
      "Loss for 3 qubits and 5 layers: -2.999960793697339\n",
      "Loss for 3 qubits and 5 layers: -2.9999829699588343\n",
      "Loss for 3 qubits and 5 layers: -2.999995937006814\n",
      "Loss for 3 qubits and 5 layers: -2.9999990770451768\n",
      "Loss for 3 qubits and 5 layers: -2.9999943874062454\n",
      "Loss for 3 qubits and 5 layers: -2.9999851632556602\n",
      "Loss for 3 qubits and 5 layers: -2.9999751195014097\n",
      "Loss for 3 qubits and 5 layers: -2.9999677520958854\n",
      "Loss for 3 qubits and 5 layers: -2.9999655002761054\n",
      "Loss for 3 qubits and 5 layers: -2.999968903926316\n",
      "Loss for 3 qubits and 5 layers: -2.999976450664861\n",
      "Loss for 3 qubits and 5 layers: -2.9999854206652516\n",
      "Loss for 3 qubits and 5 layers: -2.999993188101646\n",
      "Loss for 3 qubits and 5 layers: -2.9999981198819383\n",
      "Loss for 3 qubits and 5 layers: -2.999999730283688\n",
      "Loss for 4 qubits and 7 layers: -2.3098790109348077\n",
      "Loss for 4 qubits and 7 layers: -2.8773641048359186\n",
      "Loss for 4 qubits and 7 layers: -3.3283639666092335\n",
      "Loss for 4 qubits and 7 layers: -3.6312710567031328\n",
      "Loss for 4 qubits and 7 layers: -3.7801063173517684\n",
      "Loss for 4 qubits and 7 layers: -3.808072683932591\n",
      "Loss for 4 qubits and 7 layers: -3.782661473811202\n",
      "Loss for 4 qubits and 7 layers: -3.7605547873098533\n",
      "Loss for 4 qubits and 7 layers: -3.7635662105338743\n",
      "Loss for 4 qubits and 7 layers: -3.7875352233540127\n",
      "Loss for 4 qubits and 7 layers: -3.817879637126823\n",
      "Loss for 4 qubits and 7 layers: -3.8414485831314074\n",
      "Loss for 4 qubits and 7 layers: -3.85363725371631\n",
      "Loss for 4 qubits and 7 layers: -3.858695908834026\n",
      "Loss for 4 qubits and 7 layers: -3.864412907715696\n",
      "Loss for 4 qubits and 7 layers: -3.87647363742791\n",
      "Loss for 4 qubits and 7 layers: -3.8956888078481566\n",
      "Loss for 4 qubits and 7 layers: -3.918302623962293\n",
      "Loss for 4 qubits and 7 layers: -3.938346953482858\n",
      "Loss for 4 qubits and 7 layers: -3.950678632271823\n",
      "Loss for 4 qubits and 7 layers: -3.9534280627381424\n",
      "Loss for 4 qubits and 7 layers: -3.948820237716725\n",
      "Loss for 4 qubits and 7 layers: -3.9419501773120724\n",
      "Loss for 4 qubits and 7 layers: -3.938221137761992\n",
      "Loss for 4 qubits and 7 layers: -3.9409679116611613\n",
      "Loss for 4 qubits and 7 layers: -3.950422205951544\n",
      "Loss for 4 qubits and 7 layers: -3.9641262745011137\n",
      "Loss for 4 qubits and 7 layers: -3.978223250175528\n",
      "Loss for 4 qubits and 7 layers: -3.9889771825892044\n",
      "Loss for 4 qubits and 7 layers: -3.9940364093444494\n",
      "Loss for 4 qubits and 7 layers: -3.9931101721226\n",
      "Loss for 4 qubits and 7 layers: -3.987886913178795\n",
      "Loss for 4 qubits and 7 layers: -3.9812440772811084\n",
      "Loss for 4 qubits and 7 layers: -3.976067152192733\n",
      "Loss for 4 qubits and 7 layers: -3.9741902538822087\n",
      "Loss for 4 qubits and 7 layers: -3.975919389492092\n",
      "Loss for 4 qubits and 7 layers: -3.9802676318162833\n",
      "Loss for 4 qubits and 7 layers: -3.9856495017820963\n",
      "Loss for 4 qubits and 7 layers: -3.990615307850339\n",
      "Loss for 4 qubits and 7 layers: -3.994298571136775\n",
      "Loss for 4 qubits and 7 layers: -3.9964770765750615\n",
      "Loss for 4 qubits and 7 layers: -3.9973614850089083\n",
      "Loss for 4 qubits and 7 layers: -3.997320132223151\n",
      "Loss for 4 qubits and 7 layers: -3.9967039657986216\n",
      "Loss for 4 qubits and 7 layers: -3.995812589628872\n",
      "Loss for 4 qubits and 7 layers: -3.9949319140001123\n",
      "Loss for 4 qubits and 7 layers: -3.994343092750329\n",
      "Loss for 4 qubits and 7 layers: -3.9942596924732428\n",
      "Loss for 4 qubits and 7 layers: -3.9947368195654773\n",
      "Loss for 4 qubits and 7 layers: -3.9956337878342874\n",
      "Loss for 4 qubits and 7 layers: -3.9966741889633037\n",
      "Loss for 4 qubits and 7 layers: -3.997574182391042\n",
      "Loss for 4 qubits and 7 layers: -3.998162058893328\n",
      "Loss for 4 qubits and 7 layers: -3.998422870588495\n",
      "Loss for 4 qubits and 7 layers: -3.998457939055088\n",
      "Loss for 4 qubits and 7 layers: -3.998403453774732\n",
      "Loss for 4 qubits and 7 layers: -3.998365049592439\n",
      "Loss for 4 qubits and 7 layers: -3.998396635968849\n",
      "Loss for 4 qubits and 7 layers: -3.9985136218134727\n",
      "Loss for 4 qubits and 7 layers: -3.998711746561704\n",
      "Loss for 4 qubits and 7 layers: -3.99897043931238\n",
      "Loss for 4 qubits and 7 layers: -3.9992432945107756\n",
      "Loss for 4 qubits and 7 layers: -3.999457411778976\n",
      "Loss for 4 qubits and 7 layers: -3.999539905016668\n",
      "Loss for 4 qubits and 7 layers: -3.999464020501798\n",
      "Loss for 4 qubits and 7 layers: -3.999281400665441\n",
      "Loss for 4 qubits and 7 layers: -3.999107595759041\n",
      "Loss for 4 qubits and 7 layers: -3.9990601652031037\n",
      "Loss for 4 qubits and 7 layers: -3.99918806638806\n",
      "Loss for 4 qubits and 7 layers: -3.999442570779343\n",
      "Loss for 4 qubits and 7 layers: -3.9997109794942123\n",
      "Loss for 4 qubits and 7 layers: -3.999888008184941\n",
      "Loss for 4 qubits and 7 layers: -3.9999339029652887\n",
      "Loss for 4 qubits and 7 layers: -3.9998825000557523\n",
      "Loss for 4 qubits and 7 layers: -3.9998024457118824\n",
      "Loss for 4 qubits and 7 layers: -3.9997472040302973\n",
      "Loss for 4 qubits and 7 layers: -3.999730413883116\n",
      "Loss for 4 qubits and 7 layers: -3.9997367538325346\n",
      "Loss for 4 qubits and 7 layers: -3.9997496930441745\n",
      "Loss for 4 qubits and 7 layers: -3.999768911860932\n",
      "Loss for 4 qubits and 7 layers: -3.9998048929991317\n",
      "Loss for 4 qubits and 7 layers: -3.9998606252962805\n",
      "Loss for 4 qubits and 7 layers: -3.9999208296879347\n",
      "Loss for 4 qubits and 7 layers: -3.9999601090054635\n",
      "Loss for 4 qubits and 7 layers: -3.999962788069539\n",
      "Loss for 4 qubits and 7 layers: -3.9999362892764543\n",
      "Loss for 4 qubits and 7 layers: -3.999905957770833\n",
      "Loss for 4 qubits and 7 layers: -3.9998959243611427\n",
      "Loss for 4 qubits and 7 layers: -3.999912333420614\n",
      "Loss for 4 qubits and 7 layers: -3.9999419705869808\n",
      "Loss for 4 qubits and 7 layers: -3.999965291569568\n",
      "Loss for 4 qubits and 7 layers: -3.999971494406939\n",
      "Loss for 4 qubits and 7 layers: -3.9999638702142595\n",
      "Loss for 4 qubits and 7 layers: -3.9999536987910327\n",
      "Loss for 4 qubits and 7 layers: -3.9999502552959765\n",
      "Loss for 4 qubits and 7 layers: -3.999955475112144\n",
      "Loss for 4 qubits and 7 layers: -3.9999656362062965\n",
      "Loss for 4 qubits and 7 layers: -3.9999761264962777\n",
      "Loss for 4 qubits and 7 layers: -3.9999843623793776\n",
      "Loss for 4 qubits and 7 layers: -3.9999895116709974\n",
      "Loss for 5 qubits and 10 layers: -4.674592598142568\n",
      "Loss for 5 qubits and 10 layers: -4.941364997767437\n",
      "Loss for 5 qubits and 10 layers: -4.923822564561335\n",
      "Loss for 5 qubits and 10 layers: -4.781015882362303\n",
      "Loss for 5 qubits and 10 layers: -4.730488648828279\n",
      "Loss for 5 qubits and 10 layers: -4.770481305515881\n",
      "Loss for 5 qubits and 10 layers: -4.842891828673422\n",
      "Loss for 5 qubits and 10 layers: -4.908330213341965\n",
      "Loss for 5 qubits and 10 layers: -4.9498631827767525\n",
      "Loss for 5 qubits and 10 layers: -4.964388453530866\n",
      "Loss for 5 qubits and 10 layers: -4.956540632445932\n",
      "Loss for 5 qubits and 10 layers: -4.937396249658347\n",
      "Loss for 5 qubits and 10 layers: -4.920207522155658\n",
      "Loss for 5 qubits and 10 layers: -4.914730205548272\n",
      "Loss for 5 qubits and 10 layers: -4.9238728798898395\n",
      "Loss for 5 qubits and 10 layers: -4.943241824185927\n",
      "Loss for 5 qubits and 10 layers: -4.96458703114892\n",
      "Loss for 5 qubits and 10 layers: -4.9808379931118445\n",
      "Loss for 5 qubits and 10 layers: -4.9885922645687275\n",
      "Loss for 5 qubits and 10 layers: -4.987589201454714\n",
      "Loss for 5 qubits and 10 layers: -4.979910673066136\n",
      "Loss for 5 qubits and 10 layers: -4.9699736281311875\n",
      "Loss for 5 qubits and 10 layers: -4.963549958449755\n",
      "Loss for 5 qubits and 10 layers: -4.96489703410211\n",
      "Loss for 5 qubits and 10 layers: -4.974037373966147\n",
      "Loss for 5 qubits and 10 layers: -4.986632289964301\n",
      "Loss for 5 qubits and 10 layers: -4.996596141478797\n",
      "Loss for 5 qubits and 10 layers: -4.999680308894671\n",
      "Loss for 5 qubits and 10 layers: -4.995839539038168\n",
      "Loss for 5 qubits and 10 layers: -4.988896454632636\n",
      "Loss for 5 qubits and 10 layers: -4.9837748028812054\n",
      "Loss for 5 qubits and 10 layers: -4.9834052059198415\n",
      "Loss for 5 qubits and 10 layers: -4.9874228475165525\n",
      "Loss for 5 qubits and 10 layers: -4.9930899741350165\n",
      "Loss for 5 qubits and 10 layers: -4.997389370692212\n",
      "Loss for 5 qubits and 10 layers: -4.998805236037615\n",
      "Loss for 5 qubits and 10 layers: -4.9977668885631905\n",
      "Loss for 5 qubits and 10 layers: -4.995793281826592\n",
      "Loss for 5 qubits and 10 layers: -4.994284835417469\n",
      "Loss for 5 qubits and 10 layers: -4.993870092209071\n",
      "Loss for 5 qubits and 10 layers: -4.9944954338269785\n",
      "Loss for 5 qubits and 10 layers: -4.995830940213102\n",
      "Loss for 5 qubits and 10 layers: -4.997470475525563\n",
      "Loss for 5 qubits and 10 layers: -4.99887388156312\n",
      "Loss for 5 qubits and 10 layers: -4.999457460435398\n",
      "Loss for 5 qubits and 10 layers: -4.998984745285731\n",
      "Loss for 5 qubits and 10 layers: -4.9978602833931856\n",
      "Loss for 5 qubits and 10 layers: -4.996944405489676\n",
      "Loss for 5 qubits and 10 layers: -4.99696268250941\n",
      "Loss for 5 qubits and 10 layers: -4.997948165151164\n",
      "Loss for 5 qubits and 10 layers: -4.99921628922598\n",
      "Loss for 5 qubits and 10 layers: -4.999941925378996\n",
      "Loss for 5 qubits and 10 layers: -4.999804328938499\n",
      "Loss for 5 qubits and 10 layers: -4.999146856048727\n",
      "Loss for 5 qubits and 10 layers: -4.998602880469181\n",
      "Loss for 5 qubits and 10 layers: -4.998563630451907\n",
      "Loss for 5 qubits and 10 layers: -4.998950574753359\n",
      "Loss for 5 qubits and 10 layers: -4.999427117649861\n",
      "Loss for 5 qubits and 10 layers: -4.9997307104793824\n",
      "Loss for 5 qubits and 10 layers: -4.999800077569482\n",
      "Loss for 5 qubits and 10 layers: -4.999705664740697\n",
      "Loss for 5 qubits and 10 layers: -4.99954616118116\n",
      "Loss for 5 qubits and 10 layers: -4.99941264758446\n",
      "Loss for 5 qubits and 10 layers: -4.99939630345661\n",
      "Loss for 5 qubits and 10 layers: -4.999544251208489\n",
      "Loss for 5 qubits and 10 layers: -4.999786624362568\n",
      "Loss for 5 qubits and 10 layers: -4.999961738661184\n",
      "Loss for 5 qubits and 10 layers: -4.999950644458857\n",
      "Loss for 5 qubits and 10 layers: -4.999792271685105\n",
      "Loss for 5 qubits and 10 layers: -4.999652350411679\n",
      "Loss for 5 qubits and 10 layers: -4.999662315761542\n",
      "Loss for 5 qubits and 10 layers: -4.999801388711279\n",
      "Loss for 5 qubits and 10 layers: -4.999941565423444\n",
      "Loss for 5 qubits and 10 layers: -4.999982799084414\n",
      "Loss for 5 qubits and 10 layers: -4.999932537919913\n",
      "Loss for 5 qubits and 10 layers: -4.999866943070582\n",
      "Loss for 5 qubits and 10 layers: -4.999843384710361\n",
      "Loss for 5 qubits and 10 layers: -4.999866306091363\n",
      "Loss for 5 qubits and 10 layers: -4.999911688029772\n",
      "Loss for 5 qubits and 10 layers: -4.999955271686962\n",
      "Loss for 5 qubits and 10 layers: -4.999979060238763\n",
      "Loss for 5 qubits and 10 layers: -4.999971434311392\n",
      "Loss for 5 qubits and 10 layers: -4.999939769216862\n",
      "Loss for 5 qubits and 10 layers: -4.999914929231299\n",
      "Loss for 5 qubits and 10 layers: -4.9999248423239955\n",
      "Loss for 5 qubits and 10 layers: -4.999963653056053\n",
      "Loss for 5 qubits and 10 layers: -4.999995775119662\n",
      "Loss for 5 qubits and 10 layers: -4.9999946098418\n",
      "Loss for 5 qubits and 10 layers: -4.999970238162004\n",
      "Loss for 5 qubits and 10 layers: -4.999953147360576\n",
      "Loss for 5 qubits and 10 layers: -4.999959423718477\n",
      "Loss for 5 qubits and 10 layers: -4.999978742742547\n",
      "Loss for 5 qubits and 10 layers: -4.999992430674392\n",
      "Loss for 5 qubits and 10 layers: -4.999993617324323\n",
      "Loss for 5 qubits and 10 layers: -4.999987409065478\n",
      "Loss for 5 qubits and 10 layers: -4.999981051294691\n",
      "Loss for 5 qubits and 10 layers: -4.9999790166380835\n",
      "Loss for 5 qubits and 10 layers: -4.999983301197195\n",
      "Loss for 5 qubits and 10 layers: -4.999992015970716\n",
      "Loss for 5 qubits and 10 layers: -4.99999833577439\n",
      "Loss for 6 qubits and 12 layers: -2.9035455346097847\n",
      "Loss for 6 qubits and 12 layers: -4.242264549293628\n",
      "Loss for 6 qubits and 12 layers: -4.793456299445729\n",
      "Loss for 6 qubits and 12 layers: -5.041247850572573\n",
      "Loss for 6 qubits and 12 layers: -5.237096357946253\n",
      "Loss for 6 qubits and 12 layers: -5.260687159533236\n",
      "Loss for 6 qubits and 12 layers: -5.204367247811169\n",
      "Loss for 6 qubits and 12 layers: -5.278630028888593\n",
      "Loss for 6 qubits and 12 layers: -5.513658046642298\n",
      "Loss for 6 qubits and 12 layers: -5.783986988928159\n",
      "Loss for 6 qubits and 12 layers: -5.925604417233675\n",
      "Loss for 6 qubits and 12 layers: -5.862093869833709\n",
      "Loss for 6 qubits and 12 layers: -5.688622280317832\n",
      "Loss for 6 qubits and 12 layers: -5.568731748470191\n",
      "Loss for 6 qubits and 12 layers: -5.575787237197121\n",
      "Loss for 6 qubits and 12 layers: -5.687788650815915\n",
      "Loss for 6 qubits and 12 layers: -5.8364490425672955\n",
      "Loss for 6 qubits and 12 layers: -5.944475407420593\n",
      "Loss for 6 qubits and 12 layers: -5.966766808686111\n",
      "Loss for 6 qubits and 12 layers: -5.920919921639582\n",
      "Loss for 6 qubits and 12 layers: -5.866351136720069\n",
      "Loss for 6 qubits and 12 layers: -5.843919323571522\n",
      "Loss for 6 qubits and 12 layers: -5.8546410863224265\n",
      "Loss for 6 qubits and 12 layers: -5.880492457324466\n",
      "Loss for 6 qubits and 12 layers: -5.907940228994856\n",
      "Loss for 6 qubits and 12 layers: -5.92960573783651\n",
      "Loss for 6 qubits and 12 layers: -5.938147596001806\n",
      "Loss for 6 qubits and 12 layers: -5.932892391425511\n",
      "Loss for 6 qubits and 12 layers: -5.926425913088365\n",
      "Loss for 6 qubits and 12 layers: -5.9343084329848255\n",
      "Loss for 6 qubits and 12 layers: -5.9567962289839835\n",
      "Loss for 6 qubits and 12 layers: -5.9780402134088035\n",
      "Loss for 6 qubits and 12 layers: -5.984225701141486\n",
      "Loss for 6 qubits and 12 layers: -5.977087093757353\n",
      "Loss for 6 qubits and 12 layers: -5.968242873341962\n",
      "Loss for 6 qubits and 12 layers: -5.965493698693409\n",
      "Loss for 6 qubits and 12 layers: -5.968581404633635\n",
      "Loss for 6 qubits and 12 layers: -5.974021348722458\n",
      "Loss for 6 qubits and 12 layers: -5.979095334835989\n",
      "Loss for 6 qubits and 12 layers: -5.981978919738196\n",
      "Loss for 6 qubits and 12 layers: -5.981958704914071\n",
      "Loss for 6 qubits and 12 layers: -5.981018532999839\n",
      "Loss for 6 qubits and 12 layers: -5.982999750462453\n",
      "Loss for 6 qubits and 12 layers: -5.989041020043658\n",
      "Loss for 6 qubits and 12 layers: -5.995155120146523\n",
      "Loss for 6 qubits and 12 layers: -5.996501362219135\n",
      "Loss for 6 qubits and 12 layers: -5.993185033622297\n",
      "Loss for 6 qubits and 12 layers: -5.989644289698948\n",
      "Loss for 6 qubits and 12 layers: -5.9890569185786555\n",
      "Loss for 6 qubits and 12 layers: -5.990922730667482\n",
      "Loss for 6 qubits and 12 layers: -5.9934678379580895\n",
      "Loss for 6 qubits and 12 layers: -5.995547626969328\n",
      "Loss for 6 qubits and 12 layers: -5.996205938967897\n",
      "Loss for 6 qubits and 12 layers: -5.995124809635807\n",
      "Loss for 6 qubits and 12 layers: -5.993892103874912\n",
      "Loss for 6 qubits and 12 layers: -5.994669585053356\n",
      "Loss for 6 qubits and 12 layers: -5.997284147082211\n",
      "Loss for 6 qubits and 12 layers: -5.999335837197947\n",
      "Loss for 6 qubits and 12 layers: -5.99928264565106\n",
      "Loss for 6 qubits and 12 layers: -5.9979028218261305\n",
      "Loss for 6 qubits and 12 layers: -5.996742698456896\n",
      "Loss for 6 qubits and 12 layers: -5.996591737520797\n",
      "Loss for 6 qubits and 12 layers: -5.9974605370303555\n",
      "Loss for 6 qubits and 12 layers: -5.998731763827428\n",
      "Loss for 6 qubits and 12 layers: -5.999342191893884\n",
      "Loss for 6 qubits and 12 layers: -5.998841826590066\n",
      "Loss for 6 qubits and 12 layers: -5.998108645015838\n",
      "Loss for 6 qubits and 12 layers: -5.9982308164928195\n",
      "Loss for 6 qubits and 12 layers: -5.999091691738436\n",
      "Loss for 6 qubits and 12 layers: -5.999769073789546\n",
      "Loss for 6 qubits and 12 layers: -5.999798650955854\n",
      "Loss for 6 qubits and 12 layers: -5.9994048478966935\n",
      "Loss for 6 qubits and 12 layers: -5.998996931323618\n",
      "Loss for 6 qubits and 12 layers: -5.9989262523707865\n",
      "Loss for 6 qubits and 12 layers: -5.999287285018971\n",
      "Loss for 6 qubits and 12 layers: -5.999706416658506\n",
      "Loss for 6 qubits and 12 layers: -5.999751692743984\n",
      "Loss for 6 qubits and 12 layers: -5.999532099560493\n",
      "Loss for 6 qubits and 12 layers: -5.999454937570515\n",
      "Loss for 6 qubits and 12 layers: -5.999608334923805\n",
      "Loss for 6 qubits and 12 layers: -5.999797039603735\n",
      "Loss for 6 qubits and 12 layers: -5.999893706940006\n",
      "Loss for 6 qubits and 12 layers: -5.999864531869522\n",
      "Loss for 6 qubits and 12 layers: -5.999742355320661\n",
      "Loss for 6 qubits and 12 layers: -5.9996768237058085\n",
      "Loss for 6 qubits and 12 layers: -5.9997559698780485\n",
      "Loss for 6 qubits and 12 layers: -5.999859167923035\n",
      "Loss for 6 qubits and 12 layers: -5.999875068179766\n",
      "Loss for 6 qubits and 12 layers: -5.999858674631239\n",
      "Loss for 6 qubits and 12 layers: -5.99986864721589\n",
      "Loss for 6 qubits and 12 layers: -5.999888661038082\n",
      "Loss for 6 qubits and 12 layers: -5.99991635565196\n",
      "Loss for 6 qubits and 12 layers: -5.999944647851431\n",
      "Loss for 6 qubits and 12 layers: -5.999938210019206\n",
      "Loss for 6 qubits and 12 layers: -5.999914217285813\n",
      "Loss for 6 qubits and 12 layers: -5.999917662132928\n",
      "Loss for 6 qubits and 12 layers: -5.999932801116367\n",
      "Loss for 6 qubits and 12 layers: -5.999935272399423\n",
      "Loss for 6 qubits and 12 layers: -5.9999440748772175\n",
      "Loss for 6 qubits and 12 layers: -5.999961401114513\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def setup_circuit(num_qubits):\n",
    "    dev = qml.device('default.qubit', wires=num_qubits)\n",
    "    edges = [(i, j) for i in range(num_qubits) for j in range(i + 1, num_qubits)]\n",
    "    nodes = range(num_qubits)\n",
    "    h1 = qml.Hamiltonian([1.0 for _ in edges], [qml.PauliZ(i) @ qml.PauliZ(j) for (i, j) in edges])\n",
    "    h2 = qml.Hamiltonian([1.0 for _ in nodes], [qml.PauliX(i) for i in nodes])\n",
    "    stab1 = qml.Hamiltonian([1], [qml.operation.Tensor(*(qml.PauliX(i) for i in nodes))])\n",
    "    stab2 = qml.Hamiltonian([1 for _ in range(len(nodes) - 1)], [qml.PauliZ(i) @ qml.PauliZ(i + 1) for i in range(len(nodes) - 1)])\n",
    "    stab = stab1 + stab2\n",
    "    return dev, h1, h2, stab\n",
    "\n",
    "def test_circuit(num_qubits, P):\n",
    "    dev, h1, h2, stab = setup_circuit(num_qubits)\n",
    "    params = np.random.random(2 * P)\n",
    "    opt = qml.AdamOptimizer(stepsize=0.01)\n",
    "\n",
    "    @qml.qnode(dev)\n",
    "    def circuit(params):\n",
    "        for i in range(num_qubits):\n",
    "            qml.Hadamard(wires=i)\n",
    "        for p in range(P):\n",
    "            qml.templates.ApproxTimeEvolution(h1, params[2 * p], 1)\n",
    "            qml.templates.ApproxTimeEvolution(h2, params[2 * p + 1], 1)\n",
    "        return qml.expval(stab)\n",
    "\n",
    "    for _ in range(100):\n",
    "        params = opt.step(lambda x: -circuit(x), params)\n",
    "        loss = -circuit(params)\n",
    "        print(f\"Loss for {num_qubits} qubits and {P} layers: {loss}\")\n",
    "\n",
    "# Values for n and corresponding P\n",
    "n_and_p_values = [(2, 3), (3, 5), (4, 7), (5, 10), (6, 12)]\n",
    "\n",
    "for n, p in n_and_p_values:\n",
    "    test_circuit(n, p)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for 2 qubits and 2 layers: -0.9909397774085725\n",
      "Loss for 2 qubits and 2 layers: -1.04517404662145\n",
      "Loss for 2 qubits and 2 layers: -1.099292122786149\n",
      "Loss for 2 qubits and 2 layers: -1.1536295185214476\n",
      "Loss for 2 qubits and 2 layers: -1.2083316205913675\n",
      "Loss for 2 qubits and 2 layers: -1.263281082983738\n",
      "Loss for 2 qubits and 2 layers: -1.3182704771135845\n",
      "Loss for 2 qubits and 2 layers: -1.373047662061151\n",
      "Loss for 2 qubits and 2 layers: -1.4273259192970593\n",
      "Loss for 2 qubits and 2 layers: -1.4807883785129878\n",
      "Loss for 2 qubits and 2 layers: -1.5330919462486956\n",
      "Loss for 2 qubits and 2 layers: -1.583872078677931\n",
      "Loss for 2 qubits and 2 layers: -1.6327490520516124\n",
      "Loss for 2 qubits and 2 layers: -1.6793362768744453\n",
      "Loss for 2 qubits and 2 layers: -1.723251246441277\n",
      "Loss for 2 qubits and 2 layers: -1.7641297944109946\n",
      "Loss for 2 qubits and 2 layers: -1.8016443526272345\n",
      "Loss for 2 qubits and 2 layers: -1.835526628645873\n",
      "Loss for 2 qubits and 2 layers: -1.865594139536689\n",
      "Loss for 2 qubits and 2 layers: -1.8917777766058386\n",
      "Loss for 2 qubits and 2 layers: -1.9141438699614843\n",
      "Loss for 2 qubits and 2 layers: -1.9329005828958237\n",
      "Loss for 2 qubits and 2 layers: -1.9483789073315536\n",
      "Loss for 2 qubits and 2 layers: -1.9609868439052018\n",
      "Loss for 2 qubits and 2 layers: -1.971148839189515\n",
      "Loss for 2 qubits and 2 layers: -1.9792510637581762\n",
      "Loss for 2 qubits and 2 layers: -1.985609100233629\n",
      "Loss for 2 qubits and 2 layers: -1.9904619415274492\n",
      "Loss for 2 qubits and 2 layers: -1.9939852037564552\n",
      "Loss for 2 qubits and 2 layers: -1.996313043001388\n",
      "Loss for 2 qubits and 2 layers: -1.9975607564967492\n",
      "Loss for 2 qubits and 2 layers: -1.9978439864787996\n",
      "Loss for 2 qubits and 2 layers: -1.9972930793196961\n",
      "Loss for 2 qubits and 2 layers: -1.996062048900225\n",
      "Loss for 2 qubits and 2 layers: -1.9943314426389813\n",
      "Loss for 2 qubits and 2 layers: -1.992304157206946\n",
      "Loss for 2 qubits and 2 layers: -1.9901937472651843\n",
      "Loss for 2 qubits and 2 layers: -1.9882064226762748\n",
      "Loss for 2 qubits and 2 layers: -1.9865202595670888\n",
      "Loss for 2 qubits and 2 layers: -1.9852667952815972\n",
      "Loss for 2 qubits and 2 layers: -1.984519827299509\n",
      "Loss for 2 qubits and 2 layers: -1.9842938572157238\n",
      "Loss for 2 qubits and 2 layers: -1.984551565977512\n",
      "Loss for 2 qubits and 2 layers: -1.9852175196839326\n",
      "Loss for 2 qubits and 2 layers: -1.986194591511055\n",
      "Loss for 2 qubits and 2 layers: -1.987379979279806\n",
      "Loss for 2 qubits and 2 layers: -1.9886785364737407\n",
      "Loss for 2 qubits and 2 layers: -1.9900119899095765\n",
      "Loss for 2 qubits and 2 layers: -1.9913233946918198\n",
      "Loss for 2 qubits and 2 layers: -1.9925769434029947\n",
      "Loss for 2 qubits and 2 layers: -1.9937540243920557\n",
      "Loss for 2 qubits and 2 layers: -1.9948470861190495\n",
      "Loss for 2 qubits and 2 layers: -1.995853182408295\n",
      "Loss for 2 qubits and 2 layers: -1.996768884276018\n",
      "Loss for 2 qubits and 2 layers: -1.9975875979506748\n",
      "Loss for 2 qubits and 2 layers: -1.9982994835201762\n",
      "Loss for 2 qubits and 2 layers: -1.9988934351971142\n",
      "Loss for 2 qubits and 2 layers: -1.999360160987855\n",
      "Loss for 2 qubits and 2 layers: -1.9996953168617875\n",
      "Loss for 2 qubits and 2 layers: -1.9999018244656233\n",
      "Loss for 2 qubits and 2 layers: -1.9999908186532878\n",
      "Loss for 2 qubits and 2 layers: -1.9999810392420176\n",
      "Loss for 2 qubits and 2 layers: -1.9998968348643835\n",
      "Loss for 2 qubits and 2 layers: -1.9997652323767277\n",
      "Loss for 2 qubits and 2 layers: -1.9996126948827455\n",
      "Loss for 2 qubits and 2 layers: -1.9994622119831802\n",
      "Loss for 2 qubits and 2 layers: -1.9993312375622796\n",
      "Loss for 2 qubits and 2 layers: -1.999230754937904\n",
      "Loss for 2 qubits and 2 layers: -1.9991654773553753\n",
      "Loss for 2 qubits and 2 layers: -1.9991349567277221\n",
      "Loss for 2 qubits and 2 layers: -1.9991352258352169\n",
      "Loss for 2 qubits and 2 layers: -1.9991605567496\n",
      "Loss for 2 qubits and 2 layers: -1.999204970760922\n",
      "Loss for 2 qubits and 2 layers: -1.9992632553241283\n",
      "Loss for 2 qubits and 2 layers: -1.9993313970725564\n",
      "Loss for 2 qubits and 2 layers: -1.9994064910870266\n",
      "Loss for 2 qubits and 2 layers: -1.9994863028659609\n",
      "Loss for 2 qubits and 2 layers: -1.9995687169487337\n",
      "Loss for 2 qubits and 2 layers: -1.9996512957269659\n",
      "Loss for 2 qubits and 2 layers: -1.9997311025746853\n",
      "Loss for 2 qubits and 2 layers: -1.999804839763386\n",
      "Loss for 2 qubits and 2 layers: -1.9998692461312997\n",
      "Loss for 2 qubits and 2 layers: -1.9999216218424656\n",
      "Loss for 2 qubits and 2 layers: -1.9999603163619266\n",
      "Loss for 2 qubits and 2 layers: -1.9999850346852814\n",
      "Loss for 2 qubits and 2 layers: -1.9999968756853024\n",
      "Loss for 2 qubits and 2 layers: -1.9999980952568535\n",
      "Loss for 2 qubits and 2 layers: -1.9999916620453906\n",
      "Loss for 2 qubits and 2 layers: -1.9999807239209417\n",
      "Loss for 2 qubits and 2 layers: -1.9999681164574517\n",
      "Loss for 2 qubits and 2 layers: -1.9999560198103004\n",
      "Loss for 2 qubits and 2 layers: -1.999945818378924\n",
      "Loss for 2 qubits and 2 layers: -1.9999381566473988\n",
      "Loss for 2 qubits and 2 layers: -1.9999331336572002\n",
      "Loss for 2 qubits and 2 layers: -1.9999305514881396\n",
      "Loss for 2 qubits and 2 layers: -1.9999301347737362\n",
      "Loss for 2 qubits and 2 layers: -1.9999316642014588\n",
      "Loss for 2 qubits and 2 layers: -1.999935005875102\n",
      "Loss for 2 qubits and 2 layers: -1.999940056493954\n",
      "Loss for 2 qubits and 2 layers: -1.9999466496506004\n",
      "Loss for 3 qubits and 4 layers: -2.3883192978182413\n",
      "Loss for 3 qubits and 4 layers: -2.539732887246283\n",
      "Loss for 3 qubits and 4 layers: -2.6724719699337895\n",
      "Loss for 3 qubits and 4 layers: -2.7833693440120375\n",
      "Loss for 3 qubits and 4 layers: -2.870180478207947\n",
      "Loss for 3 qubits and 4 layers: -2.9323872229166876\n",
      "Loss for 3 qubits and 4 layers: -2.971717114804796\n",
      "Loss for 3 qubits and 4 layers: -2.991908954040383\n",
      "Loss for 3 qubits and 4 layers: -2.997277303038896\n",
      "Loss for 3 qubits and 4 layers: -2.99182128943455\n",
      "Loss for 3 qubits and 4 layers: -2.97952892620918\n",
      "Loss for 3 qubits and 4 layers: -2.9645683603304605\n",
      "Loss for 3 qubits and 4 layers: -2.9508882099995164\n",
      "Loss for 3 qubits and 4 layers: -2.9414728126804284\n",
      "Loss for 3 qubits and 4 layers: -2.937795192146715\n",
      "Loss for 3 qubits and 4 layers: -2.9398329186727725\n",
      "Loss for 3 qubits and 4 layers: -2.946505893750969\n",
      "Loss for 3 qubits and 4 layers: -2.9561986158338476\n",
      "Loss for 3 qubits and 4 layers: -2.967178363444323\n",
      "Loss for 3 qubits and 4 layers: -2.977878856377716\n",
      "Loss for 3 qubits and 4 layers: -2.987074706978297\n",
      "Loss for 3 qubits and 4 layers: -2.9939720225409174\n",
      "Loss for 3 qubits and 4 layers: -2.998229976276851\n",
      "Loss for 3 qubits and 4 layers: -2.9999234930994136\n",
      "Loss for 3 qubits and 4 layers: -2.99945946468989\n",
      "Loss for 3 qubits and 4 layers: -2.997463400005315\n",
      "Loss for 3 qubits and 4 layers: -2.9946553183384177\n",
      "Loss for 3 qubits and 4 layers: -2.9917321068639957\n",
      "Loss for 3 qubits and 4 layers: -2.9892710457663645\n",
      "Loss for 3 qubits and 4 layers: -2.987666536937863\n",
      "Loss for 3 qubits and 4 layers: -2.987106452232044\n",
      "Loss for 3 qubits and 4 layers: -2.9875852891774786\n",
      "Loss for 3 qubits and 4 layers: -2.9889431245725016\n",
      "Loss for 3 qubits and 4 layers: -2.990916747424175\n",
      "Loss for 3 qubits and 4 layers: -2.9931921457267303\n",
      "Loss for 3 qubits and 4 layers: -2.995452446847901\n",
      "Loss for 3 qubits and 4 layers: -2.9974193306681274\n",
      "Loss for 3 qubits and 4 layers: -2.998887121767166\n",
      "Loss for 3 qubits and 4 layers: -2.9997472693560057\n",
      "Loss for 3 qubits and 4 layers: -2.999998728623331\n",
      "Loss for 3 qubits and 4 layers: -2.9997398307044905\n",
      "Loss for 3 qubits and 4 layers: -2.999141218511123\n",
      "Loss for 3 qubits and 4 layers: -2.998405796860558\n",
      "Loss for 3 qubits and 4 layers: -2.9977263846152047\n",
      "Loss for 3 qubits and 4 layers: -2.9972516291557194\n",
      "Loss for 3 qubits and 4 layers: -2.997066190323906\n",
      "Loss for 3 qubits and 4 layers: -2.9971857894012732\n",
      "Loss for 3 qubits and 4 layers: -2.9975646508215354\n",
      "Loss for 3 qubits and 4 layers: -2.998112322815565\n",
      "Loss for 3 qubits and 4 layers: -2.998716803345346\n",
      "Loss for 3 qubits and 4 layers: -2.9992698678959684\n",
      "Loss for 3 qubits and 4 layers: -2.9996891632713827\n",
      "Loss for 3 qubits and 4 layers: -2.999931783900572\n",
      "Loss for 3 qubits and 4 layers: -2.9999966198081127\n",
      "Loss for 3 qubits and 4 layers: -2.9999167600486754\n",
      "Loss for 3 qubits and 4 layers: -2.9997462951897527\n",
      "Loss for 3 qubits and 4 layers: -2.9995462146562892\n",
      "Loss for 3 qubits and 4 layers: -2.999372221946527\n",
      "Loss for 3 qubits and 4 layers: -2.999265378741173\n",
      "Loss for 3 qubits and 4 layers: -2.9992460672502257\n",
      "Loss for 3 qubits and 4 layers: -2.9993122592142654\n",
      "Loss for 3 qubits and 4 layers: -2.999442861666763\n",
      "Loss for 3 qubits and 4 layers: -2.999605353105242\n",
      "Loss for 3 qubits and 4 layers: -2.9997651459572587\n",
      "Loss for 3 qubits and 4 layers: -2.999893670140975\n",
      "Loss for 3 qubits and 4 layers: -2.99997336762935\n",
      "Loss for 3 qubits and 4 layers: -2.9999994433087265\n",
      "Loss for 3 qubits and 4 layers: -2.9999790347665707\n",
      "Loss for 3 qubits and 4 layers: -2.999928325000276\n",
      "Loss for 3 qubits and 4 layers: -2.9998679441361373\n",
      "Loss for 3 qubits and 4 layers: -2.9998174770728245\n",
      "Loss for 3 qubits and 4 layers: -2.999790624016023\n",
      "Loss for 3 qubits and 4 layers: -2.999792594239773\n",
      "Loss for 3 qubits and 4 layers: -2.9998203266205095\n",
      "Loss for 3 qubits and 4 layers: -2.9998648569712643\n",
      "Loss for 3 qubits and 4 layers: -2.9999145606841315\n",
      "Loss for 3 qubits and 4 layers: -2.9999582767179214\n",
      "Loss for 3 qubits and 4 layers: -2.999987837337702\n",
      "Loss for 3 qubits and 4 layers: -2.9999997015536195\n",
      "Loss for 3 qubits and 4 layers: -2.9999953236827213\n",
      "Loss for 3 qubits and 4 layers: -2.9999800806219143\n",
      "Loss for 3 qubits and 4 layers: -2.9999611548872216\n",
      "Loss for 3 qubits and 4 layers: -2.999945254286743\n",
      "Loss for 3 qubits and 4 layers: -2.9999369713377377\n",
      "Loss for 3 qubits and 4 layers: -2.999938066057635\n",
      "Loss for 3 qubits and 4 layers: -2.999947527767029\n",
      "Loss for 3 qubits and 4 layers: -2.9999622135126125\n",
      "Loss for 3 qubits and 4 layers: -2.999977938666429\n",
      "Loss for 3 qubits and 4 layers: -2.9999907964907297\n",
      "Loss for 3 qubits and 4 layers: -2.999998280506494\n",
      "Loss for 3 qubits and 4 layers: -2.9999997984313747\n",
      "Loss for 3 qubits and 4 layers: -2.999996484987277\n",
      "Loss for 3 qubits and 4 layers: -2.999990558835407\n",
      "Loss for 3 qubits and 4 layers: -2.9999845465394666\n",
      "Loss for 3 qubits and 4 layers: -2.9999805700975295\n",
      "Loss for 3 qubits and 4 layers: -2.999979807636921\n",
      "Loss for 3 qubits and 4 layers: -2.999982256742233\n",
      "Loss for 3 qubits and 4 layers: -2.999986899851432\n",
      "Loss for 3 qubits and 4 layers: -2.999992199926873\n",
      "Loss for 3 qubits and 4 layers: -2.9999966824157522\n",
      "Loss for 4 qubits and 5 layers: -2.1716314285778435\n",
      "Loss for 4 qubits and 5 layers: -2.5764961733601357\n",
      "Loss for 4 qubits and 5 layers: -2.9448417036270795\n",
      "Loss for 4 qubits and 5 layers: -3.2521962693717548\n",
      "Loss for 4 qubits and 5 layers: -3.4906137401392305\n",
      "Loss for 4 qubits and 5 layers: -3.6655387857876525\n",
      "Loss for 4 qubits and 5 layers: -3.7884919472414667\n",
      "Loss for 4 qubits and 5 layers: -3.8710379957616103\n",
      "Loss for 4 qubits and 5 layers: -3.921997241072048\n",
      "Loss for 4 qubits and 5 layers: -3.9483991019208857\n",
      "Loss for 4 qubits and 5 layers: -3.957489818801255\n",
      "Loss for 4 qubits and 5 layers: -3.9564744124876214\n",
      "Loss for 4 qubits and 5 layers: -3.950131278336487\n",
      "Loss for 4 qubits and 5 layers: -3.940196932854188\n",
      "Loss for 4 qubits and 5 layers: -3.9273676283972874\n",
      "Loss for 4 qubits and 5 layers: -3.9131821093828725\n",
      "Loss for 4 qubits and 5 layers: -3.9001596450684906\n",
      "Loss for 4 qubits and 5 layers: -3.8907004212152785\n",
      "Loss for 4 qubits and 5 layers: -3.886098686948332\n",
      "Loss for 4 qubits and 5 layers: -3.886448578381322\n",
      "Loss for 4 qubits and 5 layers: -3.891199611850418\n",
      "Loss for 4 qubits and 5 layers: -3.899662908678336\n",
      "Loss for 4 qubits and 5 layers: -3.911026097196403\n",
      "Loss for 4 qubits and 5 layers: -3.9240440042589735\n",
      "Loss for 4 qubits and 5 layers: -3.936985619621652\n",
      "Loss for 4 qubits and 5 layers: -3.9481760608896215\n",
      "Loss for 4 qubits and 5 layers: -3.9567834616119395\n",
      "Loss for 4 qubits and 5 layers: -3.9631097817398384\n",
      "Loss for 4 qubits and 5 layers: -3.9680744108053854\n",
      "Loss for 4 qubits and 5 layers: -3.972458435831751\n",
      "Loss for 4 qubits and 5 layers: -3.9766537379530797\n",
      "Loss for 4 qubits and 5 layers: -3.980888500215138\n",
      "Loss for 4 qubits and 5 layers: -3.98534592904456\n",
      "Loss for 4 qubits and 5 layers: -3.9899405992495254\n",
      "Loss for 4 qubits and 5 layers: -3.9941227364566534\n",
      "Loss for 4 qubits and 5 layers: -3.997087038551458\n",
      "Loss for 4 qubits and 5 layers: -3.998263366837348\n",
      "Loss for 4 qubits and 5 layers: -3.997646209400526\n",
      "Loss for 4 qubits and 5 layers: -3.9957117624139595\n",
      "Loss for 4 qubits and 5 layers: -3.9931164232765193\n",
      "Loss for 4 qubits and 5 layers: -3.9905117862787014\n",
      "Loss for 4 qubits and 5 layers: -3.988510774278015\n",
      "Loss for 4 qubits and 5 layers: -3.9876008608370594\n",
      "Loss for 4 qubits and 5 layers: -3.9879544830555425\n",
      "Loss for 4 qubits and 5 layers: -3.9893321149211007\n",
      "Loss for 4 qubits and 5 layers: -3.9912266091217212\n",
      "Loss for 4 qubits and 5 layers: -3.9931394986468733\n",
      "Loss for 4 qubits and 5 layers: -3.9947588898701234\n",
      "Loss for 4 qubits and 5 layers: -3.995955603871445\n",
      "Loss for 4 qubits and 5 layers: -3.996716914550138\n",
      "Loss for 4 qubits and 5 layers: -3.997134903318018\n",
      "Loss for 4 qubits and 5 layers: -3.997403566449802\n",
      "Loss for 4 qubits and 5 layers: -3.997728523005913\n",
      "Loss for 4 qubits and 5 layers: -3.9981946097189445\n",
      "Loss for 4 qubits and 5 layers: -3.9987319808958355\n",
      "Loss for 4 qubits and 5 layers: -3.9992067115743426\n",
      "Loss for 4 qubits and 5 layers: -3.9995161781252784\n",
      "Loss for 4 qubits and 5 layers: -3.999606344829888\n",
      "Loss for 4 qubits and 5 layers: -3.9994607887246794\n",
      "Loss for 4 qubits and 5 layers: -3.9991239530640197\n",
      "Loss for 4 qubits and 5 layers: -3.9987193471507543\n",
      "Loss for 4 qubits and 5 layers: -3.9983993747426476\n",
      "Loss for 4 qubits and 5 layers: -3.9982614823492932\n",
      "Loss for 4 qubits and 5 layers: -3.9983174873104392\n",
      "Loss for 4 qubits and 5 layers: -3.9985257990911807\n",
      "Loss for 4 qubits and 5 layers: -3.9988234631169597\n",
      "Loss for 4 qubits and 5 layers: -3.9991334517949566\n",
      "Loss for 4 qubits and 5 layers: -3.9993823517552887\n",
      "Loss for 4 qubits and 5 layers: -3.9995371354231053\n",
      "Loss for 4 qubits and 5 layers: -3.999617364327288\n",
      "Loss for 4 qubits and 5 layers: -3.999664057021138\n",
      "Loss for 4 qubits and 5 layers: -3.9997060387497507\n",
      "Loss for 4 qubits and 5 layers: -3.99975609449418\n",
      "Loss for 4 qubits and 5 layers: -3.9998171199403467\n",
      "Loss for 4 qubits and 5 layers: -3.9998774800557735\n",
      "Loss for 4 qubits and 5 layers: -3.9999126145277915\n",
      "Loss for 4 qubits and 5 layers: -3.99990553145009\n",
      "Loss for 4 qubits and 5 layers: -3.9998625187995978\n",
      "Loss for 4 qubits and 5 layers: -3.9998048305375344\n",
      "Loss for 4 qubits and 5 layers: -3.999753318414051\n",
      "Loss for 4 qubits and 5 layers: -3.99972452439224\n",
      "Loss for 4 qubits and 5 layers: -3.9997293343840368\n",
      "Loss for 4 qubits and 5 layers: -3.999765634234433\n",
      "Loss for 4 qubits and 5 layers: -3.999817511784032\n",
      "Loss for 4 qubits and 5 layers: -3.999867500751961\n",
      "Loss for 4 qubits and 5 layers: -3.999906643051519\n",
      "Loss for 4 qubits and 5 layers: -3.9999327996530116\n",
      "Loss for 4 qubits and 5 layers: -3.9999471762480683\n",
      "Loss for 4 qubits and 5 layers: -3.9999549286950864\n",
      "Loss for 4 qubits and 5 layers: -3.999962469026285\n",
      "Loss for 4 qubits and 5 layers: -3.9999712679423607\n",
      "Loss for 4 qubits and 5 layers: -3.9999777737285527\n",
      "Loss for 4 qubits and 5 layers: -3.999979064453458\n",
      "Loss for 4 qubits and 5 layers: -3.9999749819503383\n",
      "Loss for 4 qubits and 5 layers: -3.9999666004177823\n",
      "Loss for 4 qubits and 5 layers: -3.999956669081293\n",
      "Loss for 4 qubits and 5 layers: -3.9999498755713\n",
      "Loss for 4 qubits and 5 layers: -3.9999494582557715\n",
      "Loss for 4 qubits and 5 layers: -3.999954787929618\n",
      "Loss for 4 qubits and 5 layers: -3.9999633271016117\n",
      "Loss for 5 qubits and 7 layers: -2.860268659348657\n",
      "Loss for 5 qubits and 7 layers: -3.9537900256848775\n",
      "Loss for 5 qubits and 7 layers: -4.561838441510093\n",
      "Loss for 5 qubits and 7 layers: -4.641467967601425\n",
      "Loss for 5 qubits and 7 layers: -4.466204054416204\n",
      "Loss for 5 qubits and 7 layers: -4.298407981784261\n",
      "Loss for 5 qubits and 7 layers: -4.236658494953213\n",
      "Loss for 5 qubits and 7 layers: -4.288885111961178\n",
      "Loss for 5 qubits and 7 layers: -4.429121021610622\n",
      "Loss for 5 qubits and 7 layers: -4.616157299463944\n",
      "Loss for 5 qubits and 7 layers: -4.800985428491268\n",
      "Loss for 5 qubits and 7 layers: -4.936374447587301\n",
      "Loss for 5 qubits and 7 layers: -4.9919184827653105\n",
      "Loss for 5 qubits and 7 layers: -4.967874209966463\n",
      "Loss for 5 qubits and 7 layers: -4.895026406352883\n",
      "Loss for 5 qubits and 7 layers: -4.816908710457096\n",
      "Loss for 5 qubits and 7 layers: -4.767893418336046\n",
      "Loss for 5 qubits and 7 layers: -4.7622263394316455\n",
      "Loss for 5 qubits and 7 layers: -4.795346106102533\n",
      "Loss for 5 qubits and 7 layers: -4.850766761437207\n",
      "Loss for 5 qubits and 7 layers: -4.907905436488058\n",
      "Loss for 5 qubits and 7 layers: -4.949401666693089\n",
      "Loss for 5 qubits and 7 layers: -4.966695346180382\n",
      "Loss for 5 qubits and 7 layers: -4.961926147046937\n",
      "Loss for 5 qubits and 7 layers: -4.9451399157455\n",
      "Loss for 5 qubits and 7 layers: -4.928477314675168\n",
      "Loss for 5 qubits and 7 layers: -4.920855602136749\n",
      "Loss for 5 qubits and 7 layers: -4.925519156095907\n",
      "Loss for 5 qubits and 7 layers: -4.9403924106867905\n",
      "Loss for 5 qubits and 7 layers: -4.96001231625884\n",
      "Loss for 5 qubits and 7 layers: -4.9779985182439805\n",
      "Loss for 5 qubits and 7 layers: -4.989375796218534\n",
      "Loss for 5 qubits and 7 layers: -4.992142480868912\n",
      "Loss for 5 qubits and 7 layers: -4.987588530080885\n",
      "Loss for 5 qubits and 7 layers: -4.979328738572889\n",
      "Loss for 5 qubits and 7 layers: -4.971612001689795\n",
      "Loss for 5 qubits and 7 layers: -4.967703614863495\n",
      "Loss for 5 qubits and 7 layers: -4.968919188876876\n",
      "Loss for 5 qubits and 7 layers: -4.974508215878979\n",
      "Loss for 5 qubits and 7 layers: -4.982270245754139\n",
      "Loss for 5 qubits and 7 layers: -4.989584607787502\n",
      "Loss for 5 qubits and 7 layers: -4.994440426011391\n",
      "Loss for 5 qubits and 7 layers: -4.996084769658051\n",
      "Loss for 5 qubits and 7 layers: -4.995081308611655\n",
      "Loss for 5 qubits and 7 layers: -4.992838989379371\n",
      "Loss for 5 qubits and 7 layers: -4.990892990358685\n",
      "Loss for 5 qubits and 7 layers: -4.9902759614318795\n",
      "Loss for 5 qubits and 7 layers: -4.99120917598485\n",
      "Loss for 5 qubits and 7 layers: -4.993171608727441\n",
      "Loss for 5 qubits and 7 layers: -4.995257819078645\n",
      "Loss for 5 qubits and 7 layers: -4.996644641993375\n",
      "Loss for 5 qubits and 7 layers: -4.996962118935686\n",
      "Loss for 5 qubits and 7 layers: -4.9964152887232505\n",
      "Loss for 5 qubits and 7 layers: -4.995619808716145\n",
      "Loss for 5 qubits and 7 layers: -4.995249492725299\n",
      "Loss for 5 qubits and 7 layers: -4.995682654667617\n",
      "Loss for 5 qubits and 7 layers: -4.996828173881605\n",
      "Loss for 5 qubits and 7 layers: -4.998210458633448\n",
      "Loss for 5 qubits and 7 layers: -4.999249788327548\n",
      "Loss for 5 qubits and 7 layers: -4.999572911780062\n",
      "Loss for 5 qubits and 7 layers: -4.999185678082933\n",
      "Loss for 5 qubits and 7 layers: -4.998430736682828\n",
      "Loss for 5 qubits and 7 layers: -4.9977780305123565\n",
      "Loss for 5 qubits and 7 layers: -4.997578586038079\n",
      "Loss for 5 qubits and 7 layers: -4.9979123188168195\n",
      "Loss for 5 qubits and 7 layers: -4.99859190580556\n",
      "Loss for 5 qubits and 7 layers: -4.999296063200976\n",
      "Loss for 5 qubits and 7 layers: -4.999745668334413\n",
      "Loss for 5 qubits and 7 layers: -4.999829443257565\n",
      "Loss for 5 qubits and 7 layers: -4.999626430517333\n",
      "Loss for 5 qubits and 7 layers: -4.999332287769301\n",
      "Loss for 5 qubits and 7 layers: -4.999142344447729\n",
      "Loss for 5 qubits and 7 layers: -4.999155998163349\n",
      "Loss for 5 qubits and 7 layers: -4.999345409608837\n",
      "Loss for 5 qubits and 7 layers: -4.999593457230257\n",
      "Loss for 5 qubits and 7 layers: -4.999771641642591\n",
      "Loss for 5 qubits and 7 layers: -4.999812463524551\n",
      "Loss for 5 qubits and 7 layers: -4.999738310926364\n",
      "Loss for 5 qubits and 7 layers: -4.999635619648977\n",
      "Loss for 5 qubits and 7 layers: -4.999594764219922\n",
      "Loss for 5 qubits and 7 layers: -4.999654552309643\n",
      "Loss for 5 qubits and 7 layers: -4.999784278994782\n",
      "Loss for 5 qubits and 7 layers: -4.999910525917355\n",
      "Loss for 5 qubits and 7 layers: -4.999967583683336\n",
      "Loss for 5 qubits and 7 layers: -4.999937637708326\n",
      "Loss for 5 qubits and 7 layers: -4.999856649076618\n",
      "Loss for 5 qubits and 7 layers: -4.999786132513959\n",
      "Loss for 5 qubits and 7 layers: -4.999772717662257\n",
      "Loss for 5 qubits and 7 layers: -4.99982238730351\n",
      "Loss for 5 qubits and 7 layers: -4.999903216089528\n",
      "Loss for 5 qubits and 7 layers: -4.999970415949029\n",
      "Loss for 5 qubits and 7 layers: -4.999994697436703\n",
      "Loss for 5 qubits and 7 layers: -4.999976309954213\n",
      "Loss for 5 qubits and 7 layers: -4.999939089381218\n",
      "Loss for 5 qubits and 7 layers: -4.999911907502016\n",
      "Loss for 5 qubits and 7 layers: -4.9999109237009325\n",
      "Loss for 5 qubits and 7 layers: -4.99993290305073\n",
      "Loss for 5 qubits and 7 layers: -4.999961373216763\n",
      "Loss for 5 qubits and 7 layers: -4.999979529063536\n",
      "Loss for 5 qubits and 7 layers: -4.999980866058902\n",
      "Loss for 6 qubits and 7 layers: -3.5097078679534683\n",
      "Loss for 6 qubits and 7 layers: -4.0786014418914895\n",
      "Loss for 6 qubits and 7 layers: -4.516044594704065\n",
      "Loss for 6 qubits and 7 layers: -4.890275275500508\n",
      "Loss for 6 qubits and 7 layers: -5.2393816558117905\n",
      "Loss for 6 qubits and 7 layers: -5.525696580519105\n",
      "Loss for 6 qubits and 7 layers: -5.695616817457618\n",
      "Loss for 6 qubits and 7 layers: -5.750130018012357\n",
      "Loss for 6 qubits and 7 layers: -5.73735913704069\n",
      "Loss for 6 qubits and 7 layers: -5.725579164562301\n",
      "Loss for 6 qubits and 7 layers: -5.749246676618521\n",
      "Loss for 6 qubits and 7 layers: -5.790812762645848\n",
      "Loss for 6 qubits and 7 layers: -5.816796793181877\n",
      "Loss for 6 qubits and 7 layers: -5.813091428526732\n",
      "Loss for 6 qubits and 7 layers: -5.789885336133636\n",
      "Loss for 6 qubits and 7 layers: -5.772141023037473\n",
      "Loss for 6 qubits and 7 layers: -5.782912716999277\n",
      "Loss for 6 qubits and 7 layers: -5.827373960533391\n",
      "Loss for 6 qubits and 7 layers: -5.890174388420689\n",
      "Loss for 6 qubits and 7 layers: -5.946083210306594\n",
      "Loss for 6 qubits and 7 layers: -5.974881415702127\n",
      "Loss for 6 qubits and 7 layers: -5.972403837516776\n",
      "Loss for 6 qubits and 7 layers: -5.952057581399695\n",
      "Loss for 6 qubits and 7 layers: -5.934341829129151\n",
      "Loss for 6 qubits and 7 layers: -5.931936861720664\n",
      "Loss for 6 qubits and 7 layers: -5.943428795602694\n",
      "Loss for 6 qubits and 7 layers: -5.958372836006651\n",
      "Loss for 6 qubits and 7 layers: -5.966540637580824\n",
      "Loss for 6 qubits and 7 layers: -5.964469178710374\n",
      "Loss for 6 qubits and 7 layers: -5.9562910148720825\n",
      "Loss for 6 qubits and 7 layers: -5.949695555724512\n",
      "Loss for 6 qubits and 7 layers: -5.950578392590115\n",
      "Loss for 6 qubits and 7 layers: -5.959839235979629\n",
      "Loss for 6 qubits and 7 layers: -5.973510344533476\n",
      "Loss for 6 qubits and 7 layers: -5.985456868746081\n",
      "Loss for 6 qubits and 7 layers: -5.991049942783349\n",
      "Loss for 6 qubits and 7 layers: -5.989853654617969\n",
      "Loss for 6 qubits and 7 layers: -5.985632201999349\n",
      "Loss for 6 qubits and 7 layers: -5.983434855678361\n",
      "Loss for 6 qubits and 7 layers: -5.9857449521061215\n",
      "Loss for 6 qubits and 7 layers: -5.990773485342471\n",
      "Loss for 6 qubits and 7 layers: -5.994337919789094\n",
      "Loss for 6 qubits and 7 layers: -5.993663184106017\n",
      "Loss for 6 qubits and 7 layers: -5.989791979286892\n",
      "Loss for 6 qubits and 7 layers: -5.986517168470204\n",
      "Loss for 6 qubits and 7 layers: -5.986917482133089\n",
      "Loss for 6 qubits and 7 layers: -5.99080309134429\n",
      "Loss for 6 qubits and 7 layers: -5.995286647646849\n",
      "Loss for 6 qubits and 7 layers: -5.997575757866766\n",
      "Loss for 6 qubits and 7 layers: -5.997220735304705\n",
      "Loss for 6 qubits and 7 layers: -5.995933640310677\n",
      "Loss for 6 qubits and 7 layers: -5.9955906227571845\n",
      "Loss for 6 qubits and 7 layers: -5.996585808745257\n",
      "Loss for 6 qubits and 7 layers: -5.997895956962854\n",
      "Loss for 6 qubits and 7 layers: -5.998346736106787\n",
      "Loss for 6 qubits and 7 layers: -5.9976844760341725\n",
      "Loss for 6 qubits and 7 layers: -5.996628677241809\n",
      "Loss for 6 qubits and 7 layers: -5.996141489180349\n",
      "Loss for 6 qubits and 7 layers: -5.996659760200817\n",
      "Loss for 6 qubits and 7 layers: -5.997843021947813\n",
      "Loss for 6 qubits and 7 layers: -5.998921373513801\n",
      "Loss for 6 qubits and 7 layers: -5.999324099159243\n",
      "Loss for 6 qubits and 7 layers: -5.9990993158950765\n",
      "Loss for 6 qubits and 7 layers: -5.9987877241603655\n",
      "Loss for 6 qubits and 7 layers: -5.998861915033043\n",
      "Loss for 6 qubits and 7 layers: -5.999268507069621\n",
      "Loss for 6 qubits and 7 layers: -5.999542042518246\n",
      "Loss for 6 qubits and 7 layers: -5.999363216177016\n",
      "Loss for 6 qubits and 7 layers: -5.9989326368612375\n",
      "Loss for 6 qubits and 7 layers: -5.998738812730988\n",
      "Loss for 6 qubits and 7 layers: -5.999011598512097\n",
      "Loss for 6 qubits and 7 layers: -5.999503567272364\n",
      "Loss for 6 qubits and 7 layers: -5.999808069005533\n",
      "Loss for 6 qubits and 7 layers: -5.999789207851918\n",
      "Loss for 6 qubits and 7 layers: -5.9996432414929295\n",
      "Loss for 6 qubits and 7 layers: -5.9996088204675\n",
      "Loss for 6 qubits and 7 layers: -5.99971584082177\n",
      "Loss for 6 qubits and 7 layers: -5.999818084234448\n",
      "Loss for 6 qubits and 7 layers: -5.9997952634851615\n",
      "Loss for 6 qubits and 7 layers: -5.999679904831978\n",
      "Loss for 6 qubits and 7 layers: -5.9996070991013895\n",
      "Loss for 6 qubits and 7 layers: -5.999668901652463\n",
      "Loss for 6 qubits and 7 layers: -5.999822338250301\n",
      "Loss for 6 qubits and 7 layers: -5.99993820768866\n",
      "Loss for 6 qubits and 7 layers: -5.999939557183959\n",
      "Loss for 6 qubits and 7 layers: -5.999877807995885\n",
      "Loss for 6 qubits and 7 layers: -5.999857489627374\n",
      "Loss for 6 qubits and 7 layers: -5.999904644745761\n",
      "Loss for 6 qubits and 7 layers: -5.999947392062641\n",
      "Loss for 6 qubits and 7 layers: -5.9999255751441485\n",
      "Loss for 6 qubits and 7 layers: -5.999873128027747\n",
      "Loss for 6 qubits and 7 layers: -5.999865367836017\n",
      "Loss for 6 qubits and 7 layers: -5.999916621052727\n",
      "Loss for 6 qubits and 7 layers: -5.999970235835917\n",
      "Loss for 6 qubits and 7 layers: -5.999979254292123\n",
      "Loss for 6 qubits and 7 layers: -5.9999581809255105\n",
      "Loss for 6 qubits and 7 layers: -5.999948690289531\n",
      "Loss for 6 qubits and 7 layers: -5.999963141754339\n",
      "Loss for 6 qubits and 7 layers: -5.999978895494911\n",
      "Loss for 6 qubits and 7 layers: -5.999975220169722\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def setup_circuit(num_qubits):\n",
    "    dev = qml.device('default.qubit', wires=num_qubits)\n",
    "    edges = [(i, j) for i in range(num_qubits) for j in range(i + 1, num_qubits)]\n",
    "    nodes = range(num_qubits)\n",
    "    h1 = qml.Hamiltonian([1.0 for _ in edges], [qml.PauliZ(i) @ qml.PauliZ(j) for (i, j) in edges])\n",
    "    h2 = qml.Hamiltonian([1.0 for _ in nodes], [qml.PauliX(i) for i in nodes])\n",
    "    stab1 = qml.Hamiltonian([1], [qml.operation.Tensor(*(qml.PauliX(i) for i in nodes))])\n",
    "    stab2 = qml.Hamiltonian([1 for _ in range(len(nodes) - 1)], [qml.PauliZ(i) @ qml.PauliZ(i + 1) for i in range(len(nodes) - 1)])\n",
    "    stab = stab1 + stab2\n",
    "    return dev, h1, h2, stab\n",
    "\n",
    "def test_circuit(num_qubits, P):\n",
    "    dev, h1, h2, stab = setup_circuit(num_qubits)\n",
    "    params = np.random.random(2 * P)\n",
    "    opt = qml.AdamOptimizer(stepsize=0.01)\n",
    "\n",
    "    @qml.qnode(dev)\n",
    "    def circuit(params):\n",
    "        for i in range(num_qubits):\n",
    "            qml.Hadamard(wires=i)\n",
    "        for p in range(P):\n",
    "            qml.templates.ApproxTimeEvolution(h1, params[2 * p], 1)\n",
    "            qml.templates.ApproxTimeEvolution(h2, params[2 * p + 1], 1)\n",
    "        return qml.expval(stab)\n",
    "\n",
    "    for _ in range(100):\n",
    "        params = opt.step(lambda x: -circuit(x), params)\n",
    "        loss = -circuit(params)\n",
    "        print(f\"Loss for {num_qubits} qubits and {P} layers: {loss}\")\n",
    "\n",
    "# Values for n and corresponding P\n",
    "n_and_p_values = [(2, 2), (3, 2), (4, 3), (5, 7), (6, 7)]\n",
    "\n",
    "for n, p in n_and_p_values:\n",
    "    test_circuit(n, p)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for 2 qubits and 4 layers: -1.3505953259991932\n",
      "Loss for 2 qubits and 4 layers: -1.4563998652419898\n",
      "Loss for 2 qubits and 4 layers: -1.5557206895854767\n",
      "Loss for 2 qubits and 4 layers: -1.6472179785283527\n",
      "Loss for 2 qubits and 4 layers: -1.729342194407871\n",
      "Loss for 2 qubits and 4 layers: -1.8006893603894962\n",
      "Loss for 2 qubits and 4 layers: -1.8601728289551112\n",
      "Loss for 2 qubits and 4 layers: -1.9071738787265988\n",
      "Loss for 2 qubits and 4 layers: -1.9416854451541177\n",
      "Loss for 2 qubits and 4 layers: -1.964418985218047\n",
      "Loss for 2 qubits and 4 layers: -1.9768258992445218\n",
      "Loss for 2 qubits and 4 layers: -1.9809858513832355\n",
      "Loss for 2 qubits and 4 layers: -1.9793513902605264\n",
      "Loss for 2 qubits and 4 layers: -1.9744045308348257\n",
      "Loss for 2 qubits and 4 layers: -1.9683297343510169\n",
      "Loss for 2 qubits and 4 layers: -1.9627946316894693\n",
      "Loss for 2 qubits and 4 layers: -1.9588689192900506\n",
      "Loss for 2 qubits and 4 layers: -1.9570562344015427\n",
      "Loss for 2 qubits and 4 layers: -1.957393278637582\n",
      "Loss for 2 qubits and 4 layers: -1.9595768435039327\n",
      "Loss for 2 qubits and 4 layers: -1.963093804209699\n",
      "Loss for 2 qubits and 4 layers: -1.967340801645346\n",
      "Loss for 2 qubits and 4 layers: -1.971726894054821\n",
      "Loss for 2 qubits and 4 layers: -1.975755129725357\n",
      "Loss for 2 qubits and 4 layers: -1.9790796029585405\n",
      "Loss for 2 qubits and 4 layers: -1.9815347746508585\n",
      "Loss for 2 qubits and 4 layers: -1.9831350545374746\n",
      "Loss for 2 qubits and 4 layers: -1.9840457136827263\n",
      "Loss for 2 qubits and 4 layers: -1.9845309405222384\n",
      "Loss for 2 qubits and 4 layers: -1.984889744557545\n",
      "Loss for 2 qubits and 4 layers: -1.9853931495387256\n",
      "Loss for 2 qubits and 4 layers: -1.9862350794579853\n",
      "Loss for 2 qubits and 4 layers: -1.9875048108970577\n",
      "Loss for 2 qubits and 4 layers: -1.9891828632619064\n",
      "Loss for 2 qubits and 4 layers: -1.9911570486101104\n",
      "Loss for 2 qubits and 4 layers: -1.9932523962548836\n",
      "Loss for 2 qubits and 4 layers: -1.9952677898891902\n",
      "Loss for 2 qubits and 4 layers: -1.9970126763386402\n",
      "Loss for 2 qubits and 4 layers: -1.998338378130224\n",
      "Loss for 2 qubits and 4 layers: -1.9991599535463935\n",
      "Loss for 2 qubits and 4 layers: -1.9994661108706566\n",
      "Loss for 2 qubits and 4 layers: -1.9993164486636044\n",
      "Loss for 2 qubits and 4 layers: -1.998827235866817\n",
      "Loss for 2 qubits and 4 layers: -1.9981488312366067\n",
      "Loss for 2 qubits and 4 layers: -1.9974392497457725\n",
      "Loss for 2 qubits and 4 layers: -1.9968388845345073\n",
      "Loss for 2 qubits and 4 layers: -1.996450792350379\n",
      "Loss for 2 qubits and 4 layers: -1.9963294284044446\n",
      "Loss for 2 qubits and 4 layers: -1.9964787551484138\n",
      "Loss for 2 qubits and 4 layers: -1.9968588004276207\n",
      "Loss for 2 qubits and 4 layers: -1.9973983992407223\n",
      "Loss for 2 qubits and 4 layers: -1.9980111678953023\n",
      "Loss for 2 qubits and 4 layers: -1.9986116799262958\n",
      "Loss for 2 qubits and 4 layers: -1.999129201003283\n",
      "Loss for 2 qubits and 4 layers: -1.9995170517429504\n",
      "Loss for 2 qubits and 4 layers: -1.9997565809679074\n",
      "Loss for 2 qubits and 4 layers: -1.9998557299868578\n",
      "Loss for 2 qubits and 4 layers: -1.9998431130356504\n",
      "Loss for 2 qubits and 4 layers: -1.9997592711057708\n",
      "Loss for 2 qubits and 4 layers: -1.999647129759786\n",
      "Loss for 2 qubits and 4 layers: -1.999543626265607\n",
      "Loss for 2 qubits and 4 layers: -1.9994739951843057\n",
      "Loss for 2 qubits and 4 layers: -1.9994494509159084\n",
      "Loss for 2 qubits and 4 layers: -1.999468182417981\n",
      "Loss for 2 qubits and 4 layers: -1.9995188800954458\n",
      "Loss for 2 qubits and 4 layers: -1.9995855901043926\n",
      "Loss for 2 qubits and 4 layers: -1.999652593518352\n",
      "Loss for 2 qubits and 4 layers: -1.9997082107832018\n",
      "Loss for 2 qubits and 4 layers: -1.9997468486671606\n",
      "Loss for 2 qubits and 4 layers: -1.999769117693723\n",
      "Loss for 2 qubits and 4 layers: -1.9997803276145003\n",
      "Loss for 2 qubits and 4 layers: -1.9997880113579791\n",
      "Loss for 2 qubits and 4 layers: -1.9997992694429536\n",
      "Loss for 2 qubits and 4 layers: -1.9998186560954734\n",
      "Loss for 2 qubits and 4 layers: -1.9998470875933352\n",
      "Loss for 2 qubits and 4 layers: -1.999881923477069\n",
      "Loss for 2 qubits and 4 layers: -1.9999180465281272\n",
      "Loss for 2 qubits and 4 layers: -1.9999495292516976\n",
      "Loss for 2 qubits and 4 layers: -1.9999713720811132\n",
      "Loss for 2 qubits and 4 layers: -1.9999808403936385\n",
      "Loss for 2 qubits and 4 layers: -1.9999780855449147\n",
      "Loss for 2 qubits and 4 layers: -1.9999659560735403\n",
      "Loss for 2 qubits and 4 layers: -1.999949126078444\n",
      "Loss for 2 qubits and 4 layers: -1.999932832251552\n",
      "Loss for 2 qubits and 4 layers: -1.9999215827288648\n",
      "Loss for 2 qubits and 4 layers: -1.999918170715823\n",
      "Loss for 2 qubits and 4 layers: -1.999923212056957\n",
      "Loss for 2 qubits and 4 layers: -1.9999352670573085\n",
      "Loss for 2 qubits and 4 layers: -1.9999514494820354\n",
      "Loss for 2 qubits and 4 layers: -1.9999683111096571\n",
      "Loss for 2 qubits and 4 layers: -1.999982744088796\n",
      "Loss for 2 qubits and 4 layers: -1.9999926703014\n",
      "Loss for 2 qubits and 4 layers: -1.999997371617975\n",
      "Loss for 2 qubits and 4 layers: -1.9999974281270954\n",
      "Loss for 2 qubits and 4 layers: -1.9999943392942154\n",
      "Loss for 2 qubits and 4 layers: -1.9999899767701956\n",
      "Loss for 2 qubits and 4 layers: -1.9999860410000565\n",
      "Loss for 2 qubits and 4 layers: -1.9999836665626156\n",
      "Loss for 2 qubits and 4 layers: -1.9999832574013658\n",
      "Loss for 2 qubits and 4 layers: -1.9999845557094122\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def setup_circuit(num_qubits):\n",
    "    dev = qml.device('default.qubit', wires=num_qubits)\n",
    "    edges = [(i, j) for i in range(num_qubits) for j in range(i + 1, num_qubits)]\n",
    "    nodes = range(num_qubits)\n",
    "    h1 = qml.Hamiltonian([1.0 for _ in edges], [qml.PauliZ(i) @ qml.PauliZ(j) for (i, j) in edges])\n",
    "    h2 = qml.Hamiltonian([1.0 for _ in nodes], [qml.PauliX(i) for i in nodes])\n",
    "    stab1 = qml.Hamiltonian([1], [qml.operation.Tensor(*(qml.PauliX(i) for i in nodes))])\n",
    "    stab2 = qml.Hamiltonian([1 for _ in range(len(nodes) - 1)], [qml.PauliZ(i) @ qml.PauliZ(i + 1) for i in range(len(nodes) - 1)])\n",
    "    stab = stab1 + stab2\n",
    "    return dev, h1, h2, stab\n",
    "\n",
    "def test_circuit(num_qubits, P):\n",
    "    dev, h1, h2, stab = setup_circuit(num_qubits)\n",
    "    params = np.random.random(2 * P)\n",
    "    opt = qml.AdamOptimizer(stepsize=0.01)\n",
    "\n",
    "    @qml.qnode(dev)\n",
    "    def circuit(params):\n",
    "        for i in range(num_qubits):\n",
    "            qml.Hadamard(wires=i)\n",
    "        for p in range(P):\n",
    "            qml.templates.ApproxTimeEvolution(h1, params[2 * p], 1)\n",
    "            qml.templates.ApproxTimeEvolution(h2, params[2 * p + 1], 1)\n",
    "        return qml.expval(stab)\n",
    "\n",
    "    for _ in range(100):\n",
    "        params = opt.step(lambda x: -circuit(x), params)\n",
    "        loss = -circuit(params)\n",
    "        print(f\"Loss for {num_qubits} qubits and {P} layers: {loss}\")\n",
    "\n",
    "# Values for n and corresponding P\n",
    "n_and_p_values = [(2, 4)]\n",
    "\n",
    "for n, p in n_and_p_values:\n",
    "    test_circuit(n, p)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for 3 qubits and 3 layers: -1.3863513693771425\n",
      "Loss for 3 qubits and 3 layers: -1.5247073422695832\n",
      "Loss for 3 qubits and 3 layers: -1.6635048787739335\n",
      "Loss for 3 qubits and 3 layers: -1.8034422998167512\n",
      "Loss for 3 qubits and 3 layers: -1.9432618375246482\n",
      "Loss for 3 qubits and 3 layers: -2.081341252362843\n",
      "Loss for 3 qubits and 3 layers: -2.2158782375555526\n",
      "Loss for 3 qubits and 3 layers: -2.3449782398025314\n",
      "Loss for 3 qubits and 3 layers: -2.466729647607176\n",
      "Loss for 3 qubits and 3 layers: -2.5792739567664054\n",
      "Loss for 3 qubits and 3 layers: -2.68086855900993\n",
      "Loss for 3 qubits and 3 layers: -2.7699459031172564\n",
      "Loss for 3 qubits and 3 layers: -2.8451842659427324\n",
      "Loss for 3 qubits and 3 layers: -2.9056085390234463\n",
      "Loss for 3 qubits and 3 layers: -2.9507266301576047\n",
      "Loss for 3 qubits and 3 layers: -2.980684958510101\n",
      "Loss for 3 qubits and 3 layers: -2.9964070175990325\n",
      "Loss for 3 qubits and 3 layers: -2.9996673366602264\n",
      "Loss for 3 qubits and 3 layers: -2.9930496402242506\n",
      "Loss for 3 qubits and 3 layers: -2.979748645020257\n",
      "Loss for 3 qubits and 3 layers: -2.9632138110697035\n",
      "Loss for 3 qubits and 3 layers: -2.946701367467939\n",
      "Loss for 3 qubits and 3 layers: -2.9328594221759388\n",
      "Loss for 3 qubits and 3 layers: -2.9234649297773725\n",
      "Loss for 3 qubits and 3 layers: -2.919357742866655\n",
      "Loss for 3 qubits and 3 layers: -2.920536186019361\n",
      "Loss for 3 qubits and 3 layers: -2.926343182119064\n",
      "Loss for 3 qubits and 3 layers: -2.9356813667116235\n",
      "Loss for 3 qubits and 3 layers: -2.947221071484905\n",
      "Loss for 3 qubits and 3 layers: -2.959585425902281\n",
      "Loss for 3 qubits and 3 layers: -2.971506719989863\n",
      "Loss for 3 qubits and 3 layers: -2.9819500959284575\n",
      "Loss for 3 qubits and 3 layers: -2.990198418636672\n",
      "Loss for 3 qubits and 3 layers: -2.995889928099757\n",
      "Loss for 3 qubits and 3 layers: -2.999003668624611\n",
      "Loss for 3 qubits and 3 layers: -2.999800508184721\n",
      "Loss for 3 qubits and 3 layers: -2.998742223789035\n",
      "Loss for 3 qubits and 3 layers: -2.9964114815191545\n",
      "Loss for 3 qubits and 3 layers: -2.9934400177211984\n",
      "Loss for 3 qubits and 3 layers: -2.9904395534918793\n",
      "Loss for 3 qubits and 3 layers: -2.9879326322185413\n",
      "Loss for 3 qubits and 3 layers: -2.9862918938627767\n",
      "Loss for 3 qubits and 3 layers: -2.9857031316917344\n",
      "Loss for 3 qubits and 3 layers: -2.9861635134874818\n",
      "Loss for 3 qubits and 3 layers: -2.98751441894848\n",
      "Loss for 3 qubits and 3 layers: -2.989496327342432\n",
      "Loss for 3 qubits and 3 layers: -2.9918080298244227\n",
      "Loss for 3 qubits and 3 layers: -2.994155965749961\n",
      "Loss for 3 qubits and 3 layers: -2.996288071672657\n",
      "Loss for 3 qubits and 3 layers: -2.998014105744488\n",
      "Loss for 3 qubits and 3 layers: -2.9992169643623745\n",
      "Loss for 3 qubits and 3 layers: -2.9998576596201554\n",
      "Loss for 3 qubits and 3 layers: -2.999973672069617\n",
      "Loss for 3 qubits and 3 layers: -2.9996688542457455\n",
      "Loss for 3 qubits and 3 layers: -2.9990937256116506\n",
      "Loss for 3 qubits and 3 layers: -2.998417706597123\n",
      "Loss for 3 qubits and 3 layers: -2.997798490859778\n",
      "Loss for 3 qubits and 3 layers: -2.997356036783049\n",
      "Loss for 3 qubits and 3 layers: -2.997157501326823\n",
      "Loss for 3 qubits and 3 layers: -2.9972152782020505\n",
      "Loss for 3 qubits and 3 layers: -2.997495843838896\n",
      "Loss for 3 qubits and 3 layers: -2.997934696777592\n",
      "Loss for 3 qubits and 3 layers: -2.9984526906808417\n",
      "Loss for 3 qubits and 3 layers: -2.998970574157281\n",
      "Loss for 3 qubits and 3 layers: -2.9994203862553306\n",
      "Loss for 3 qubits and 3 layers: -2.9997535892484786\n",
      "Loss for 3 qubits and 3 layers: -2.999946033071888\n",
      "Loss for 3 qubits and 3 layers: -2.9999993674142154\n",
      "Loss for 3 qubits and 3 layers: -2.9999382377604817\n",
      "Loss for 3 qubits and 3 layers: -2.9998032373469803\n",
      "Loss for 3 qubits and 3 layers: -2.999641004770215\n",
      "Loss for 3 qubits and 3 layers: -2.999494117868389\n",
      "Loss for 3 qubits and 3 layers: -2.999393555029719\n",
      "Loss for 3 qubits and 3 layers: -2.9993552792002083\n",
      "Loss for 3 qubits and 3 layers: -2.9993807213458243\n",
      "Loss for 3 qubits and 3 layers: -2.99945971648696\n",
      "Loss for 3 qubits and 3 layers: -2.9995743579610954\n",
      "Loss for 3 qubits and 3 layers: -2.999702948895472\n",
      "Loss for 3 qubits and 3 layers: -2.9998238990810604\n",
      "Loss for 3 qubits and 3 layers: -2.9999194703264003\n",
      "Loss for 3 qubits and 3 layers: -2.9999788498533295\n",
      "Loss for 3 qubits and 3 layers: -2.999999718298744\n",
      "Loss for 3 qubits and 3 layers: -2.999987728609468\n",
      "Loss for 3 qubits and 3 layers: -2.9999540652216448\n",
      "Loss for 3 qubits and 3 layers: -2.9999120169992257\n",
      "Loss for 3 qubits and 3 layers: -2.999873759015725\n",
      "Loss for 3 qubits and 3 layers: -2.999848176039528\n",
      "Loss for 3 qubits and 3 layers: -2.9998399128432127\n",
      "Loss for 3 qubits and 3 layers: -2.9998493741510135\n",
      "Loss for 3 qubits and 3 layers: -2.9998733311792627\n",
      "Loss for 3 qubits and 3 layers: -2.999905967118007\n",
      "Loss for 3 qubits and 3 layers: -2.999940304442756\n",
      "Loss for 3 qubits and 3 layers: -2.999969845448164\n",
      "Loss for 3 qubits and 3 layers: -2.9999900399724417\n",
      "Loss for 3 qubits and 3 layers: -2.999999124203512\n",
      "Loss for 3 qubits and 3 layers: -2.999998086971114\n",
      "Loss for 3 qubits and 3 layers: -2.9999898977531463\n",
      "Loss for 3 qubits and 3 layers: -2.999978409439824\n",
      "Loss for 3 qubits and 3 layers: -2.9999673546825556\n",
      "Loss for 3 qubits and 3 layers: -2.999959645760822\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def setup_circuit(num_qubits):\n",
    "    dev = qml.device('default.qubit', wires=num_qubits)\n",
    "    edges = [(i, j) for i in range(num_qubits) for j in range(i + 1, num_qubits)]\n",
    "    nodes = range(num_qubits)\n",
    "    h1 = qml.Hamiltonian([1.0 for _ in edges], [qml.PauliZ(i) @ qml.PauliZ(j) for (i, j) in edges])\n",
    "    h2 = qml.Hamiltonian([1.0 for _ in nodes], [qml.PauliX(i) for i in nodes])\n",
    "    stab1 = qml.Hamiltonian([1], [qml.operation.Tensor(*(qml.PauliX(i) for i in nodes))])\n",
    "    stab2 = qml.Hamiltonian([1 for _ in range(len(nodes) - 1)], [qml.PauliZ(i) @ qml.PauliZ(i + 1) for i in range(len(nodes) - 1)])\n",
    "    stab = stab1 + stab2\n",
    "    return dev, h1, h2, stab\n",
    "\n",
    "def test_circuit(num_qubits, P):\n",
    "    dev, h1, h2, stab = setup_circuit(num_qubits)\n",
    "    params = np.random.random(2 * P)\n",
    "    opt = qml.AdamOptimizer(stepsize=0.01)\n",
    "\n",
    "    @qml.qnode(dev)\n",
    "    def circuit(params):\n",
    "        for i in range(num_qubits):\n",
    "            qml.Hadamard(wires=i)\n",
    "        for p in range(P):\n",
    "            qml.templates.ApproxTimeEvolution(h1, params[2 * p], 1)\n",
    "            qml.templates.ApproxTimeEvolution(h2, params[2 * p + 1], 1)\n",
    "        return qml.expval(stab)\n",
    "\n",
    "    for _ in range(100):\n",
    "        params = opt.step(lambda x: -circuit(x), params)\n",
    "        loss = -circuit(params)\n",
    "        print(f\"Loss for {num_qubits} qubits and {P} layers: {loss}\")\n",
    "\n",
    "n_and_p_values = [(3, 3)]\n",
    "\n",
    "for n, p in n_and_p_values:\n",
    "    test_circuit(n, p)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss = -2.5050373358231397, Params = [0.60004235 0.81312239 0.30938957 0.81566956]\n",
      "Step 10: Loss = -2.8972210178620696, Params = [0.60004234 0.78647922 0.21131236 0.77010798]\n",
      "Step 20: Loss = -4.114326496478137, Params = [0.60004233 0.77695598 0.10351109 0.79156297]\n",
      "Step 30: Loss = -4.995511233707608, Params = [ 0.60004228  0.78106869 -0.00121049  0.78746764]\n",
      "Step 40: Loss = -4.886796329088025, Params = [ 0.60004225  0.78464892 -0.03746198  0.78469072]\n",
      "Step 50: Loss = -4.989594418826419, Params = [ 0.60004224  0.7864273  -0.00775446  0.78384335]\n",
      "Step 60: Loss = -4.987504751066032, Params = [0.60004222 0.78725412 0.01287453 0.78389227]\n",
      "Step 70: Loss = -4.9984983181539855, Params = [0.60004222 0.78722739 0.00285639 0.78376619]\n",
      "Step 80: Loss = -4.998058134862365, Params = [ 0.60004226  0.78703075 -0.0049482   0.78356393]\n",
      "Step 90: Loss = -4.999970726786127, Params = [ 6.00042300e-01  7.87141208e-01 -2.28961189e-05  7.83762064e-01]\n",
      "Step 100: Loss = -4.999703419129588, Params = [0.60004228 0.78705046 0.00176432 0.7836846 ]\n",
      "Step 110: Loss = -4.999978812708599, Params = [ 6.00042226e-01  7.87088103e-01 -6.74276986e-04  7.83751069e-01]\n",
      "Step 120: Loss = -4.999984892540187, Params = [ 6.00042228e-01  7.87053709e-01 -2.97916870e-04  7.83718201e-01]\n",
      "Step 130: Loss = -4.999986221505313, Params = [6.00042219e-01 7.87062750e-01 3.98071330e-04 7.83733696e-01]\n",
      "Step 140: Loss = -4.999999057421891, Params = [ 6.00042159e-01  7.87066203e-01 -1.47958838e-04  7.83739963e-01]\n",
      "Step 150: Loss = -4.9999996514202, Params = [ 6.00042119e-01  7.87062278e-01 -3.17616733e-05  7.83735999e-01]\n",
      "Step 160: Loss = -4.999999399744017, Params = [6.00042109e-01 7.87060556e-01 7.40470290e-05 7.83734247e-01]\n",
      "Step 170: Loss = -4.999999807215198, Params = [ 6.00042079e-01  7.87060484e-01 -5.05972129e-05  7.83734316e-01]\n",
      "Step 180: Loss = -4.999999983025096, Params = [6.00042026e-01 7.87060768e-01 2.02900326e-05 7.83734731e-01]\n",
      "Step 190: Loss = -4.99999999953496, Params = [ 6.00041964e-01  7.87061080e-01 -2.54839347e-06  7.83735142e-01]\n",
      "Step 200: Loss = -4.999999996229992, Params = [ 6.00041972e-01  7.87061288e-01 -3.90846036e-06  7.83735409e-01]\n",
      "Step 210: Loss = -4.999999997040506, Params = [6.00041941e-01 7.87061211e-01 4.66044038e-06 7.83735318e-01]\n",
      "Step 220: Loss = -4.999999998637055, Params = [ 6.00041991e-01  7.87061045e-01 -3.56929589e-06  7.83735116e-01]\n",
      "Step 230: Loss = -4.999999999499619, Params = [6.00042060e-01 7.87061146e-01 2.33296367e-06 7.83735240e-01]\n",
      "Step 240: Loss = -4.999999999831333, Params = [ 6.00042012e-01  7.87061106e-01 -1.42614899e-06  7.83735191e-01]\n",
      "Step 250: Loss = -4.999999999942887, Params = [6.00041923e-01 7.87061133e-01 8.53712380e-07 7.83735225e-01]\n",
      "Step 260: Loss = -4.999999999979574, Params = [ 6.00041902e-01  7.87061109e-01 -5.11140673e-07  7.83735196e-01]\n",
      "Step 270: Loss = -4.999999999992256, Params = [6.00041966e-01 7.87061116e-01 3.05537809e-07 7.83735204e-01]\n",
      "Step 280: Loss = -4.999999999997028, Params = [ 6.00041992e-01  7.87061120e-01 -1.77583174e-07  7.83735209e-01]\n",
      "Step 290: Loss = -4.999999999998951, Params = [6.00041998e-01 7.87061120e-01 9.46886422e-08 7.83735209e-01]\n",
      "Step 300: Loss = -4.9999999999997, Params = [ 6.00041919e-01  7.87061119e-01 -4.06146356e-08  7.83735207e-01]\n",
      "Step 310: Loss = -4.999999999999939, Params = [6.00041833e-01 7.87061118e-01 7.62785228e-09 7.83735207e-01]\n",
      "Step 320: Loss = -4.99999999999998, Params = [6.00041615e-01 7.87061119e-01 8.61955729e-09 7.83735208e-01]\n",
      "Step 330: Loss = -4.999999999999996, Params = [ 6.00041474e-01  7.87061119e-01 -1.21839581e-08  7.83735207e-01]\n",
      "Step 340: Loss = -4.999999999999984, Params = [6.00041440e-01 7.87061119e-01 8.14610736e-09 7.83735208e-01]\n",
      "Step 350: Loss = -4.999999999999996, Params = [ 6.00041377e-01  7.87061119e-01 -2.12231346e-09  7.83735208e-01]\n",
      "Step 360: Loss = -4.999999999999981, Params = [ 6.00041295e-01  7.87061119e-01 -1.59013798e-09  7.83735208e-01]\n",
      "Step 370: Loss = -4.999999999999981, Params = [6.00041151e-01 7.87061119e-01 1.79623089e-09 7.83735208e-01]\n",
      "Step 380: Loss = -4.999999999999975, Params = [ 6.00041018e-01  7.87061120e-01 -2.84842145e-10  7.83735209e-01]\n",
      "Step 390: Loss = -4.999999999998494, Params = [ 6.00040936e-01  7.87061345e-01 -5.65122294e-10  7.83735485e-01]\n",
      "Step 400: Loss = -4.999998753472813, Params = [6.00040809e-01 7.87299907e-01 2.25443249e-10 7.84028734e-01]\n",
      "Step 410: Loss = -4.999848159697724, Params = [6.00040729e-01 7.87046858e-01 1.88424139e-10 7.83675503e-01]\n",
      "Step 420: Loss = -4.999994402001761, Params = [ 6.00040687e-01  7.86328768e-01 -6.48722754e-11  7.82933563e-01]\n",
      "Step 430: Loss = -4.999991030144367, Params = [ 6.00040649e-01  7.87387258e-01 -8.06713111e-11  7.84031872e-01]\n",
      "Step 440: Loss = -4.999994202320295, Params = [ 6.00040621e-01  7.87243796e-01 -8.06615567e-12  7.83883121e-01]\n",
      "Step 450: Loss = -4.999999035817175, Params = [6.00040546e-01 7.86968244e-01 2.28602130e-11 7.83598225e-01]\n",
      "Step 460: Loss = -4.9999999683392184, Params = [6.00040533e-01 7.86977307e-01 1.73577331e-11 7.83607614e-01]\n",
      "Step 470: Loss = -4.999999952651763, Params = [6.00040531e-01 7.87017438e-01 5.97922319e-12 7.83649093e-01]\n",
      "Step 480: Loss = -4.999999996886684, Params = [ 6.00040585e-01  7.87047940e-01 -1.04513165e-13  7.83680615e-01]\n",
      "Step 490: Loss = -4.99999994765733, Params = [ 6.00040619e-01  7.87089413e-01 -1.75721481e-12  7.83723473e-01]\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "\n",
    "P = 2  # Number of layers or repetitions\n",
    "Q = 2  # Two types of Hamiltonians per layer\n",
    "\n",
    "num_qubits = 6\n",
    "edges = [(i, j) for i in range(num_qubits) for j in range(i + 1, num_qubits)]\n",
    "nodes = range(num_qubits)\n",
    "\n",
    "dev = qml.device('default.qubit', wires=num_qubits)\n",
    "\n",
    "# Define the Hamiltonians\n",
    "h1 = qml.Hamiltonian([1.0 for _ in edges], [qml.PauliZ(i) @ qml.PauliZ(j) for (i, j) in edges])\n",
    "h2 = qml.Hamiltonian([1.0 for _ in nodes], [qml.PauliX(i) for i in nodes])\n",
    "\n",
    "stab1 = qml.Hamiltonian([1], [qml.operation.Tensor(*(qml.PauliX(i) for i in nodes))])\n",
    "stab2 = qml.Hamiltonian([1 for _ in range(len(nodes) - 1)], [qml.PauliZ(i) @ qml.PauliZ(i + 1) for i in range(len(nodes) - 1)])\n",
    "stab = stab1 + stab2\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def circuit(params):\n",
    "    for p in range(P):\n",
    "        qml.templates.ApproxTimeEvolution(h1, params[2 * p], 1)      # params[2 * p] for h1 in layer p\n",
    "        qml.templates.ApproxTimeEvolution(h2, params[2 * p + 1], 1)  # params[2 * p + 1] for h2 in layer p\n",
    "\n",
    "    \n",
    "    return qml.expval(stab)  \n",
    "\n",
    "params = np.random.random(P * Q)\n",
    "\n",
    "def loss(params):\n",
    "    return -circuit(params)\n",
    "\n",
    "\n",
    "opt = qml.AdamOptimizer(stepsize=0.01)\n",
    "for i in range(500):\n",
    "    params, val = opt.step_and_cost(loss, params)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Step {i}: Loss = {val}, Params = {params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimization for 2 qubits and 1 layers\n",
      "Step 0: Loss = -0.9873002363366321 for 2 qubits with 1 layers\n",
      "Step 10: Loss = -0.9983677816326373 for 2 qubits with 1 layers\n",
      "Step 20: Loss = -0.999999114899468 for 2 qubits with 1 layers\n",
      "Step 30: Loss = -0.9998370018019624 for 2 qubits with 1 layers\n",
      "Step 40: Loss = -0.9998810662645463 for 2 qubits with 1 layers\n",
      "Step 50: Loss = -0.9999622171671185 for 2 qubits with 1 layers\n",
      "Step 60: Loss = -0.999991127914698 for 2 qubits with 1 layers\n",
      "Step 70: Loss = -0.9999978502901236 for 2 qubits with 1 layers\n",
      "Step 80: Loss = -0.9999992823157496 for 2 qubits with 1 layers\n",
      "Step 90: Loss = -0.999999648781062 for 2 qubits with 1 layers\n",
      "Step 100: Loss = -0.9999997981493478 for 2 qubits with 1 layers\n",
      "Step 110: Loss = -0.9999998977669491 for 2 qubits with 1 layers\n",
      "Step 120: Loss = -0.9999999661085612 for 2 qubits with 1 layers\n",
      "Step 130: Loss = -0.9999999962735153 for 2 qubits with 1 layers\n",
      "Step 140: Loss = -0.9999999996448212 for 2 qubits with 1 layers\n",
      "Step 150: Loss = -0.9999999984058994 for 2 qubits with 1 layers\n",
      "Step 160: Loss = -0.9999999996458704 for 2 qubits with 1 layers\n",
      "Step 170: Loss = -0.9999999999646323 for 2 qubits with 1 layers\n",
      "Step 180: Loss = -0.9999999999189457 for 2 qubits with 1 layers\n",
      "Step 190: Loss = -0.9999999999994331 for 2 qubits with 1 layers\n",
      "Step 200: Loss = -0.9999999999896191 for 2 qubits with 1 layers\n",
      "Step 210: Loss = -0.9999999999992608 for 2 qubits with 1 layers\n",
      "Step 220: Loss = -0.9999999999993521 for 2 qubits with 1 layers\n",
      "Step 230: Loss = -0.9999999999995172 for 2 qubits with 1 layers\n",
      "Step 240: Loss = -0.9999999999999628 for 2 qubits with 1 layers\n",
      "Step 250: Loss = -0.9999999999999947 for 2 qubits with 1 layers\n",
      "Step 260: Loss = -0.9999999999999858 for 2 qubits with 1 layers\n",
      "Step 270: Loss = -0.9999999999999918 for 2 qubits with 1 layers\n",
      "Step 280: Loss = -0.9999999999999967 for 2 qubits with 1 layers\n",
      "Step 290: Loss = -0.9999999999999984 for 2 qubits with 1 layers\n",
      "Step 300: Loss = -0.9999999999999987 for 2 qubits with 1 layers\n",
      "Step 310: Loss = -0.9999999999999989 for 2 qubits with 1 layers\n",
      "Step 320: Loss = -0.9999999999999991 for 2 qubits with 1 layers\n",
      "Step 330: Loss = -0.9999999999999991 for 2 qubits with 1 layers\n",
      "Step 340: Loss = -0.9999999999999991 for 2 qubits with 1 layers\n",
      "Step 350: Loss = -0.9999999999999991 for 2 qubits with 1 layers\n",
      "Step 360: Loss = -0.9999999999999991 for 2 qubits with 1 layers\n",
      "Step 370: Loss = -0.9999999999999989 for 2 qubits with 1 layers\n",
      "Step 380: Loss = -0.9999999999999989 for 2 qubits with 1 layers\n",
      "Step 390: Loss = -0.999999999999999 for 2 qubits with 1 layers\n",
      "Step 400: Loss = -0.9999999999999991 for 2 qubits with 1 layers\n",
      "Step 410: Loss = -0.9999999999999991 for 2 qubits with 1 layers\n",
      "Step 420: Loss = -0.999999999999999 for 2 qubits with 1 layers\n",
      "Step 430: Loss = -0.999999999999999 for 2 qubits with 1 layers\n",
      "Step 440: Loss = -0.9999999999999991 for 2 qubits with 1 layers\n",
      "Step 450: Loss = -0.9999999999999989 for 2 qubits with 1 layers\n",
      "Step 460: Loss = -0.9999999999999991 for 2 qubits with 1 layers\n",
      "Step 470: Loss = -0.9999999999999991 for 2 qubits with 1 layers\n",
      "Step 480: Loss = -0.9999999999999991 for 2 qubits with 1 layers\n",
      "Step 490: Loss = -0.9999999999999991 for 2 qubits with 1 layers\n",
      "Running optimization for 2 qubits and 2 layers\n",
      "Step 0: Loss = -0.7544895608170331 for 2 qubits with 2 layers\n",
      "Step 10: Loss = -0.937602362146719 for 2 qubits with 2 layers\n",
      "Step 20: Loss = -0.9971484408741156 for 2 qubits with 2 layers\n",
      "Step 30: Loss = -0.997950340656901 for 2 qubits with 2 layers\n",
      "Step 40: Loss = -0.9970646128247349 for 2 qubits with 2 layers\n",
      "Step 50: Loss = -0.9994938075822863 for 2 qubits with 2 layers\n",
      "Step 60: Loss = -0.9999699096586616 for 2 qubits with 2 layers\n",
      "Step 70: Loss = -0.9998651629521296 for 2 qubits with 2 layers\n",
      "Step 80: Loss = -0.9999804580512297 for 2 qubits with 2 layers\n",
      "Step 90: Loss = -0.9999950113049627 for 2 qubits with 2 layers\n",
      "Step 100: Loss = -0.9999935445879545 for 2 qubits with 2 layers\n",
      "Step 110: Loss = -0.999999990927745 for 2 qubits with 2 layers\n",
      "Step 120: Loss = -0.9999991782736192 for 2 qubits with 2 layers\n",
      "Step 130: Loss = -0.9999999540394342 for 2 qubits with 2 layers\n",
      "Step 140: Loss = -0.9999999100370494 for 2 qubits with 2 layers\n",
      "Step 150: Loss = -0.9999999898171594 for 2 qubits with 2 layers\n",
      "Step 160: Loss = -0.9999999885426698 for 2 qubits with 2 layers\n",
      "Step 170: Loss = -0.9999999990876273 for 2 qubits with 2 layers\n",
      "Step 180: Loss = -0.9999999982576586 for 2 qubits with 2 layers\n",
      "Step 190: Loss = -0.9999999999858049 for 2 qubits with 2 layers\n",
      "Step 200: Loss = -0.9999999997622 for 2 qubits with 2 layers\n",
      "Step 210: Loss = -0.9999999999828528 for 2 qubits with 2 layers\n",
      "Step 220: Loss = -0.9999999999850702 for 2 qubits with 2 layers\n",
      "Step 230: Loss = -0.999999999990194 for 2 qubits with 2 layers\n",
      "Step 240: Loss = -0.9999999999999265 for 2 qubits with 2 layers\n",
      "Step 250: Loss = -0.9999999999991291 for 2 qubits with 2 layers\n",
      "Step 260: Loss = -0.9999999999995289 for 2 qubits with 2 layers\n",
      "Step 270: Loss = -0.9999999999999742 for 2 qubits with 2 layers\n",
      "Step 280: Loss = -0.9999999999999847 for 2 qubits with 2 layers\n",
      "Step 290: Loss = -0.9999999999999769 for 2 qubits with 2 layers\n",
      "Step 300: Loss = -0.9999999999999909 for 2 qubits with 2 layers\n",
      "Step 310: Loss = -0.9999999999999971 for 2 qubits with 2 layers\n",
      "Step 320: Loss = -0.9999999999999982 for 2 qubits with 2 layers\n",
      "Step 330: Loss = -0.9999999999999982 for 2 qubits with 2 layers\n",
      "Step 340: Loss = -0.9999999999999982 for 2 qubits with 2 layers\n",
      "Step 350: Loss = -0.9999999999999984 for 2 qubits with 2 layers\n",
      "Step 360: Loss = -0.9999999999999986 for 2 qubits with 2 layers\n",
      "Step 370: Loss = -0.9999999999999987 for 2 qubits with 2 layers\n",
      "Step 380: Loss = -0.9999999999999984 for 2 qubits with 2 layers\n",
      "Step 390: Loss = -0.9999999999999984 for 2 qubits with 2 layers\n",
      "Step 400: Loss = -0.9999999999999984 for 2 qubits with 2 layers\n",
      "Step 410: Loss = -0.9999999999999984 for 2 qubits with 2 layers\n",
      "Step 420: Loss = -0.9999999999999984 for 2 qubits with 2 layers\n",
      "Step 430: Loss = -0.9999999999999983 for 2 qubits with 2 layers\n",
      "Step 440: Loss = -0.9999999999999983 for 2 qubits with 2 layers\n",
      "Step 450: Loss = -0.9999999999999986 for 2 qubits with 2 layers\n",
      "Step 460: Loss = -0.9999999999999984 for 2 qubits with 2 layers\n",
      "Step 470: Loss = -0.9999999999999984 for 2 qubits with 2 layers\n",
      "Step 480: Loss = -0.9999999999999983 for 2 qubits with 2 layers\n",
      "Step 490: Loss = -0.9999999999999984 for 2 qubits with 2 layers\n",
      "Running optimization for 2 qubits and 3 layers\n",
      "Step 0: Loss = -0.8885527744722362 for 2 qubits with 3 layers\n",
      "Step 10: Loss = -0.9779860556010115 for 2 qubits with 3 layers\n",
      "Step 20: Loss = -0.9975564024681257 for 2 qubits with 3 layers\n",
      "Step 30: Loss = -0.9982061078015843 for 2 qubits with 3 layers\n",
      "Step 40: Loss = -0.9996152132964873 for 2 qubits with 3 layers\n",
      "Step 50: Loss = -0.9999572243578373 for 2 qubits with 3 layers\n",
      "Step 60: Loss = -0.9999076164849221 for 2 qubits with 3 layers\n",
      "Step 70: Loss = -0.9999928546990732 for 2 qubits with 3 layers\n",
      "Step 80: Loss = -0.9999817905400807 for 2 qubits with 3 layers\n",
      "Step 90: Loss = -0.9999965097744066 for 2 qubits with 3 layers\n",
      "Step 100: Loss = -0.9999980448019683 for 2 qubits with 3 layers\n",
      "Step 110: Loss = -0.9999999525552172 for 2 qubits with 3 layers\n",
      "Step 120: Loss = -0.9999996414079322 for 2 qubits with 3 layers\n",
      "Step 130: Loss = -0.9999999844794103 for 2 qubits with 3 layers\n",
      "Step 140: Loss = -0.9999999667079692 for 2 qubits with 3 layers\n",
      "Step 150: Loss = -0.9999999918698599 for 2 qubits with 3 layers\n",
      "Step 160: Loss = -0.9999999973962815 for 2 qubits with 3 layers\n",
      "Step 170: Loss = -0.9999999989273998 for 2 qubits with 3 layers\n",
      "Step 180: Loss = -0.9999999994885034 for 2 qubits with 3 layers\n",
      "Step 190: Loss = -0.9999999998793873 for 2 qubits with 3 layers\n",
      "Step 200: Loss = -0.9999999999527214 for 2 qubits with 3 layers\n",
      "Step 210: Loss = -0.9999999999795732 for 2 qubits with 3 layers\n",
      "Step 220: Loss = -0.9999999999957978 for 2 qubits with 3 layers\n",
      "Step 230: Loss = -0.9999999999995446 for 2 qubits with 3 layers\n",
      "Step 240: Loss = -0.9999999999997222 for 2 qubits with 3 layers\n",
      "Step 250: Loss = -0.9999999999997795 for 2 qubits with 3 layers\n",
      "Step 260: Loss = -0.9999999999998862 for 2 qubits with 3 layers\n",
      "Step 270: Loss = -0.9999999999999416 for 2 qubits with 3 layers\n",
      "Step 280: Loss = -0.9999999999999724 for 2 qubits with 3 layers\n",
      "Step 290: Loss = -0.9999999999999909 for 2 qubits with 3 layers\n",
      "Step 300: Loss = -0.999999999999995 for 2 qubits with 3 layers\n",
      "Step 310: Loss = -0.9999999999999962 for 2 qubits with 3 layers\n",
      "Step 320: Loss = -0.9999999999999981 for 2 qubits with 3 layers\n",
      "Step 330: Loss = -0.9999999999999977 for 2 qubits with 3 layers\n",
      "Step 340: Loss = -0.9999999999999973 for 2 qubits with 3 layers\n",
      "Step 350: Loss = -0.9999999999999978 for 2 qubits with 3 layers\n",
      "Step 360: Loss = -0.999999999999997 for 2 qubits with 3 layers\n",
      "Step 370: Loss = -0.9999999999999972 for 2 qubits with 3 layers\n",
      "Step 380: Loss = -0.9999999999999976 for 2 qubits with 3 layers\n",
      "Step 390: Loss = -0.9999999999999977 for 2 qubits with 3 layers\n",
      "Step 400: Loss = -0.9999999999999977 for 2 qubits with 3 layers\n",
      "Step 410: Loss = -0.9999999999999977 for 2 qubits with 3 layers\n",
      "Step 420: Loss = -0.9999999999999979 for 2 qubits with 3 layers\n",
      "Step 430: Loss = -0.9999999999999976 for 2 qubits with 3 layers\n",
      "Step 440: Loss = -0.999999999999997 for 2 qubits with 3 layers\n",
      "Step 450: Loss = -0.9999999999999979 for 2 qubits with 3 layers\n",
      "Step 460: Loss = -0.9999999999999973 for 2 qubits with 3 layers\n",
      "Step 470: Loss = -0.9999999999999978 for 2 qubits with 3 layers\n",
      "Step 480: Loss = -0.9999999999999978 for 2 qubits with 3 layers\n",
      "Step 490: Loss = -0.9999999999999974 for 2 qubits with 3 layers\n",
      "Running optimization for 2 qubits and 4 layers\n",
      "Step 0: Loss = -0.686848270009582 for 2 qubits with 4 layers\n",
      "Step 10: Loss = -0.9242976504069413 for 2 qubits with 4 layers\n",
      "Step 20: Loss = -0.9989087892058631 for 2 qubits with 4 layers\n",
      "Step 30: Loss = -0.9926400637699565 for 2 qubits with 4 layers\n",
      "Step 40: Loss = -0.9969830364166379 for 2 qubits with 4 layers\n",
      "Step 50: Loss = -0.9997698892597222 for 2 qubits with 4 layers\n",
      "Step 60: Loss = -0.9994489996527925 for 2 qubits with 4 layers\n",
      "Step 70: Loss = -0.9999862475136954 for 2 qubits with 4 layers\n",
      "Step 80: Loss = -0.9999276828455959 for 2 qubits with 4 layers\n",
      "Step 90: Loss = -0.9999973473440154 for 2 qubits with 4 layers\n",
      "Step 100: Loss = -0.9999904391053369 for 2 qubits with 4 layers\n",
      "Step 110: Loss = -0.99999986756162 for 2 qubits with 4 layers\n",
      "Step 120: Loss = -0.9999987768073022 for 2 qubits with 4 layers\n",
      "Step 130: Loss = -0.9999998133657869 for 2 qubits with 4 layers\n",
      "Step 140: Loss = -0.9999999385259084 for 2 qubits with 4 layers\n",
      "Step 150: Loss = -0.9999999390188745 for 2 qubits with 4 layers\n",
      "Step 160: Loss = -0.9999999957822098 for 2 qubits with 4 layers\n",
      "Step 170: Loss = -0.9999999966869133 for 2 qubits with 4 layers\n",
      "Step 180: Loss = -0.999999997184631 for 2 qubits with 4 layers\n",
      "Step 190: Loss = -0.9999999995735426 for 2 qubits with 4 layers\n",
      "Step 200: Loss = -0.9999999999738937 for 2 qubits with 4 layers\n",
      "Step 210: Loss = -0.9999999999357477 for 2 qubits with 4 layers\n",
      "Step 220: Loss = -0.9999999999545358 for 2 qubits with 4 layers\n",
      "Step 230: Loss = -0.9999999999858709 for 2 qubits with 4 layers\n",
      "Step 240: Loss = -0.9999999999971028 for 2 qubits with 4 layers\n",
      "Step 250: Loss = -0.9999999999997693 for 2 qubits with 4 layers\n",
      "Step 260: Loss = -0.9999999999999958 for 2 qubits with 4 layers\n",
      "Step 270: Loss = -0.9999999999999729 for 2 qubits with 4 layers\n",
      "Step 280: Loss = -0.9999999999999774 for 2 qubits with 4 layers\n",
      "Step 290: Loss = -0.9999999999999876 for 2 qubits with 4 layers\n",
      "Step 300: Loss = -0.9999999999999941 for 2 qubits with 4 layers\n",
      "Step 310: Loss = -0.9999999999999963 for 2 qubits with 4 layers\n",
      "Step 320: Loss = -0.9999999999999963 for 2 qubits with 4 layers\n",
      "Step 330: Loss = -0.9999999999999982 for 2 qubits with 4 layers\n",
      "Step 340: Loss = -0.9999999999999974 for 2 qubits with 4 layers\n",
      "Step 350: Loss = -0.9999999999999964 for 2 qubits with 4 layers\n",
      "Step 360: Loss = -0.9999999999999972 for 2 qubits with 4 layers\n",
      "Step 370: Loss = -0.9999999999999976 for 2 qubits with 4 layers\n",
      "Step 380: Loss = -0.9999999999999969 for 2 qubits with 4 layers\n",
      "Step 390: Loss = -0.9999999999999969 for 2 qubits with 4 layers\n",
      "Step 400: Loss = -0.9999999999999972 for 2 qubits with 4 layers\n",
      "Step 410: Loss = -0.9999999999999974 for 2 qubits with 4 layers\n",
      "Step 420: Loss = -0.9999999999999964 for 2 qubits with 4 layers\n",
      "Step 430: Loss = -0.9999999999999987 for 2 qubits with 4 layers\n",
      "Step 440: Loss = -0.9999999999999967 for 2 qubits with 4 layers\n",
      "Step 450: Loss = -0.9999999999999971 for 2 qubits with 4 layers\n",
      "Step 460: Loss = -0.9999999999999968 for 2 qubits with 4 layers\n",
      "Step 470: Loss = -0.9999999999999976 for 2 qubits with 4 layers\n",
      "Step 480: Loss = -0.9999999999999964 for 2 qubits with 4 layers\n",
      "Step 490: Loss = -0.9999999999999961 for 2 qubits with 4 layers\n",
      "Running optimization for 2 qubits and 5 layers\n",
      "Step 0: Loss = -0.10288208960157455 for 2 qubits with 5 layers\n",
      "Step 10: Loss = -0.9033835478871808 for 2 qubits with 5 layers\n",
      "Step 20: Loss = -0.9251189973975835 for 2 qubits with 5 layers\n",
      "Step 30: Loss = -0.9960393834097949 for 2 qubits with 5 layers\n",
      "Step 40: Loss = -0.9875890371409717 for 2 qubits with 5 layers\n",
      "Step 50: Loss = -0.9993684938608467 for 2 qubits with 5 layers\n",
      "Step 60: Loss = -0.9991783370109277 for 2 qubits with 5 layers\n",
      "Step 70: Loss = -0.9994231785658094 for 2 qubits with 5 layers\n",
      "Step 80: Loss = -0.999928123183655 for 2 qubits with 5 layers\n",
      "Step 90: Loss = -0.9999907980319943 for 2 qubits with 5 layers\n",
      "Step 100: Loss = -0.9999829876471394 for 2 qubits with 5 layers\n",
      "Step 110: Loss = -0.9999896865870233 for 2 qubits with 5 layers\n",
      "Step 120: Loss = -0.9999962025154887 for 2 qubits with 5 layers\n",
      "Step 130: Loss = -0.9999988822571797 for 2 qubits with 5 layers\n",
      "Step 140: Loss = -0.9999996863009288 for 2 qubits with 5 layers\n",
      "Step 150: Loss = -0.9999999034064454 for 2 qubits with 5 layers\n",
      "Step 160: Loss = -0.9999999652124021 for 2 qubits with 5 layers\n",
      "Step 170: Loss = -0.9999999857629711 for 2 qubits with 5 layers\n",
      "Step 180: Loss = -0.9999999937322509 for 2 qubits with 5 layers\n",
      "Step 190: Loss = -0.9999999972142434 for 2 qubits with 5 layers\n",
      "Step 200: Loss = -0.9999999989376427 for 2 qubits with 5 layers\n",
      "Step 210: Loss = -0.9999999997617783 for 2 qubits with 5 layers\n",
      "Step 220: Loss = -0.9999999999934156 for 2 qubits with 5 layers\n",
      "Step 230: Loss = -0.9999999999850543 for 2 qubits with 5 layers\n",
      "Step 240: Loss = -0.9999999999824978 for 2 qubits with 5 layers\n",
      "Step 250: Loss = -0.999999999996686 for 2 qubits with 5 layers\n",
      "Step 260: Loss = -0.9999999999996727 for 2 qubits with 5 layers\n",
      "Step 270: Loss = -0.999999999999178 for 2 qubits with 5 layers\n",
      "Step 280: Loss = -0.9999999999999577 for 2 qubits with 5 layers\n",
      "Step 290: Loss = -0.9999999999999037 for 2 qubits with 5 layers\n",
      "Step 300: Loss = -0.9999999999999901 for 2 qubits with 5 layers\n",
      "Step 310: Loss = -0.9999999999999833 for 2 qubits with 5 layers\n",
      "Step 320: Loss = -0.9999999999999969 for 2 qubits with 5 layers\n",
      "Step 330: Loss = -0.9999999999999956 for 2 qubits with 5 layers\n",
      "Step 340: Loss = -0.9999999999999953 for 2 qubits with 5 layers\n",
      "Step 350: Loss = -0.9999999999999956 for 2 qubits with 5 layers\n",
      "Step 360: Loss = -0.9999999999999972 for 2 qubits with 5 layers\n",
      "Step 370: Loss = -0.9999999999999959 for 2 qubits with 5 layers\n",
      "Step 380: Loss = -0.9999999999999976 for 2 qubits with 5 layers\n",
      "Step 390: Loss = -0.9999999999999966 for 2 qubits with 5 layers\n",
      "Step 400: Loss = -0.9999999999999967 for 2 qubits with 5 layers\n",
      "Step 410: Loss = -0.9999999999999963 for 2 qubits with 5 layers\n",
      "Step 420: Loss = -0.9999999999999954 for 2 qubits with 5 layers\n",
      "Step 430: Loss = -0.999999999999996 for 2 qubits with 5 layers\n",
      "Step 440: Loss = -0.9999999999999958 for 2 qubits with 5 layers\n",
      "Step 450: Loss = -0.999999999999996 for 2 qubits with 5 layers\n",
      "Step 460: Loss = -0.9999999999999966 for 2 qubits with 5 layers\n",
      "Step 470: Loss = -0.9999999999999954 for 2 qubits with 5 layers\n",
      "Step 480: Loss = -0.9999999999999962 for 2 qubits with 5 layers\n",
      "Step 490: Loss = -0.9999999999999967 for 2 qubits with 5 layers\n",
      "Running optimization for 2 qubits and 6 layers\n",
      "Step 0: Loss = -0.9825247921477953 for 2 qubits with 6 layers\n",
      "Step 10: Loss = -0.9969440759137717 for 2 qubits with 6 layers\n",
      "Step 20: Loss = -0.9996254416823409 for 2 qubits with 6 layers\n",
      "Step 30: Loss = -0.9997796987024103 for 2 qubits with 6 layers\n",
      "Step 40: Loss = -0.9999344568566451 for 2 qubits with 6 layers\n",
      "Step 50: Loss = -0.9999445948580709 for 2 qubits with 6 layers\n",
      "Step 60: Loss = -0.9999961321913338 for 2 qubits with 6 layers\n",
      "Step 70: Loss = -0.9999998723568457 for 2 qubits with 6 layers\n",
      "Step 80: Loss = -0.9999995856471527 for 2 qubits with 6 layers\n",
      "Step 90: Loss = -0.9999998752097479 for 2 qubits with 6 layers\n",
      "Step 100: Loss = -0.9999999991133091 for 2 qubits with 6 layers\n",
      "Step 110: Loss = -0.9999999564459525 for 2 qubits with 6 layers\n",
      "Step 120: Loss = -0.9999999475661653 for 2 qubits with 6 layers\n",
      "Step 130: Loss = -0.9999999946688382 for 2 qubits with 6 layers\n",
      "Step 140: Loss = -0.9999999958923662 for 2 qubits with 6 layers\n",
      "Step 150: Loss = -0.9999999992478923 for 2 qubits with 6 layers\n",
      "Step 160: Loss = -0.9999999991044869 for 2 qubits with 6 layers\n",
      "Step 170: Loss = -0.9999999999190119 for 2 qubits with 6 layers\n",
      "Step 180: Loss = -0.9999999999987389 for 2 qubits with 6 layers\n",
      "Step 190: Loss = -0.999999999993387 for 2 qubits with 6 layers\n",
      "Step 200: Loss = -0.9999999999974095 for 2 qubits with 6 layers\n",
      "Step 210: Loss = -0.9999999999999013 for 2 qubits with 6 layers\n",
      "Step 220: Loss = -0.9999999999995699 for 2 qubits with 6 layers\n",
      "Step 230: Loss = -0.9999999999992756 for 2 qubits with 6 layers\n",
      "Step 240: Loss = -0.9999999999999255 for 2 qubits with 6 layers\n",
      "Step 250: Loss = -0.9999999999999234 for 2 qubits with 6 layers\n",
      "Step 260: Loss = -0.9999999999999947 for 2 qubits with 6 layers\n",
      "Step 270: Loss = -0.9999999999999856 for 2 qubits with 6 layers\n",
      "Step 280: Loss = -0.9999999999998992 for 2 qubits with 6 layers\n",
      "Step 290: Loss = -0.9999999877197531 for 2 qubits with 6 layers\n",
      "Step 300: Loss = -0.9999859075109135 for 2 qubits with 6 layers\n",
      "Step 310: Loss = -0.9999973506725243 for 2 qubits with 6 layers\n",
      "Step 320: Loss = -0.9999999642493087 for 2 qubits with 6 layers\n",
      "Step 330: Loss = -0.9999992461298728 for 2 qubits with 6 layers\n",
      "Step 340: Loss = -0.9999999434154768 for 2 qubits with 6 layers\n",
      "Step 350: Loss = -0.9999999907249206 for 2 qubits with 6 layers\n",
      "Step 360: Loss = -0.9999999613608289 for 2 qubits with 6 layers\n",
      "Step 370: Loss = -0.9999999998523645 for 2 qubits with 6 layers\n",
      "Step 380: Loss = -0.9999999976976122 for 2 qubits with 6 layers\n",
      "Step 390: Loss = -0.9999999998773403 for 2 qubits with 6 layers\n",
      "Step 400: Loss = -0.999999910186919 for 2 qubits with 6 layers\n",
      "Step 410: Loss = -0.9999921953376376 for 2 qubits with 6 layers\n",
      "Step 420: Loss = -0.9999982463266697 for 2 qubits with 6 layers\n",
      "Step 430: Loss = -0.9999976356427525 for 2 qubits with 6 layers\n",
      "Step 440: Loss = -0.9999995637473036 for 2 qubits with 6 layers\n",
      "Step 450: Loss = -0.9999994725307726 for 2 qubits with 6 layers\n",
      "Step 460: Loss = -0.9999997196472308 for 2 qubits with 6 layers\n",
      "Step 470: Loss = -0.9999999433161464 for 2 qubits with 6 layers\n",
      "Step 480: Loss = -0.9999401158797429 for 2 qubits with 6 layers\n",
      "Step 490: Loss = -0.9999996578428731 for 2 qubits with 6 layers\n",
      "Running optimization for 2 qubits and 7 layers\n",
      "Step 0: Loss = -0.18168421679002128 for 2 qubits with 7 layers\n",
      "Step 10: Loss = -0.9937379230635299 for 2 qubits with 7 layers\n",
      "Step 20: Loss = -0.946399932143446 for 2 qubits with 7 layers\n",
      "Step 30: Loss = -0.9903097593862833 for 2 qubits with 7 layers\n",
      "Step 40: Loss = -0.9982401970239654 for 2 qubits with 7 layers\n",
      "Step 50: Loss = -0.9968504645200279 for 2 qubits with 7 layers\n",
      "Step 60: Loss = -0.9993480467705065 for 2 qubits with 7 layers\n",
      "Step 70: Loss = -0.9999874133182334 for 2 qubits with 7 layers\n",
      "Step 80: Loss = -0.9999556981402995 for 2 qubits with 7 layers\n",
      "Step 90: Loss = -0.9999569615988594 for 2 qubits with 7 layers\n",
      "Step 100: Loss = -0.9999792662638127 for 2 qubits with 7 layers\n",
      "Step 110: Loss = -0.9999926018627369 for 2 qubits with 7 layers\n",
      "Step 120: Loss = -0.9999977041964123 for 2 qubits with 7 layers\n",
      "Step 130: Loss = -0.999999167021792 for 2 qubits with 7 layers\n",
      "Step 140: Loss = -0.9999996924622951 for 2 qubits with 7 layers\n",
      "Step 150: Loss = -0.9999998783742333 for 2 qubits with 7 layers\n",
      "Step 160: Loss = -0.9999999548022325 for 2 qubits with 7 layers\n",
      "Step 170: Loss = -0.9999999863738679 for 2 qubits with 7 layers\n",
      "Step 180: Loss = -0.9999999981265635 for 2 qubits with 7 layers\n",
      "Step 190: Loss = -0.99999999990303 for 2 qubits with 7 layers\n",
      "Step 200: Loss = -0.9999999996203299 for 2 qubits with 7 layers\n",
      "Step 210: Loss = -0.9999999997281124 for 2 qubits with 7 layers\n",
      "Step 220: Loss = -0.9999999999704128 for 2 qubits with 7 layers\n",
      "Step 230: Loss = -0.9999999999921164 for 2 qubits with 7 layers\n",
      "Step 240: Loss = -0.9999999999879111 for 2 qubits with 7 layers\n",
      "Step 250: Loss = -0.9999999999999398 for 2 qubits with 7 layers\n",
      "Step 260: Loss = -0.9999999999984797 for 2 qubits with 7 layers\n",
      "Step 270: Loss = -0.9999999999999711 for 2 qubits with 7 layers\n",
      "Step 280: Loss = -0.9999999999997826 for 2 qubits with 7 layers\n",
      "Step 290: Loss = -0.9999999999999833 for 2 qubits with 7 layers\n",
      "Step 300: Loss = -0.9999999999999798 for 2 qubits with 7 layers\n",
      "Step 310: Loss = -0.9999999999999855 for 2 qubits with 7 layers\n",
      "Step 320: Loss = -0.9999999999999943 for 2 qubits with 7 layers\n",
      "Step 330: Loss = -0.9999999999999947 for 2 qubits with 7 layers\n",
      "Step 340: Loss = -0.999999999999994 for 2 qubits with 7 layers\n",
      "Step 350: Loss = -0.9999999999999954 for 2 qubits with 7 layers\n",
      "Step 360: Loss = -0.9999999999999954 for 2 qubits with 7 layers\n",
      "Step 370: Loss = -0.9999999999999947 for 2 qubits with 7 layers\n",
      "Step 380: Loss = -0.9999999999999949 for 2 qubits with 7 layers\n",
      "Step 390: Loss = -0.9999999999999949 for 2 qubits with 7 layers\n",
      "Step 400: Loss = -0.999999999999996 for 2 qubits with 7 layers\n",
      "Step 410: Loss = -0.9999999999999948 for 2 qubits with 7 layers\n",
      "Step 420: Loss = -0.9999999999999939 for 2 qubits with 7 layers\n",
      "Step 430: Loss = -0.9999999999999942 for 2 qubits with 7 layers\n",
      "Step 440: Loss = -0.9999999999999953 for 2 qubits with 7 layers\n",
      "Step 450: Loss = -0.999999999999996 for 2 qubits with 7 layers\n",
      "Step 460: Loss = -0.9999999999999949 for 2 qubits with 7 layers\n",
      "Step 470: Loss = -0.9999999999999951 for 2 qubits with 7 layers\n",
      "Step 480: Loss = -0.9999999999999938 for 2 qubits with 7 layers\n",
      "Step 490: Loss = -0.9999999999999936 for 2 qubits with 7 layers\n",
      "Running optimization for 2 qubits and 8 layers\n",
      "Step 0: Loss = -0.5310046729571003 for 2 qubits with 8 layers\n",
      "Step 10: Loss = -0.9284742777868437 for 2 qubits with 8 layers\n",
      "Step 20: Loss = -0.9877497833717803 for 2 qubits with 8 layers\n",
      "Step 30: Loss = -0.9935274679263683 for 2 qubits with 8 layers\n",
      "Step 40: Loss = -0.9961509125534682 for 2 qubits with 8 layers\n",
      "Step 50: Loss = -0.9983849158154765 for 2 qubits with 8 layers\n",
      "Step 60: Loss = -0.999440181794756 for 2 qubits with 8 layers\n",
      "Step 70: Loss = -0.9998379395976478 for 2 qubits with 8 layers\n",
      "Step 80: Loss = -0.9999500970954336 for 2 qubits with 8 layers\n",
      "Step 90: Loss = -0.9999734831246572 for 2 qubits with 8 layers\n",
      "Step 100: Loss = -0.999991707783038 for 2 qubits with 8 layers\n",
      "Step 110: Loss = -0.9999990363358253 for 2 qubits with 8 layers\n",
      "Step 120: Loss = -0.9999995376249193 for 2 qubits with 8 layers\n",
      "Step 130: Loss = -0.9999997717791249 for 2 qubits with 8 layers\n",
      "Step 140: Loss = -0.9999998515906432 for 2 qubits with 8 layers\n",
      "Step 150: Loss = -0.9999999811328518 for 2 qubits with 8 layers\n",
      "Step 160: Loss = -0.9999999873718652 for 2 qubits with 8 layers\n",
      "Step 170: Loss = -0.9999999970631959 for 2 qubits with 8 layers\n",
      "Step 180: Loss = -0.9999999977363595 for 2 qubits with 8 layers\n",
      "Step 190: Loss = -0.9999999994420526 for 2 qubits with 8 layers\n",
      "Step 200: Loss = -0.999999999629622 for 2 qubits with 8 layers\n",
      "Step 210: Loss = -0.9999999999528407 for 2 qubits with 8 layers\n",
      "Step 220: Loss = -0.9999999999658598 for 2 qubits with 8 layers\n",
      "Step 230: Loss = -0.9999999999815523 for 2 qubits with 8 layers\n",
      "Step 240: Loss = -0.9999999999960201 for 2 qubits with 8 layers\n",
      "Step 250: Loss = -0.9999999999997853 for 2 qubits with 8 layers\n",
      "Step 260: Loss = -0.9999999999998703 for 2 qubits with 8 layers\n",
      "Step 270: Loss = -0.9999999999998854 for 2 qubits with 8 layers\n",
      "Step 280: Loss = -0.999999999999992 for 2 qubits with 8 layers\n",
      "Step 290: Loss = -0.9999999999999808 for 2 qubits with 8 layers\n",
      "Step 300: Loss = -0.9999999999999931 for 2 qubits with 8 layers\n",
      "Step 310: Loss = -0.9999999999999912 for 2 qubits with 8 layers\n",
      "Step 320: Loss = -0.9999999999999916 for 2 qubits with 8 layers\n",
      "Step 330: Loss = -0.999999999999994 for 2 qubits with 8 layers\n",
      "Step 340: Loss = -0.9999999999999951 for 2 qubits with 8 layers\n",
      "Step 350: Loss = -0.9999999999999939 for 2 qubits with 8 layers\n",
      "Step 360: Loss = -0.999999999999995 for 2 qubits with 8 layers\n",
      "Step 370: Loss = -0.9999999999999938 for 2 qubits with 8 layers\n",
      "Step 380: Loss = -0.9999999999999933 for 2 qubits with 8 layers\n",
      "Step 390: Loss = -0.9999999999999944 for 2 qubits with 8 layers\n",
      "Step 400: Loss = -0.9999999999999933 for 2 qubits with 8 layers\n",
      "Step 410: Loss = -0.9999999999999946 for 2 qubits with 8 layers\n",
      "Step 420: Loss = -0.9999999999999949 for 2 qubits with 8 layers\n",
      "Step 430: Loss = -0.9999999999999929 for 2 qubits with 8 layers\n",
      "Step 440: Loss = -0.9999999999999941 for 2 qubits with 8 layers\n",
      "Step 450: Loss = -0.9999999999999923 for 2 qubits with 8 layers\n",
      "Step 460: Loss = -0.9999999999999936 for 2 qubits with 8 layers\n",
      "Step 470: Loss = -0.999999999999994 for 2 qubits with 8 layers\n",
      "Step 480: Loss = -0.9999999999999943 for 2 qubits with 8 layers\n",
      "Step 490: Loss = -0.9999999999999944 for 2 qubits with 8 layers\n",
      "Running optimization for 2 qubits and 9 layers\n",
      "Step 0: Loss = -0.7465348482847013 for 2 qubits with 9 layers\n",
      "Step 10: Loss = -0.9669837390040532 for 2 qubits with 9 layers\n",
      "Step 20: Loss = -0.9868967040657518 for 2 qubits with 9 layers\n",
      "Step 30: Loss = -0.9948285192935542 for 2 qubits with 9 layers\n",
      "Step 40: Loss = -0.9982167959539071 for 2 qubits with 9 layers\n",
      "Step 50: Loss = -0.9995207264198663 for 2 qubits with 9 layers\n",
      "Step 60: Loss = -0.9999516066922216 for 2 qubits with 9 layers\n",
      "Step 70: Loss = -0.9999900956556652 for 2 qubits with 9 layers\n",
      "Step 80: Loss = -0.9999667783798427 for 2 qubits with 9 layers\n",
      "Step 90: Loss = -0.9999911298446149 for 2 qubits with 9 layers\n",
      "Step 100: Loss = -0.9999993016615624 for 2 qubits with 9 layers\n",
      "Step 110: Loss = -0.9999982372986784 for 2 qubits with 9 layers\n",
      "Step 120: Loss = -0.9999999402711168 for 2 qubits with 9 layers\n",
      "Step 130: Loss = -0.9999998123191455 for 2 qubits with 9 layers\n",
      "Step 140: Loss = -0.9999999436972431 for 2 qubits with 9 layers\n",
      "Step 150: Loss = -0.9999999997754588 for 2 qubits with 9 layers\n",
      "Step 160: Loss = -0.999999993118538 for 2 qubits with 9 layers\n",
      "Step 170: Loss = -0.9999999958374326 for 2 qubits with 9 layers\n",
      "Step 180: Loss = -0.9999999986967931 for 2 qubits with 9 layers\n",
      "Step 190: Loss = -0.9999999996489393 for 2 qubits with 9 layers\n",
      "Step 200: Loss = -0.9999999998885987 for 2 qubits with 9 layers\n",
      "Step 210: Loss = -0.9999999999519998 for 2 qubits with 9 layers\n",
      "Step 220: Loss = -0.9999999999764244 for 2 qubits with 9 layers\n",
      "Step 230: Loss = -0.9999999999914647 for 2 qubits with 9 layers\n",
      "Step 240: Loss = -0.9999999999988136 for 2 qubits with 9 layers\n",
      "Step 250: Loss = -0.9999999999998914 for 2 qubits with 9 layers\n",
      "Step 260: Loss = -0.9999999999995843 for 2 qubits with 9 layers\n",
      "Step 270: Loss = -0.9999999999999714 for 2 qubits with 9 layers\n",
      "Step 280: Loss = -0.9999999999999478 for 2 qubits with 9 layers\n",
      "Step 290: Loss = -0.9999999999999913 for 2 qubits with 9 layers\n",
      "Step 300: Loss = -0.9999999999999867 for 2 qubits with 9 layers\n",
      "Step 310: Loss = -0.9999999999999913 for 2 qubits with 9 layers\n",
      "Step 320: Loss = -0.9999999999999939 for 2 qubits with 9 layers\n",
      "Step 330: Loss = -0.999999999999994 for 2 qubits with 9 layers\n",
      "Step 340: Loss = -0.9999999999999932 for 2 qubits with 9 layers\n",
      "Step 350: Loss = -0.9999999999999925 for 2 qubits with 9 layers\n",
      "Step 360: Loss = -0.999999999999992 for 2 qubits with 9 layers\n",
      "Step 370: Loss = -0.9999999999999946 for 2 qubits with 9 layers\n",
      "Step 380: Loss = -0.9999999999999927 for 2 qubits with 9 layers\n",
      "Step 390: Loss = -0.9999999999999949 for 2 qubits with 9 layers\n",
      "Step 400: Loss = -0.9999999999999928 for 2 qubits with 9 layers\n",
      "Step 410: Loss = -0.9999999999999922 for 2 qubits with 9 layers\n",
      "Step 420: Loss = -0.9999999999999938 for 2 qubits with 9 layers\n",
      "Step 430: Loss = -0.9999999999999936 for 2 qubits with 9 layers\n",
      "Step 440: Loss = -0.9999999999999923 for 2 qubits with 9 layers\n",
      "Step 450: Loss = -0.999999999999992 for 2 qubits with 9 layers\n",
      "Step 460: Loss = -0.9999999999999932 for 2 qubits with 9 layers\n",
      "Step 470: Loss = -0.9999999999999932 for 2 qubits with 9 layers\n",
      "Step 480: Loss = -0.9999999999999913 for 2 qubits with 9 layers\n",
      "Step 490: Loss = -0.999999999999995 for 2 qubits with 9 layers\n",
      "Running optimization for 2 qubits and 10 layers\n",
      "Step 0: Loss = -0.45745682975369095 for 2 qubits with 10 layers\n",
      "Step 10: Loss = -0.9237033541350002 for 2 qubits with 10 layers\n",
      "Step 20: Loss = -0.9971005291219592 for 2 qubits with 10 layers\n",
      "Step 30: Loss = -0.997926560934565 for 2 qubits with 10 layers\n",
      "Step 40: Loss = -0.9966001413315583 for 2 qubits with 10 layers\n",
      "Step 50: Loss = -0.9982694198517144 for 2 qubits with 10 layers\n",
      "Step 60: Loss = -0.999438575219222 for 2 qubits with 10 layers\n",
      "Step 70: Loss = -0.9997800420456553 for 2 qubits with 10 layers\n",
      "Step 80: Loss = -0.999926554071609 for 2 qubits with 10 layers\n",
      "Step 90: Loss = -0.9999686686416145 for 2 qubits with 10 layers\n",
      "Step 100: Loss = -0.9999900893941156 for 2 qubits with 10 layers\n",
      "Step 110: Loss = -0.9999982679388781 for 2 qubits with 10 layers\n",
      "Step 120: Loss = -0.9999999567901086 for 2 qubits with 10 layers\n",
      "Step 130: Loss = -0.9999997905906708 for 2 qubits with 10 layers\n",
      "Step 140: Loss = -0.999999813645989 for 2 qubits with 10 layers\n",
      "Step 150: Loss = -0.9999999823503707 for 2 qubits with 10 layers\n",
      "Step 160: Loss = -0.9999999902091359 for 2 qubits with 10 layers\n",
      "Step 170: Loss = -0.9999999934727678 for 2 qubits with 10 layers\n",
      "Step 180: Loss = -0.9999999993014363 for 2 qubits with 10 layers\n",
      "Step 190: Loss = -0.9999999990362157 for 2 qubits with 10 layers\n",
      "Step 200: Loss = -0.999999999854125 for 2 qubits with 10 layers\n",
      "Step 210: Loss = -0.999999999945512 for 2 qubits with 10 layers\n",
      "Step 220: Loss = -0.9999999999506006 for 2 qubits with 10 layers\n",
      "Step 230: Loss = -0.9999999999922542 for 2 qubits with 10 layers\n",
      "Step 240: Loss = -0.9999999999991869 for 2 qubits with 10 layers\n",
      "Step 250: Loss = -0.9999999999989572 for 2 qubits with 10 layers\n",
      "Step 260: Loss = -0.9999999999993222 for 2 qubits with 10 layers\n",
      "Step 270: Loss = -0.9999999999997076 for 2 qubits with 10 layers\n",
      "Step 280: Loss = -0.9999999999998883 for 2 qubits with 10 layers\n",
      "Step 290: Loss = -0.999999999999956 for 2 qubits with 10 layers\n",
      "Step 300: Loss = -0.9999999999999807 for 2 qubits with 10 layers\n",
      "Step 310: Loss = -0.9999999999999902 for 2 qubits with 10 layers\n",
      "Step 320: Loss = -0.9999999999999922 for 2 qubits with 10 layers\n",
      "Step 330: Loss = -0.9999999999999938 for 2 qubits with 10 layers\n",
      "Step 340: Loss = -0.9999999999999925 for 2 qubits with 10 layers\n",
      "Step 350: Loss = -0.9999999999999933 for 2 qubits with 10 layers\n",
      "Step 360: Loss = -0.9999999999999917 for 2 qubits with 10 layers\n",
      "Step 370: Loss = -0.9999999999999928 for 2 qubits with 10 layers\n",
      "Step 380: Loss = -0.9999999999999937 for 2 qubits with 10 layers\n",
      "Step 390: Loss = -0.9999999999999936 for 2 qubits with 10 layers\n",
      "Step 400: Loss = -0.9999999999999942 for 2 qubits with 10 layers\n",
      "Step 410: Loss = -0.9999999999999921 for 2 qubits with 10 layers\n",
      "Step 420: Loss = -0.999999999999991 for 2 qubits with 10 layers\n",
      "Step 430: Loss = -0.9999999999999937 for 2 qubits with 10 layers\n",
      "Step 440: Loss = -0.999999999999992 for 2 qubits with 10 layers\n",
      "Step 450: Loss = -0.9999999999999926 for 2 qubits with 10 layers\n",
      "Step 460: Loss = -0.9999999999999944 for 2 qubits with 10 layers\n",
      "Step 470: Loss = -0.9999999999999927 for 2 qubits with 10 layers\n",
      "Step 480: Loss = -0.9999999999999931 for 2 qubits with 10 layers\n",
      "Step 490: Loss = -0.9999999999999911 for 2 qubits with 10 layers\n",
      "Running optimization for 2 qubits and 11 layers\n",
      "Step 0: Loss = -0.2578083299481865 for 2 qubits with 11 layers\n",
      "Step 10: Loss = -0.9219898441253321 for 2 qubits with 11 layers\n",
      "Step 20: Loss = -0.9991331103457659 for 2 qubits with 11 layers\n",
      "Step 30: Loss = -0.9957503570717795 for 2 qubits with 11 layers\n",
      "Step 40: Loss = -0.9950718304855728 for 2 qubits with 11 layers\n",
      "Step 50: Loss = -0.9976576514890945 for 2 qubits with 11 layers\n",
      "Step 60: Loss = -0.9991495473777559 for 2 qubits with 11 layers\n",
      "Step 70: Loss = -0.9997012449552991 for 2 qubits with 11 layers\n",
      "Step 80: Loss = -0.9998882925326107 for 2 qubits with 11 layers\n",
      "Step 90: Loss = -0.99995741077378 for 2 qubits with 11 layers\n",
      "Step 100: Loss = -0.9999861581475931 for 2 qubits with 11 layers\n",
      "Step 110: Loss = -0.9999974021380476 for 2 qubits with 11 layers\n",
      "Step 120: Loss = -0.999999974498119 for 2 qubits with 11 layers\n",
      "Step 130: Loss = -0.9999997189383023 for 2 qubits with 11 layers\n",
      "Step 140: Loss = -0.9999997358319649 for 2 qubits with 11 layers\n",
      "Step 150: Loss = -0.9999999774039191 for 2 qubits with 11 layers\n",
      "Step 160: Loss = -0.9999999859439104 for 2 qubits with 11 layers\n",
      "Step 170: Loss = -0.9999999900751206 for 2 qubits with 11 layers\n",
      "Step 180: Loss = -0.9999999993513339 for 2 qubits with 11 layers\n",
      "Step 190: Loss = -0.999999998685922 for 2 qubits with 11 layers\n",
      "Step 200: Loss = -0.9999999997885687 for 2 qubits with 11 layers\n",
      "Step 210: Loss = -0.9999999999387674 for 2 qubits with 11 layers\n",
      "Step 220: Loss = -0.9999999999255614 for 2 qubits with 11 layers\n",
      "Step 230: Loss = -0.999999999991392 for 2 qubits with 11 layers\n",
      "Step 240: Loss = -0.9999999999997454 for 2 qubits with 11 layers\n",
      "Step 250: Loss = -0.9999999999984088 for 2 qubits with 11 layers\n",
      "Step 260: Loss = -0.99999999999894 for 2 qubits with 11 layers\n",
      "Step 270: Loss = -0.9999999999995586 for 2 qubits with 11 layers\n",
      "Step 280: Loss = -0.9999999999998341 for 2 qubits with 11 layers\n",
      "Step 290: Loss = -0.9999999999999354 for 2 qubits with 11 layers\n",
      "Step 300: Loss = -0.9999999999999747 for 2 qubits with 11 layers\n",
      "Step 310: Loss = -0.999999999999991 for 2 qubits with 11 layers\n",
      "Step 320: Loss = -0.9999999999999913 for 2 qubits with 11 layers\n",
      "Step 330: Loss = -0.9999999999999934 for 2 qubits with 11 layers\n",
      "Step 340: Loss = -0.9999999999999905 for 2 qubits with 11 layers\n",
      "Step 350: Loss = -0.9999999999999922 for 2 qubits with 11 layers\n",
      "Step 360: Loss = -0.9999999999999929 for 2 qubits with 11 layers\n",
      "Step 370: Loss = -0.9999999999999913 for 2 qubits with 11 layers\n",
      "Step 380: Loss = -0.999999999999993 for 2 qubits with 11 layers\n",
      "Step 390: Loss = -0.9999999999999929 for 2 qubits with 11 layers\n",
      "Step 400: Loss = -0.9999999999999933 for 2 qubits with 11 layers\n",
      "Step 410: Loss = -0.9999999999999925 for 2 qubits with 11 layers\n",
      "Step 420: Loss = -0.9999999999999925 for 2 qubits with 11 layers\n",
      "Step 430: Loss = -0.9999999999999919 for 2 qubits with 11 layers\n",
      "Step 440: Loss = -0.9999999999999921 for 2 qubits with 11 layers\n",
      "Step 450: Loss = -0.9999999999999923 for 2 qubits with 11 layers\n",
      "Step 460: Loss = -0.9999999999999913 for 2 qubits with 11 layers\n",
      "Step 470: Loss = -0.9999999999999918 for 2 qubits with 11 layers\n",
      "Step 480: Loss = -0.9999999999999916 for 2 qubits with 11 layers\n",
      "Step 490: Loss = -0.9999999999999915 for 2 qubits with 11 layers\n",
      "Running optimization for 2 qubits and 12 layers\n",
      "Step 0: Loss = -0.1750447492335716 for 2 qubits with 12 layers\n",
      "Step 10: Loss = -0.8516716517248779 for 2 qubits with 12 layers\n",
      "Step 20: Loss = -0.9679808158837326 for 2 qubits with 12 layers\n",
      "Step 30: Loss = -0.9950402630353232 for 2 qubits with 12 layers\n",
      "Step 40: Loss = -0.9989745044845533 for 2 qubits with 12 layers\n",
      "Step 50: Loss = -0.9996948753687025 for 2 qubits with 12 layers\n",
      "Step 60: Loss = -0.9996936550930999 for 2 qubits with 12 layers\n",
      "Step 70: Loss = -0.9997549127488954 for 2 qubits with 12 layers\n",
      "Step 80: Loss = -0.9998567119188767 for 2 qubits with 12 layers\n",
      "Step 90: Loss = -0.9999668507613606 for 2 qubits with 12 layers\n",
      "Step 100: Loss = -0.999999771268747 for 2 qubits with 12 layers\n",
      "Step 110: Loss = -0.9999946338635095 for 2 qubits with 12 layers\n",
      "Step 120: Loss = -0.999998741212433 for 2 qubits with 12 layers\n",
      "Step 130: Loss = -0.9999997017627402 for 2 qubits with 12 layers\n",
      "Step 140: Loss = -0.9999997850035699 for 2 qubits with 12 layers\n",
      "Step 150: Loss = -0.9999999378613684 for 2 qubits with 12 layers\n",
      "Step 160: Loss = -0.9999999913580498 for 2 qubits with 12 layers\n",
      "Step 170: Loss = -0.999999985038095 for 2 qubits with 12 layers\n",
      "Step 180: Loss = -0.9999999974195665 for 2 qubits with 12 layers\n",
      "Step 190: Loss = -0.9999999999923586 for 2 qubits with 12 layers\n",
      "Step 200: Loss = -0.9999999997818799 for 2 qubits with 12 layers\n",
      "Step 210: Loss = -0.9999999998345899 for 2 qubits with 12 layers\n",
      "Step 220: Loss = -0.9999999999239619 for 2 qubits with 12 layers\n",
      "Step 230: Loss = -0.99999999997145 for 2 qubits with 12 layers\n",
      "Step 240: Loss = -0.9999999999911698 for 2 qubits with 12 layers\n",
      "Step 250: Loss = -0.9999999999982429 for 2 qubits with 12 layers\n",
      "Step 260: Loss = -0.9999999999999234 for 2 qubits with 12 layers\n",
      "Step 270: Loss = -0.9999999999998069 for 2 qubits with 12 layers\n",
      "Step 280: Loss = -0.9999999999997955 for 2 qubits with 12 layers\n",
      "Step 290: Loss = -0.9999999999999751 for 2 qubits with 12 layers\n",
      "Step 300: Loss = -0.9999999999999764 for 2 qubits with 12 layers\n",
      "Step 310: Loss = -0.9999999999999888 for 2 qubits with 12 layers\n",
      "Step 320: Loss = -0.9999999999999905 for 2 qubits with 12 layers\n",
      "Step 330: Loss = -0.9999999999999907 for 2 qubits with 12 layers\n",
      "Step 340: Loss = -0.9999999999999903 for 2 qubits with 12 layers\n",
      "Step 350: Loss = -0.9999999999999923 for 2 qubits with 12 layers\n",
      "Step 360: Loss = -0.9999999999999909 for 2 qubits with 12 layers\n",
      "Step 370: Loss = -0.9999999999999905 for 2 qubits with 12 layers\n",
      "Step 380: Loss = -0.9999999999999928 for 2 qubits with 12 layers\n",
      "Step 390: Loss = -0.9999999999999917 for 2 qubits with 12 layers\n",
      "Step 400: Loss = -0.999999999999991 for 2 qubits with 12 layers\n",
      "Step 410: Loss = -0.9999999999999907 for 2 qubits with 12 layers\n",
      "Step 420: Loss = -0.9999999999999913 for 2 qubits with 12 layers\n",
      "Step 430: Loss = -0.9999999999999913 for 2 qubits with 12 layers\n",
      "Step 440: Loss = -0.9999999999999896 for 2 qubits with 12 layers\n",
      "Step 450: Loss = -0.999999999999991 for 2 qubits with 12 layers\n",
      "Step 460: Loss = -0.9999999999999918 for 2 qubits with 12 layers\n",
      "Step 470: Loss = -0.9999999999999908 for 2 qubits with 12 layers\n",
      "Step 480: Loss = -0.9999999999999912 for 2 qubits with 12 layers\n",
      "Step 490: Loss = -0.9999999999999903 for 2 qubits with 12 layers\n",
      "Running optimization for 2 qubits and 13 layers\n",
      "Step 0: Loss = -0.1458290245554907 for 2 qubits with 13 layers\n",
      "Step 10: Loss = -0.8532383512384649 for 2 qubits with 13 layers\n",
      "Step 20: Loss = -0.9760982083084504 for 2 qubits with 13 layers\n",
      "Step 30: Loss = -0.9989114686971327 for 2 qubits with 13 layers\n",
      "Step 40: Loss = -0.999670897994176 for 2 qubits with 13 layers\n",
      "Step 50: Loss = -0.9994365087418516 for 2 qubits with 13 layers\n",
      "Step 60: Loss = -0.9997441609310513 for 2 qubits with 13 layers\n",
      "Step 70: Loss = -0.9999430787995228 for 2 qubits with 13 layers\n",
      "Step 80: Loss = -0.9999942956089783 for 2 qubits with 13 layers\n",
      "Step 90: Loss = -0.9999938514180794 for 2 qubits with 13 layers\n",
      "Step 100: Loss = -0.9999893286785482 for 2 qubits with 13 layers\n",
      "Step 110: Loss = -0.9999936727165009 for 2 qubits with 13 layers\n",
      "Step 120: Loss = -0.9999992030020155 for 2 qubits with 13 layers\n",
      "Step 130: Loss = -0.9999998023205254 for 2 qubits with 13 layers\n",
      "Step 140: Loss = -0.9999996925104557 for 2 qubits with 13 layers\n",
      "Step 150: Loss = -0.9999999932520491 for 2 qubits with 13 layers\n",
      "Step 160: Loss = -0.9999999616379168 for 2 qubits with 13 layers\n",
      "Step 170: Loss = -0.9999999996742066 for 2 qubits with 13 layers\n",
      "Step 180: Loss = -0.9999999950673162 for 2 qubits with 13 layers\n",
      "Step 190: Loss = -0.9999999994758424 for 2 qubits with 13 layers\n",
      "Step 200: Loss = -0.9999999998442928 for 2 qubits with 13 layers\n",
      "Step 210: Loss = -0.9999999997711779 for 2 qubits with 13 layers\n",
      "Step 220: Loss = -0.999999999933481 for 2 qubits with 13 layers\n",
      "Step 230: Loss = -0.9999999999900587 for 2 qubits with 13 layers\n",
      "Step 240: Loss = -0.9999999999994078 for 2 qubits with 13 layers\n",
      "Step 250: Loss = -0.9999999999999903 for 2 qubits with 13 layers\n",
      "Step 260: Loss = -0.9999999999999756 for 2 qubits with 13 layers\n",
      "Step 270: Loss = -0.999999999999968 for 2 qubits with 13 layers\n",
      "Step 280: Loss = -0.9999999999999442 for 2 qubits with 13 layers\n",
      "Step 290: Loss = -0.99999999999994 for 2 qubits with 13 layers\n",
      "Step 300: Loss = -0.9999999999999695 for 2 qubits with 13 layers\n",
      "Step 310: Loss = -0.9999999999999873 for 2 qubits with 13 layers\n",
      "Step 320: Loss = -0.9999999999999887 for 2 qubits with 13 layers\n",
      "Step 330: Loss = -0.9999999999999898 for 2 qubits with 13 layers\n",
      "Step 340: Loss = -0.9999999999999898 for 2 qubits with 13 layers\n",
      "Step 350: Loss = -0.9999999999999887 for 2 qubits with 13 layers\n",
      "Step 360: Loss = -0.9999999999999902 for 2 qubits with 13 layers\n",
      "Step 370: Loss = -0.9999999999999909 for 2 qubits with 13 layers\n",
      "Step 380: Loss = -0.9999999999999911 for 2 qubits with 13 layers\n",
      "Step 390: Loss = -0.9999999999999897 for 2 qubits with 13 layers\n",
      "Step 400: Loss = -0.9999999999999918 for 2 qubits with 13 layers\n",
      "Step 410: Loss = -0.9999999999999915 for 2 qubits with 13 layers\n",
      "Step 420: Loss = -0.9999999999999909 for 2 qubits with 13 layers\n",
      "Step 430: Loss = -0.9999999999999912 for 2 qubits with 13 layers\n",
      "Step 440: Loss = -0.9999999999999913 for 2 qubits with 13 layers\n",
      "Step 450: Loss = -0.999999999999989 for 2 qubits with 13 layers\n",
      "Step 460: Loss = -0.999999999999991 for 2 qubits with 13 layers\n",
      "Step 470: Loss = -0.9999999999999912 for 2 qubits with 13 layers\n",
      "Step 480: Loss = -0.9999999999999895 for 2 qubits with 13 layers\n",
      "Step 490: Loss = -0.9999999999999915 for 2 qubits with 13 layers\n",
      "Running optimization for 2 qubits and 14 layers\n",
      "Step 0: Loss = -0.9328695018675639 for 2 qubits with 14 layers\n",
      "Step 10: Loss = -0.9895429692922839 for 2 qubits with 14 layers\n",
      "Step 20: Loss = -0.9988628118427147 for 2 qubits with 14 layers\n",
      "Step 30: Loss = -0.9991078554647167 for 2 qubits with 14 layers\n",
      "Step 40: Loss = -0.9996233216291968 for 2 qubits with 14 layers\n",
      "Step 50: Loss = -0.9998710006533604 for 2 qubits with 14 layers\n",
      "Step 60: Loss = -0.9999823183218143 for 2 qubits with 14 layers\n",
      "Step 70: Loss = -0.9999861734581567 for 2 qubits with 14 layers\n",
      "Step 80: Loss = -0.9999923658204397 for 2 qubits with 14 layers\n",
      "Step 90: Loss = -0.9999968671782986 for 2 qubits with 14 layers\n",
      "Step 100: Loss = -0.9999988145086016 for 2 qubits with 14 layers\n",
      "Step 110: Loss = -0.9999996993247567 for 2 qubits with 14 layers\n",
      "Step 120: Loss = -0.9999999994283923 for 2 qubits with 14 layers\n",
      "Step 130: Loss = -0.9999999504148441 for 2 qubits with 14 layers\n",
      "Step 140: Loss = -0.9999999804407135 for 2 qubits with 14 layers\n",
      "Step 150: Loss = -0.9999999982543466 for 2 qubits with 14 layers\n",
      "Step 160: Loss = -0.9999999979452859 for 2 qubits with 14 layers\n",
      "Step 170: Loss = -0.9999999990793684 for 2 qubits with 14 layers\n",
      "Step 180: Loss = -0.999999999892209 for 2 qubits with 14 layers\n",
      "Step 190: Loss = -0.9999999999790397 for 2 qubits with 14 layers\n",
      "Step 200: Loss = -0.9999999999713614 for 2 qubits with 14 layers\n",
      "Step 210: Loss = -0.9999999999879461 for 2 qubits with 14 layers\n",
      "Step 220: Loss = -0.9999999999972136 for 2 qubits with 14 layers\n",
      "Step 230: Loss = -0.999999999999208 for 2 qubits with 14 layers\n",
      "Step 240: Loss = -0.9999999999993254 for 2 qubits with 14 layers\n",
      "Step 250: Loss = -0.9999999999997511 for 2 qubits with 14 layers\n",
      "Step 260: Loss = -0.9999999999999192 for 2 qubits with 14 layers\n",
      "Step 270: Loss = -0.9999999999999767 for 2 qubits with 14 layers\n",
      "Step 280: Loss = -0.9999999999999708 for 2 qubits with 14 layers\n",
      "Step 290: Loss = -0.9999999999999843 for 2 qubits with 14 layers\n",
      "Step 300: Loss = -0.999999999999989 for 2 qubits with 14 layers\n",
      "Step 310: Loss = -0.9999999999999897 for 2 qubits with 14 layers\n",
      "Step 320: Loss = -0.9999999999999899 for 2 qubits with 14 layers\n",
      "Step 330: Loss = -0.9999999999999891 for 2 qubits with 14 layers\n",
      "Step 340: Loss = -0.9999999999999903 for 2 qubits with 14 layers\n",
      "Step 350: Loss = -0.9999999999999899 for 2 qubits with 14 layers\n",
      "Step 360: Loss = -0.9999999999999863 for 2 qubits with 14 layers\n",
      "Step 370: Loss = -0.9999999999323497 for 2 qubits with 14 layers\n",
      "Step 380: Loss = -0.9998771821265218 for 2 qubits with 14 layers\n",
      "Step 390: Loss = -0.9993119431423851 for 2 qubits with 14 layers\n",
      "Step 400: Loss = -0.9999656650714435 for 2 qubits with 14 layers\n",
      "Step 410: Loss = -0.9998945413526266 for 2 qubits with 14 layers\n",
      "Step 420: Loss = -0.9999999685236378 for 2 qubits with 14 layers\n",
      "Step 430: Loss = -0.9999882248251528 for 2 qubits with 14 layers\n",
      "Step 440: Loss = -0.9999946927438599 for 2 qubits with 14 layers\n",
      "Step 450: Loss = -0.9999979998980852 for 2 qubits with 14 layers\n",
      "Step 460: Loss = -0.9999997284220545 for 2 qubits with 14 layers\n",
      "Step 470: Loss = -0.9999997859340686 for 2 qubits with 14 layers\n",
      "Step 480: Loss = -0.9999999798470753 for 2 qubits with 14 layers\n",
      "Step 490: Loss = -0.9999999757210839 for 2 qubits with 14 layers\n",
      "Running optimization for 2 qubits and 15 layers\n",
      "Step 0: Loss = -0.5000683511597398 for 2 qubits with 15 layers\n",
      "Step 10: Loss = -0.9605521752322908 for 2 qubits with 15 layers\n",
      "Step 20: Loss = -0.9900377875247726 for 2 qubits with 15 layers\n",
      "Step 30: Loss = -0.9981723802335651 for 2 qubits with 15 layers\n",
      "Step 40: Loss = -0.9999903126276805 for 2 qubits with 15 layers\n",
      "Step 50: Loss = -0.999513462961533 for 2 qubits with 15 layers\n",
      "Step 60: Loss = -0.9994396617703956 for 2 qubits with 15 layers\n",
      "Step 70: Loss = -0.9999253784283579 for 2 qubits with 15 layers\n",
      "Step 80: Loss = -0.9999714044929271 for 2 qubits with 15 layers\n",
      "Step 90: Loss = -0.9999801010312372 for 2 qubits with 15 layers\n",
      "Step 100: Loss = -0.9999957503066317 for 2 qubits with 15 layers\n",
      "Step 110: Loss = -0.9999989574157147 for 2 qubits with 15 layers\n",
      "Step 120: Loss = -0.9999986567597987 for 2 qubits with 15 layers\n",
      "Step 130: Loss = -0.999999800174936 for 2 qubits with 15 layers\n",
      "Step 140: Loss = -0.999999999475659 for 2 qubits with 15 layers\n",
      "Step 150: Loss = -0.9999999895579661 for 2 qubits with 15 layers\n",
      "Step 160: Loss = -0.9999999909852753 for 2 qubits with 15 layers\n",
      "Step 170: Loss = -0.9999999963603563 for 2 qubits with 15 layers\n",
      "Step 180: Loss = -0.9999999990813979 for 2 qubits with 15 layers\n",
      "Step 190: Loss = -0.9999999999442735 for 2 qubits with 15 layers\n",
      "Step 200: Loss = -0.9999999999571882 for 2 qubits with 15 layers\n",
      "Step 210: Loss = -0.99999999988834 for 2 qubits with 15 layers\n",
      "Step 220: Loss = -0.999999999961124 for 2 qubits with 15 layers\n",
      "Step 230: Loss = -0.999999999999483 for 2 qubits with 15 layers\n",
      "Step 240: Loss = -0.9999999999934759 for 2 qubits with 15 layers\n",
      "Step 250: Loss = -0.9999999999999583 for 2 qubits with 15 layers\n",
      "Step 260: Loss = -0.999999999999269 for 2 qubits with 15 layers\n",
      "Step 270: Loss = -0.9999999999997842 for 2 qubits with 15 layers\n",
      "Step 280: Loss = -0.9999999999999839 for 2 qubits with 15 layers\n",
      "Step 290: Loss = -0.9999999999999849 for 2 qubits with 15 layers\n",
      "Step 300: Loss = -0.9999999999999847 for 2 qubits with 15 layers\n",
      "Step 310: Loss = -0.9999999999999867 for 2 qubits with 15 layers\n",
      "Step 320: Loss = -0.9999999999999893 for 2 qubits with 15 layers\n",
      "Step 330: Loss = -0.99999999999999 for 2 qubits with 15 layers\n",
      "Step 340: Loss = -0.9999999999999907 for 2 qubits with 15 layers\n",
      "Step 350: Loss = -0.9999999999999889 for 2 qubits with 15 layers\n",
      "Step 360: Loss = -0.99999999999999 for 2 qubits with 15 layers\n",
      "Step 370: Loss = -0.9999999999999905 for 2 qubits with 15 layers\n",
      "Step 380: Loss = -0.9999999999999886 for 2 qubits with 15 layers\n",
      "Step 390: Loss = -0.999999999999988 for 2 qubits with 15 layers\n",
      "Step 400: Loss = -0.99999999999999 for 2 qubits with 15 layers\n",
      "Step 410: Loss = -0.9999999999999897 for 2 qubits with 15 layers\n",
      "Step 420: Loss = -0.9999999999999886 for 2 qubits with 15 layers\n",
      "Step 430: Loss = -0.9999999999999921 for 2 qubits with 15 layers\n",
      "Step 440: Loss = -0.9999999999999862 for 2 qubits with 15 layers\n",
      "Step 450: Loss = -0.9999999973305858 for 2 qubits with 15 layers\n",
      "Step 460: Loss = -0.99998345119644 for 2 qubits with 15 layers\n",
      "Step 470: Loss = -0.9999745080558936 for 2 qubits with 15 layers\n",
      "Step 480: Loss = -0.9999973945239609 for 2 qubits with 15 layers\n",
      "Step 490: Loss = -0.9999927614916224 for 2 qubits with 15 layers\n",
      "Running optimization for 3 qubits and 1 layers\n",
      "Step 0: Loss = -0.16588871118865287 for 3 qubits with 1 layers\n",
      "Step 10: Loss = -0.4498871555226794 for 3 qubits with 1 layers\n",
      "Step 20: Loss = -0.837270368758948 for 3 qubits with 1 layers\n",
      "Step 30: Loss = -1.2631508205335475 for 3 qubits with 1 layers\n",
      "Step 40: Loss = -1.6332469434769172 for 3 qubits with 1 layers\n",
      "Step 50: Loss = -1.8728036466242752 for 3 qubits with 1 layers\n",
      "Step 60: Loss = -1.9765105797521523 for 3 qubits with 1 layers\n",
      "Step 70: Loss = -1.9994965935869398 for 3 qubits with 1 layers\n",
      "Step 80: Loss = -1.998708119209986 for 3 qubits with 1 layers\n",
      "Step 90: Loss = -1.998131591143936 for 3 qubits with 1 layers\n",
      "Step 100: Loss = -1.9992844258208187 for 3 qubits with 1 layers\n",
      "Step 110: Loss = -1.9999391484262343 for 3 qubits with 1 layers\n",
      "Step 120: Loss = -1.9999894638076838 for 3 qubits with 1 layers\n",
      "Step 130: Loss = -1.9999691046159556 for 3 qubits with 1 layers\n",
      "Step 140: Loss = -1.9999899672116577 for 3 qubits with 1 layers\n",
      "Step 150: Loss = -1.999999895654411 for 3 qubits with 1 layers\n",
      "Step 160: Loss = -1.9999991280721703 for 3 qubits with 1 layers\n",
      "Step 170: Loss = -1.9999994453427326 for 3 qubits with 1 layers\n",
      "Step 180: Loss = -1.9999999834667637 for 3 qubits with 1 layers\n",
      "Step 190: Loss = -1.9999999616451307 for 3 qubits with 1 layers\n",
      "Step 200: Loss = -1.9999999762058072 for 3 qubits with 1 layers\n",
      "Step 210: Loss = -1.9999999999536462 for 3 qubits with 1 layers\n",
      "Step 220: Loss = -1.9999999972076612 for 3 qubits with 1 layers\n",
      "Step 230: Loss = -1.9999999993999005 for 3 qubits with 1 layers\n",
      "Step 240: Loss = -1.999999999899336 for 3 qubits with 1 layers\n",
      "Step 250: Loss = -1.9999999998537403 for 3 qubits with 1 layers\n",
      "Step 260: Loss = -1.9999999999999147 for 3 qubits with 1 layers\n",
      "Step 270: Loss = -1.9999999999804718 for 3 qubits with 1 layers\n",
      "Step 280: Loss = -1.9999999999995515 for 3 qubits with 1 layers\n",
      "Step 290: Loss = -1.9999999999977398 for 3 qubits with 1 layers\n",
      "Step 300: Loss = -1.9999999999998797 for 3 qubits with 1 layers\n",
      "Step 310: Loss = -1.9999999999997173 for 3 qubits with 1 layers\n",
      "Step 320: Loss = -1.9999999999999893 for 3 qubits with 1 layers\n",
      "Step 330: Loss = -1.9999999999999585 for 3 qubits with 1 layers\n",
      "Step 340: Loss = -1.999999999999998 for 3 qubits with 1 layers\n",
      "Step 350: Loss = -1.999999999999993 for 3 qubits with 1 layers\n",
      "Step 360: Loss = -1.999999999999997 for 3 qubits with 1 layers\n",
      "Step 370: Loss = -1.999999999999997 for 3 qubits with 1 layers\n",
      "Step 380: Loss = -1.999999999999997 for 3 qubits with 1 layers\n",
      "Step 390: Loss = -1.9999999999999973 for 3 qubits with 1 layers\n",
      "Step 400: Loss = -1.9999999999999971 for 3 qubits with 1 layers\n",
      "Step 410: Loss = -1.999999999999997 for 3 qubits with 1 layers\n",
      "Step 420: Loss = -1.9999999999999978 for 3 qubits with 1 layers\n",
      "Step 430: Loss = -1.9999999999999962 for 3 qubits with 1 layers\n",
      "Step 440: Loss = -1.999999999999997 for 3 qubits with 1 layers\n",
      "Step 450: Loss = -1.9999999999999964 for 3 qubits with 1 layers\n",
      "Step 460: Loss = -1.9999999999999976 for 3 qubits with 1 layers\n",
      "Step 470: Loss = -1.9999999999999978 for 3 qubits with 1 layers\n",
      "Step 480: Loss = -1.9999999999999964 for 3 qubits with 1 layers\n",
      "Step 490: Loss = -1.9999999999999962 for 3 qubits with 1 layers\n",
      "Running optimization for 3 qubits and 2 layers\n",
      "Step 0: Loss = -0.5866971699474982 for 3 qubits with 2 layers\n",
      "Step 10: Loss = -1.2566804637828537 for 3 qubits with 2 layers\n",
      "Step 20: Loss = -1.67593867828821 for 3 qubits with 2 layers\n",
      "Step 30: Loss = -1.8109478675998771 for 3 qubits with 2 layers\n",
      "Step 40: Loss = -1.900349575570789 for 3 qubits with 2 layers\n",
      "Step 50: Loss = -1.9564811835710687 for 3 qubits with 2 layers\n",
      "Step 60: Loss = -1.9765941641696902 for 3 qubits with 2 layers\n",
      "Step 70: Loss = -1.9873016282159375 for 3 qubits with 2 layers\n",
      "Step 80: Loss = -1.9920194126599875 for 3 qubits with 2 layers\n",
      "Step 90: Loss = -1.9947043425920288 for 3 qubits with 2 layers\n",
      "Step 100: Loss = -1.9962212602290548 for 3 qubits with 2 layers\n",
      "Step 110: Loss = -1.9972109282091766 for 3 qubits with 2 layers\n",
      "Step 120: Loss = -1.9978664889031874 for 3 qubits with 2 layers\n",
      "Step 130: Loss = -1.9983366915543699 for 3 qubits with 2 layers\n",
      "Step 140: Loss = -1.998679805158735 for 3 qubits with 2 layers\n",
      "Step 150: Loss = -1.9989382242150995 for 3 qubits with 2 layers\n",
      "Step 160: Loss = -1.9991367605718855 for 3 qubits with 2 layers\n",
      "Step 170: Loss = -1.9992915373243267 for 3 qubits with 2 layers\n",
      "Step 180: Loss = -1.9994138663305066 for 3 qubits with 2 layers\n",
      "Step 190: Loss = -1.9995116352793376 for 3 qubits with 2 layers\n",
      "Step 200: Loss = -1.999590526376973 for 3 qubits with 2 layers\n",
      "Step 210: Loss = -1.9996547329442804 for 3 qubits with 2 layers\n",
      "Step 220: Loss = -1.9997073901172737 for 3 qubits with 2 layers\n",
      "Step 230: Loss = -1.9997508733640044 for 3 qubits with 2 layers\n",
      "Step 240: Loss = -1.9997870056941334 for 3 qubits with 2 layers\n",
      "Step 250: Loss = -1.9998172008778399 for 3 qubits with 2 layers\n",
      "Step 260: Loss = -1.999842566013935 for 3 qubits with 2 layers\n",
      "Step 270: Loss = -1.9998639756164551 for 3 qubits with 2 layers\n",
      "Step 280: Loss = -1.999882126045215 for 3 qubits with 2 layers\n",
      "Step 290: Loss = -1.9998975759107265 for 3 qubits with 2 layers\n",
      "Step 300: Loss = -1.9999107763879356 for 3 qubits with 2 layers\n",
      "Step 310: Loss = -1.9999220941953988 for 3 qubits with 2 layers\n",
      "Step 320: Loss = -1.999931829164338 for 3 qubits with 2 layers\n",
      "Step 330: Loss = -1.9999402277892906 for 3 qubits with 2 layers\n",
      "Step 340: Loss = -1.9999474937590196 for 3 qubits with 2 layers\n",
      "Step 350: Loss = -1.9999537962012848 for 3 qubits with 2 layers\n",
      "Step 360: Loss = -1.9999592761818121 for 3 qubits with 2 layers\n",
      "Step 370: Loss = -1.9999640518598958 for 3 qubits with 2 layers\n",
      "Step 380: Loss = -1.9999682226033222 for 3 qubits with 2 layers\n",
      "Step 390: Loss = -1.9999718722911528 for 3 qubits with 2 layers\n",
      "Step 400: Loss = -1.9999750719791354 for 3 qubits with 2 layers\n",
      "Step 410: Loss = -1.9999778820615512 for 3 qubits with 2 layers\n",
      "Step 420: Loss = -1.999980354033251 for 3 qubits with 2 layers\n",
      "Step 430: Loss = -1.999982531932262 for 3 qubits with 2 layers\n",
      "Step 440: Loss = -1.9999844535261255 for 3 qubits with 2 layers\n",
      "Step 450: Loss = -1.9999861512914807 for 3 qubits with 2 layers\n",
      "Step 460: Loss = -1.9999876532261018 for 3 qubits with 2 layers\n",
      "Step 470: Loss = -1.9999889835246387 for 3 qubits with 2 layers\n",
      "Step 480: Loss = -1.9999901631428765 for 3 qubits with 2 layers\n",
      "Step 490: Loss = -1.9999912102705721 for 3 qubits with 2 layers\n",
      "Running optimization for 3 qubits and 3 layers\n",
      "Step 0: Loss = -0.6912238911368092 for 3 qubits with 3 layers\n",
      "Step 10: Loss = -1.4381711169769917 for 3 qubits with 3 layers\n",
      "Step 20: Loss = -1.9389525497993554 for 3 qubits with 3 layers\n",
      "Step 30: Loss = -1.9761370878115314 for 3 qubits with 3 layers\n",
      "Step 40: Loss = -1.9732359362825043 for 3 qubits with 3 layers\n",
      "Step 50: Loss = -1.9988955804241595 for 3 qubits with 3 layers\n",
      "Step 60: Loss = -1.9969705229348858 for 3 qubits with 3 layers\n",
      "Step 70: Loss = -1.9996069913958805 for 3 qubits with 3 layers\n",
      "Step 80: Loss = -1.9997157383686472 for 3 qubits with 3 layers\n",
      "Step 90: Loss = -1.9999276127015098 for 3 qubits with 3 layers\n",
      "Step 100: Loss = -1.9999540561353233 for 3 qubits with 3 layers\n",
      "Step 110: Loss = -1.9999960577222504 for 3 qubits with 3 layers\n",
      "Step 120: Loss = -1.9999919505268455 for 3 qubits with 3 layers\n",
      "Step 130: Loss = -1.999999972840655 for 3 qubits with 3 layers\n",
      "Step 140: Loss = -1.999999053518975 for 3 qubits with 3 layers\n",
      "Step 150: Loss = -1.9999998064891198 for 3 qubits with 3 layers\n",
      "Step 160: Loss = -1.9999999818255978 for 3 qubits with 3 layers\n",
      "Step 170: Loss = -1.9999999536005335 for 3 qubits with 3 layers\n",
      "Step 180: Loss = -1.9999999890664708 for 3 qubits with 3 layers\n",
      "Step 190: Loss = -1.9999999999422955 for 3 qubits with 3 layers\n",
      "Step 200: Loss = -1.999999998932382 for 3 qubits with 3 layers\n",
      "Step 210: Loss = -1.999999999185357 for 3 qubits with 3 layers\n",
      "Step 220: Loss = -1.9999999997786484 for 3 qubits with 3 layers\n",
      "Step 230: Loss = -1.9999999999754794 for 3 qubits with 3 layers\n",
      "Step 240: Loss = -1.9999999999985438 for 3 qubits with 3 layers\n",
      "Step 250: Loss = -1.9999999999971305 for 3 qubits with 3 layers\n",
      "Step 260: Loss = -1.999999999997335 for 3 qubits with 3 layers\n",
      "Step 270: Loss = -1.9999999999985147 for 3 qubits with 3 layers\n",
      "Step 280: Loss = -1.9999999999994276 for 3 qubits with 3 layers\n",
      "Step 290: Loss = -1.9999999999997837 for 3 qubits with 3 layers\n",
      "Step 300: Loss = -1.9999999999999152 for 3 qubits with 3 layers\n",
      "Step 310: Loss = -1.999999999999966 for 3 qubits with 3 layers\n",
      "Step 320: Loss = -1.999999999999984 for 3 qubits with 3 layers\n",
      "Step 330: Loss = -1.9999999999999878 for 3 qubits with 3 layers\n",
      "Step 340: Loss = -1.9999999999999927 for 3 qubits with 3 layers\n",
      "Step 350: Loss = -1.9999999999999916 for 3 qubits with 3 layers\n",
      "Step 360: Loss = -1.999999999999995 for 3 qubits with 3 layers\n",
      "Step 370: Loss = -1.999999999999992 for 3 qubits with 3 layers\n",
      "Step 380: Loss = -1.9999999999999911 for 3 qubits with 3 layers\n",
      "Step 390: Loss = -1.9999999999999953 for 3 qubits with 3 layers\n",
      "Step 400: Loss = -1.9999999999999938 for 3 qubits with 3 layers\n",
      "Step 410: Loss = -1.9999999999999944 for 3 qubits with 3 layers\n",
      "Step 420: Loss = -1.999999999999992 for 3 qubits with 3 layers\n",
      "Step 430: Loss = -1.9999999999999942 for 3 qubits with 3 layers\n",
      "Step 440: Loss = -1.9999999999999938 for 3 qubits with 3 layers\n",
      "Step 450: Loss = -1.999999999999992 for 3 qubits with 3 layers\n",
      "Step 460: Loss = -1.9999999999999911 for 3 qubits with 3 layers\n",
      "Step 470: Loss = -1.9999999999999911 for 3 qubits with 3 layers\n",
      "Step 480: Loss = -1.9999999999999925 for 3 qubits with 3 layers\n",
      "Step 490: Loss = -1.999999999999992 for 3 qubits with 3 layers\n",
      "Running optimization for 3 qubits and 4 layers\n",
      "Step 0: Loss = -1.632296008914049 for 3 qubits with 4 layers\n",
      "Step 10: Loss = -1.964658319467706 for 3 qubits with 4 layers\n",
      "Step 20: Loss = -1.9953701496807676 for 3 qubits with 4 layers\n",
      "Step 30: Loss = -1.9947657527637865 for 3 qubits with 4 layers\n",
      "Step 40: Loss = -1.999487563777926 for 3 qubits with 4 layers\n",
      "Step 50: Loss = -1.9995196383825689 for 3 qubits with 4 layers\n",
      "Step 60: Loss = -1.9996713592250472 for 3 qubits with 4 layers\n",
      "Step 70: Loss = -1.999937040989185 for 3 qubits with 4 layers\n",
      "Step 80: Loss = -1.9999537272943901 for 3 qubits with 4 layers\n",
      "Step 90: Loss = -1.9999956817368632 for 3 qubits with 4 layers\n",
      "Step 100: Loss = -1.9999948063581632 for 3 qubits with 4 layers\n",
      "Step 110: Loss = -1.9999998694993208 for 3 qubits with 4 layers\n",
      "Step 120: Loss = -1.9999991825472763 for 3 qubits with 4 layers\n",
      "Step 130: Loss = -1.9999998682508595 for 3 qubits with 4 layers\n",
      "Step 140: Loss = -1.999999930582857 for 3 qubits with 4 layers\n",
      "Step 150: Loss = -1.9999999809333833 for 3 qubits with 4 layers\n",
      "Step 160: Loss = -1.9999999903483745 for 3 qubits with 4 layers\n",
      "Step 170: Loss = -1.999999996133687 for 3 qubits with 4 layers\n",
      "Step 180: Loss = -1.999999999255957 for 3 qubits with 4 layers\n",
      "Step 190: Loss = -1.9999999997072937 for 3 qubits with 4 layers\n",
      "Step 200: Loss = -1.9999999998041522 for 3 qubits with 4 layers\n",
      "Step 210: Loss = -1.9999999999485651 for 3 qubits with 4 layers\n",
      "Step 220: Loss = -1.9999999999915445 for 3 qubits with 4 layers\n",
      "Step 230: Loss = -1.999999999992887 for 3 qubits with 4 layers\n",
      "Step 240: Loss = -1.9999999999965623 for 3 qubits with 4 layers\n",
      "Step 250: Loss = -1.9999999999988363 for 3 qubits with 4 layers\n",
      "Step 260: Loss = -1.9999999999997442 for 3 qubits with 4 layers\n",
      "Step 270: Loss = -1.9999999999999203 for 3 qubits with 4 layers\n",
      "Step 280: Loss = -1.9999999999999554 for 3 qubits with 4 layers\n",
      "Step 290: Loss = -1.9999999999999718 for 3 qubits with 4 layers\n",
      "Step 300: Loss = -1.9999999999999831 for 3 qubits with 4 layers\n",
      "Step 310: Loss = -1.9999999999999947 for 3 qubits with 4 layers\n",
      "Step 320: Loss = -1.9999999999999947 for 3 qubits with 4 layers\n",
      "Step 330: Loss = -1.9999999999999902 for 3 qubits with 4 layers\n",
      "Step 340: Loss = -1.9999999999999905 for 3 qubits with 4 layers\n",
      "Step 350: Loss = -1.9999999999999933 for 3 qubits with 4 layers\n",
      "Step 360: Loss = -1.9999999999999933 for 3 qubits with 4 layers\n",
      "Step 370: Loss = -1.9999999999999885 for 3 qubits with 4 layers\n",
      "Step 380: Loss = -1.99999999999999 for 3 qubits with 4 layers\n",
      "Step 390: Loss = -1.9999999999999898 for 3 qubits with 4 layers\n",
      "Step 400: Loss = -1.9999999999999931 for 3 qubits with 4 layers\n",
      "Step 410: Loss = -1.9999999999999925 for 3 qubits with 4 layers\n",
      "Step 420: Loss = -1.9999999999999902 for 3 qubits with 4 layers\n",
      "Step 430: Loss = -1.9999999999999933 for 3 qubits with 4 layers\n",
      "Step 440: Loss = -1.9999999999999916 for 3 qubits with 4 layers\n",
      "Step 450: Loss = -1.9999999999999898 for 3 qubits with 4 layers\n",
      "Step 460: Loss = -1.9999999999629865 for 3 qubits with 4 layers\n",
      "Step 470: Loss = -1.999938259491113 for 3 qubits with 4 layers\n",
      "Step 480: Loss = -1.9999826708865578 for 3 qubits with 4 layers\n",
      "Step 490: Loss = -1.9999984282366703 for 3 qubits with 4 layers\n",
      "Running optimization for 3 qubits and 5 layers\n",
      "Step 0: Loss = -0.705680531386603 for 3 qubits with 5 layers\n",
      "Step 10: Loss = -1.5631411273407858 for 3 qubits with 5 layers\n",
      "Step 20: Loss = -1.8784376957159075 for 3 qubits with 5 layers\n",
      "Step 30: Loss = -1.9722918969786358 for 3 qubits with 5 layers\n",
      "Step 40: Loss = -1.9919930289348975 for 3 qubits with 5 layers\n",
      "Step 50: Loss = -1.9935765469884257 for 3 qubits with 5 layers\n",
      "Step 60: Loss = -1.9975996480528868 for 3 qubits with 5 layers\n",
      "Step 70: Loss = -1.9995872741615672 for 3 qubits with 5 layers\n",
      "Step 80: Loss = -1.9998638570310545 for 3 qubits with 5 layers\n",
      "Step 90: Loss = -1.999955270566693 for 3 qubits with 5 layers\n",
      "Step 100: Loss = -1.9999814982344857 for 3 qubits with 5 layers\n",
      "Step 110: Loss = -1.9999926536059207 for 3 qubits with 5 layers\n",
      "Step 120: Loss = -1.9999980903253625 for 3 qubits with 5 layers\n",
      "Step 130: Loss = -1.9999987893418116 for 3 qubits with 5 layers\n",
      "Step 140: Loss = -1.999999404851459 for 3 qubits with 5 layers\n",
      "Step 150: Loss = -1.999999834109059 for 3 qubits with 5 layers\n",
      "Step 160: Loss = -1.999999917896667 for 3 qubits with 5 layers\n",
      "Step 170: Loss = -1.9999999790615397 for 3 qubits with 5 layers\n",
      "Step 180: Loss = -1.9999999994126014 for 3 qubits with 5 layers\n",
      "Step 190: Loss = -1.9999999960572068 for 3 qubits with 5 layers\n",
      "Step 200: Loss = -1.999999999228393 for 3 qubits with 5 layers\n",
      "Step 210: Loss = -1.9999999997652569 for 3 qubits with 5 layers\n",
      "Step 220: Loss = -1.9999999997780515 for 3 qubits with 5 layers\n",
      "Step 230: Loss = -1.999999999966453 for 3 qubits with 5 layers\n",
      "Step 240: Loss = -1.9999999999682712 for 3 qubits with 5 layers\n",
      "Step 250: Loss = -1.9999999999963805 for 3 qubits with 5 layers\n",
      "Step 260: Loss = -1.9999999999982774 for 3 qubits with 5 layers\n",
      "Step 270: Loss = -1.9999999999989304 for 3 qubits with 5 layers\n",
      "Step 280: Loss = -1.9999999999995635 for 3 qubits with 5 layers\n",
      "Step 290: Loss = -1.9999999999999871 for 3 qubits with 5 layers\n",
      "Step 300: Loss = -1.9999999999999427 for 3 qubits with 5 layers\n",
      "Step 310: Loss = -1.9999999999999742 for 3 qubits with 5 layers\n",
      "Step 320: Loss = -1.9999999999999827 for 3 qubits with 5 layers\n",
      "Step 330: Loss = -1.999999999999989 for 3 qubits with 5 layers\n",
      "Step 340: Loss = -1.9999999999999856 for 3 qubits with 5 layers\n",
      "Step 350: Loss = -1.999999999999986 for 3 qubits with 5 layers\n",
      "Step 360: Loss = -1.9999999999999911 for 3 qubits with 5 layers\n",
      "Step 370: Loss = -1.999999999999988 for 3 qubits with 5 layers\n",
      "Step 380: Loss = -1.9999999999999925 for 3 qubits with 5 layers\n",
      "Step 390: Loss = -1.999999999999989 for 3 qubits with 5 layers\n",
      "Step 400: Loss = -1.9999999999999902 for 3 qubits with 5 layers\n",
      "Step 410: Loss = -1.999999999999989 for 3 qubits with 5 layers\n",
      "Step 420: Loss = -1.9999999999999867 for 3 qubits with 5 layers\n",
      "Step 430: Loss = -1.9999999999999885 for 3 qubits with 5 layers\n",
      "Step 440: Loss = -1.9999999999999862 for 3 qubits with 5 layers\n",
      "Step 450: Loss = -1.9999999999999911 for 3 qubits with 5 layers\n",
      "Step 460: Loss = -1.99999999999999 for 3 qubits with 5 layers\n",
      "Step 470: Loss = -1.9999999999999893 for 3 qubits with 5 layers\n",
      "Step 480: Loss = -1.9999999999999885 for 3 qubits with 5 layers\n",
      "Step 490: Loss = -1.9999999999999918 for 3 qubits with 5 layers\n",
      "Running optimization for 3 qubits and 6 layers\n",
      "Step 0: Loss = -0.39767747684733806 for 3 qubits with 6 layers\n",
      "Step 10: Loss = -1.7396891835128818 for 3 qubits with 6 layers\n",
      "Step 20: Loss = -1.9352330380388822 for 3 qubits with 6 layers\n",
      "Step 30: Loss = -1.9921904032140825 for 3 qubits with 6 layers\n",
      "Step 40: Loss = -1.9897356775752046 for 3 qubits with 6 layers\n",
      "Step 50: Loss = -1.9976394332543312 for 3 qubits with 6 layers\n",
      "Step 60: Loss = -1.9998019145547525 for 3 qubits with 6 layers\n",
      "Step 70: Loss = -1.9995051161203923 for 3 qubits with 6 layers\n",
      "Step 80: Loss = -1.9997840937054705 for 3 qubits with 6 layers\n",
      "Step 90: Loss = -1.99997335333717 for 3 qubits with 6 layers\n",
      "Step 100: Loss = -1.999996921547126 for 3 qubits with 6 layers\n",
      "Step 110: Loss = -1.9999969229353658 for 3 qubits with 6 layers\n",
      "Step 120: Loss = -1.9999984775295372 for 3 qubits with 6 layers\n",
      "Step 130: Loss = -1.9999993965039984 for 3 qubits with 6 layers\n",
      "Step 140: Loss = -1.9999997358861785 for 3 qubits with 6 layers\n",
      "Step 150: Loss = -1.999999889788592 for 3 qubits with 6 layers\n",
      "Step 160: Loss = -1.9999999642872524 for 3 qubits with 6 layers\n",
      "Step 170: Loss = -1.9999999905969181 for 3 qubits with 6 layers\n",
      "Step 180: Loss = -1.9999999981886805 for 3 qubits with 6 layers\n",
      "Step 190: Loss = -1.999999999133339 for 3 qubits with 6 layers\n",
      "Step 200: Loss = -1.999999999247494 for 3 qubits with 6 layers\n",
      "Step 210: Loss = -1.9999999995874 for 3 qubits with 6 layers\n",
      "Step 220: Loss = -1.99999999991001 for 3 qubits with 6 layers\n",
      "Step 230: Loss = -1.9999999999890439 for 3 qubits with 6 layers\n",
      "Step 240: Loss = -1.9999999999842988 for 3 qubits with 6 layers\n",
      "Step 250: Loss = -1.999999999996287 for 3 qubits with 6 layers\n",
      "Step 260: Loss = -1.9999999999996114 for 3 qubits with 6 layers\n",
      "Step 270: Loss = -1.9999999999992393 for 3 qubits with 6 layers\n",
      "Step 280: Loss = -1.9999999999999094 for 3 qubits with 6 layers\n",
      "Step 290: Loss = -1.999999999999888 for 3 qubits with 6 layers\n",
      "Step 300: Loss = -1.9999999999999813 for 3 qubits with 6 layers\n",
      "Step 310: Loss = -1.9999999999999756 for 3 qubits with 6 layers\n",
      "Step 320: Loss = -1.9999999999999842 for 3 qubits with 6 layers\n",
      "Step 330: Loss = -1.9999999999999898 for 3 qubits with 6 layers\n",
      "Step 340: Loss = -1.9999999999999893 for 3 qubits with 6 layers\n",
      "Step 350: Loss = -1.9999999999999862 for 3 qubits with 6 layers\n",
      "Step 360: Loss = -1.9999999999999867 for 3 qubits with 6 layers\n",
      "Step 370: Loss = -1.999999999999988 for 3 qubits with 6 layers\n",
      "Step 380: Loss = -1.999999999999985 for 3 qubits with 6 layers\n",
      "Step 390: Loss = -1.9999999999999831 for 3 qubits with 6 layers\n",
      "Step 400: Loss = -1.9999999999999853 for 3 qubits with 6 layers\n",
      "Step 410: Loss = -1.9999999999999842 for 3 qubits with 6 layers\n",
      "Step 420: Loss = -1.9999999999999858 for 3 qubits with 6 layers\n",
      "Step 430: Loss = -1.999999999999991 for 3 qubits with 6 layers\n",
      "Step 440: Loss = -1.999999999999984 for 3 qubits with 6 layers\n",
      "Step 450: Loss = -1.9999999999999905 for 3 qubits with 6 layers\n",
      "Step 460: Loss = -1.9999999999999871 for 3 qubits with 6 layers\n",
      "Step 470: Loss = -1.999999999999988 for 3 qubits with 6 layers\n",
      "Step 480: Loss = -1.9999999999999873 for 3 qubits with 6 layers\n",
      "Step 490: Loss = -1.9999999999999871 for 3 qubits with 6 layers\n",
      "Running optimization for 3 qubits and 7 layers\n",
      "Step 0: Loss = -1.4710142890259494 for 3 qubits with 7 layers\n",
      "Step 10: Loss = -1.941249561294169 for 3 qubits with 7 layers\n",
      "Step 20: Loss = -1.995666895614752 for 3 qubits with 7 layers\n",
      "Step 30: Loss = -1.9954731261255318 for 3 qubits with 7 layers\n",
      "Step 40: Loss = -1.9965218719503415 for 3 qubits with 7 layers\n",
      "Step 50: Loss = -1.9987749453274786 for 3 qubits with 7 layers\n",
      "Step 60: Loss = -1.9996704517553012 for 3 qubits with 7 layers\n",
      "Step 70: Loss = -1.9998650097144615 for 3 qubits with 7 layers\n",
      "Step 80: Loss = -1.999960396687304 for 3 qubits with 7 layers\n",
      "Step 90: Loss = -1.9999822430738823 for 3 qubits with 7 layers\n",
      "Step 100: Loss = -1.9999936815669126 for 3 qubits with 7 layers\n",
      "Step 110: Loss = -1.9999984849714034 for 3 qubits with 7 layers\n",
      "Step 120: Loss = -1.9999996084985585 for 3 qubits with 7 layers\n",
      "Step 130: Loss = -1.9999997503364617 for 3 qubits with 7 layers\n",
      "Step 140: Loss = -1.9999998504102452 for 3 qubits with 7 layers\n",
      "Step 150: Loss = -1.9999999743636507 for 3 qubits with 7 layers\n",
      "Step 160: Loss = -1.9999999946148916 for 3 qubits with 7 layers\n",
      "Step 170: Loss = -1.9999999924930023 for 3 qubits with 7 layers\n",
      "Step 180: Loss = -1.9999999996730349 for 3 qubits with 7 layers\n",
      "Step 190: Loss = -1.9999999989202073 for 3 qubits with 7 layers\n",
      "Step 200: Loss = -1.9999999999812559 for 3 qubits with 7 layers\n",
      "Step 210: Loss = -1.9999999998579008 for 3 qubits with 7 layers\n",
      "Step 220: Loss = -1.9999999999908853 for 3 qubits with 7 layers\n",
      "Step 230: Loss = -1.9999999999899822 for 3 qubits with 7 layers\n",
      "Step 240: Loss = -1.9999999999950364 for 3 qubits with 7 layers\n",
      "Step 250: Loss = -1.9999999999988862 for 3 qubits with 7 layers\n",
      "Step 260: Loss = -1.9999999999995963 for 3 qubits with 7 layers\n",
      "Step 270: Loss = -1.9999999999998246 for 3 qubits with 7 layers\n",
      "Step 280: Loss = -1.9999999999999374 for 3 qubits with 7 layers\n",
      "Step 290: Loss = -1.9999999999999778 for 3 qubits with 7 layers\n",
      "Step 300: Loss = -1.9999999999999827 for 3 qubits with 7 layers\n",
      "Step 310: Loss = -1.9999999999999827 for 3 qubits with 7 layers\n",
      "Step 320: Loss = -1.999999999999981 for 3 qubits with 7 layers\n",
      "Step 330: Loss = -1.9999999999999845 for 3 qubits with 7 layers\n",
      "Step 340: Loss = -1.9999999999999831 for 3 qubits with 7 layers\n",
      "Step 350: Loss = -1.9999999999999878 for 3 qubits with 7 layers\n",
      "Step 360: Loss = -1.9999999999999842 for 3 qubits with 7 layers\n",
      "Step 370: Loss = -1.9999999999999845 for 3 qubits with 7 layers\n",
      "Step 380: Loss = -1.9999999999999851 for 3 qubits with 7 layers\n",
      "Step 390: Loss = -1.9999999999999831 for 3 qubits with 7 layers\n",
      "Step 400: Loss = -1.9999999999999836 for 3 qubits with 7 layers\n",
      "Step 410: Loss = -1.9999999999999853 for 3 qubits with 7 layers\n",
      "Step 420: Loss = -1.999999999999984 for 3 qubits with 7 layers\n",
      "Step 430: Loss = -1.999999999999983 for 3 qubits with 7 layers\n",
      "Step 440: Loss = -1.999999999999984 for 3 qubits with 7 layers\n",
      "Step 450: Loss = -1.9999999999999858 for 3 qubits with 7 layers\n",
      "Step 460: Loss = -1.9999999999999845 for 3 qubits with 7 layers\n",
      "Step 470: Loss = -1.999999999999985 for 3 qubits with 7 layers\n",
      "Step 480: Loss = -1.9999999999999845 for 3 qubits with 7 layers\n",
      "Step 490: Loss = -1.9999999999999836 for 3 qubits with 7 layers\n",
      "Running optimization for 3 qubits and 8 layers\n",
      "Step 0: Loss = -0.12355976617099579 for 3 qubits with 8 layers\n",
      "Step 10: Loss = -1.9323664535532008 for 3 qubits with 8 layers\n",
      "Step 20: Loss = -1.9211793130130035 for 3 qubits with 8 layers\n",
      "Step 30: Loss = -1.9657618935476429 for 3 qubits with 8 layers\n",
      "Step 40: Loss = -1.9890610725252138 for 3 qubits with 8 layers\n",
      "Step 50: Loss = -1.99402216435925 for 3 qubits with 8 layers\n",
      "Step 60: Loss = -1.9980056074088752 for 3 qubits with 8 layers\n",
      "Step 70: Loss = -1.9996065236435012 for 3 qubits with 8 layers\n",
      "Step 80: Loss = -1.9999276427545696 for 3 qubits with 8 layers\n",
      "Step 90: Loss = -1.9999732296569592 for 3 qubits with 8 layers\n",
      "Step 100: Loss = -1.9999802688321682 for 3 qubits with 8 layers\n",
      "Step 110: Loss = -1.9999881127808057 for 3 qubits with 8 layers\n",
      "Step 120: Loss = -1.9999968029131416 for 3 qubits with 8 layers\n",
      "Step 130: Loss = -1.9999997082267056 for 3 qubits with 8 layers\n",
      "Step 140: Loss = -1.9999997123984297 for 3 qubits with 8 layers\n",
      "Step 150: Loss = -1.999999885128124 for 3 qubits with 8 layers\n",
      "Step 160: Loss = -1.9999999607592942 for 3 qubits with 8 layers\n",
      "Step 170: Loss = -1.9999999708807046 for 3 qubits with 8 layers\n",
      "Step 180: Loss = -1.999999989478444 for 3 qubits with 8 layers\n",
      "Step 190: Loss = -1.9999999978980527 for 3 qubits with 8 layers\n",
      "Step 200: Loss = -1.9999999995626707 for 3 qubits with 8 layers\n",
      "Step 210: Loss = -1.999999999340232 for 3 qubits with 8 layers\n",
      "Step 220: Loss = -1.9999999999343403 for 3 qubits with 8 layers\n",
      "Step 230: Loss = -1.9999999999494298 for 3 qubits with 8 layers\n",
      "Step 240: Loss = -1.9999999999816238 for 3 qubits with 8 layers\n",
      "Step 250: Loss = -1.9999999999917155 for 3 qubits with 8 layers\n",
      "Step 260: Loss = -1.9999999999985714 for 3 qubits with 8 layers\n",
      "Step 270: Loss = -1.9999999999989706 for 3 qubits with 8 layers\n",
      "Step 280: Loss = -1.9999999999998566 for 3 qubits with 8 layers\n",
      "Step 290: Loss = -1.9999999999999107 for 3 qubits with 8 layers\n",
      "Step 300: Loss = -1.999999999999939 for 3 qubits with 8 layers\n",
      "Step 310: Loss = -1.9999999999999711 for 3 qubits with 8 layers\n",
      "Step 320: Loss = -1.9999999999999793 for 3 qubits with 8 layers\n",
      "Step 330: Loss = -1.9999999999999816 for 3 qubits with 8 layers\n",
      "Step 340: Loss = -1.9999999999999818 for 3 qubits with 8 layers\n",
      "Step 350: Loss = -1.9999999999999825 for 3 qubits with 8 layers\n",
      "Step 360: Loss = -1.999999999999981 for 3 qubits with 8 layers\n",
      "Step 370: Loss = -1.9999999999999818 for 3 qubits with 8 layers\n",
      "Step 380: Loss = -1.9999999999999853 for 3 qubits with 8 layers\n",
      "Step 390: Loss = -1.9999999999999847 for 3 qubits with 8 layers\n",
      "Step 400: Loss = -1.99999999999998 for 3 qubits with 8 layers\n",
      "Step 410: Loss = -1.9999999999999836 for 3 qubits with 8 layers\n",
      "Step 420: Loss = -1.99999999999998 for 3 qubits with 8 layers\n",
      "Step 430: Loss = -1.9999999999999858 for 3 qubits with 8 layers\n",
      "Step 440: Loss = -1.9999999999999862 for 3 qubits with 8 layers\n",
      "Step 450: Loss = -1.9999999999999833 for 3 qubits with 8 layers\n",
      "Step 460: Loss = -1.9999999999999818 for 3 qubits with 8 layers\n",
      "Step 470: Loss = -1.9999999999999778 for 3 qubits with 8 layers\n",
      "Step 480: Loss = -1.9999999999999802 for 3 qubits with 8 layers\n",
      "Step 490: Loss = -1.999999999999981 for 3 qubits with 8 layers\n",
      "Running optimization for 3 qubits and 9 layers\n",
      "Step 0: Loss = -0.6385926467273497 for 3 qubits with 9 layers\n",
      "Step 10: Loss = -1.8957750535966869 for 3 qubits with 9 layers\n",
      "Step 20: Loss = -1.9454022610827564 for 3 qubits with 9 layers\n",
      "Step 30: Loss = -1.9756225130151197 for 3 qubits with 9 layers\n",
      "Step 40: Loss = -1.9983875385202658 for 3 qubits with 9 layers\n",
      "Step 50: Loss = -1.9978082907633306 for 3 qubits with 9 layers\n",
      "Step 60: Loss = -1.9987279321326814 for 3 qubits with 9 layers\n",
      "Step 70: Loss = -1.999612232298094 for 3 qubits with 9 layers\n",
      "Step 80: Loss = -1.999908348435998 for 3 qubits with 9 layers\n",
      "Step 90: Loss = -1.9999833682803352 for 3 qubits with 9 layers\n",
      "Step 100: Loss = -1.999997524720502 for 3 qubits with 9 layers\n",
      "Step 110: Loss = -1.9999963312960014 for 3 qubits with 9 layers\n",
      "Step 120: Loss = -1.9999983775564072 for 3 qubits with 9 layers\n",
      "Step 130: Loss = -1.9999996959595365 for 3 qubits with 9 layers\n",
      "Step 140: Loss = -1.99999980368538 for 3 qubits with 9 layers\n",
      "Step 150: Loss = -1.9999999815931324 for 3 qubits with 9 layers\n",
      "Step 160: Loss = -1.99999997775745 for 3 qubits with 9 layers\n",
      "Step 170: Loss = -1.9999999854007031 for 3 qubits with 9 layers\n",
      "Step 180: Loss = -1.9999999942081166 for 3 qubits with 9 layers\n",
      "Step 190: Loss = -1.9999999982885144 for 3 qubits with 9 layers\n",
      "Step 200: Loss = -1.99999999969897 for 3 qubits with 9 layers\n",
      "Step 210: Loss = -1.9999999997217563 for 3 qubits with 9 layers\n",
      "Step 220: Loss = -1.999999999909687 for 3 qubits with 9 layers\n",
      "Step 230: Loss = -1.9999999999886864 for 3 qubits with 9 layers\n",
      "Step 240: Loss = -1.9999999999859055 for 3 qubits with 9 layers\n",
      "Step 250: Loss = -1.9999999999986389 for 3 qubits with 9 layers\n",
      "Step 260: Loss = -1.9999999999977627 for 3 qubits with 9 layers\n",
      "Step 270: Loss = -1.9999999999997171 for 3 qubits with 9 layers\n",
      "Step 280: Loss = -1.9999999999998201 for 3 qubits with 9 layers\n",
      "Step 290: Loss = -1.9999999999999132 for 3 qubits with 9 layers\n",
      "Step 300: Loss = -1.9999999999999685 for 3 qubits with 9 layers\n",
      "Step 310: Loss = -1.9999999999999791 for 3 qubits with 9 layers\n",
      "Step 320: Loss = -1.9999999999999774 for 3 qubits with 9 layers\n",
      "Step 330: Loss = -1.9999999999999807 for 3 qubits with 9 layers\n",
      "Step 340: Loss = -1.999999999999982 for 3 qubits with 9 layers\n",
      "Step 350: Loss = -1.999999999999981 for 3 qubits with 9 layers\n",
      "Step 360: Loss = -1.9999999999999818 for 3 qubits with 9 layers\n",
      "Step 370: Loss = -1.9999999999999791 for 3 qubits with 9 layers\n",
      "Step 380: Loss = -1.9999999999999822 for 3 qubits with 9 layers\n",
      "Step 390: Loss = -1.9999999999999791 for 3 qubits with 9 layers\n",
      "Step 400: Loss = -1.9999999999999845 for 3 qubits with 9 layers\n",
      "Step 410: Loss = -1.999999999999981 for 3 qubits with 9 layers\n",
      "Step 420: Loss = -1.9999999999999765 for 3 qubits with 9 layers\n",
      "Step 430: Loss = -1.99999999999998 for 3 qubits with 9 layers\n",
      "Step 440: Loss = -1.999999999999981 for 3 qubits with 9 layers\n",
      "Step 450: Loss = -1.99999999999998 for 3 qubits with 9 layers\n",
      "Step 460: Loss = -1.9999999999999782 for 3 qubits with 9 layers\n",
      "Step 470: Loss = -1.99999999999998 for 3 qubits with 9 layers\n",
      "Step 480: Loss = -1.9999999999999856 for 3 qubits with 9 layers\n",
      "Step 490: Loss = -1.9999999999999813 for 3 qubits with 9 layers\n",
      "Running optimization for 3 qubits and 10 layers\n",
      "Step 0: Loss = -0.5913181114598856 for 3 qubits with 10 layers\n",
      "Step 10: Loss = -1.841557089864423 for 3 qubits with 10 layers\n",
      "Step 20: Loss = -1.9550612192689512 for 3 qubits with 10 layers\n",
      "Step 30: Loss = -1.9706108055474134 for 3 qubits with 10 layers\n",
      "Step 40: Loss = -1.9917984867056058 for 3 qubits with 10 layers\n",
      "Step 50: Loss = -1.999086508133979 for 3 qubits with 10 layers\n",
      "Step 60: Loss = -1.9997105375597801 for 3 qubits with 10 layers\n",
      "Step 70: Loss = -1.9997458788221134 for 3 qubits with 10 layers\n",
      "Step 80: Loss = -1.999872928890462 for 3 qubits with 10 layers\n",
      "Step 90: Loss = -1.9999570195131624 for 3 qubits with 10 layers\n",
      "Step 100: Loss = -1.999985307690106 for 3 qubits with 10 layers\n",
      "Step 110: Loss = -1.9999939811417566 for 3 qubits with 10 layers\n",
      "Step 120: Loss = -1.99999913847586 for 3 qubits with 10 layers\n",
      "Step 130: Loss = -1.9999998030818384 for 3 qubits with 10 layers\n",
      "Step 140: Loss = -1.999999866360911 for 3 qubits with 10 layers\n",
      "Step 150: Loss = -1.9999998642632486 for 3 qubits with 10 layers\n",
      "Step 160: Loss = -1.9999999510980913 for 3 qubits with 10 layers\n",
      "Step 170: Loss = -1.9999999922290324 for 3 qubits with 10 layers\n",
      "Step 180: Loss = -1.999999996500604 for 3 qubits with 10 layers\n",
      "Step 190: Loss = -1.999999997792893 for 3 qubits with 10 layers\n",
      "Step 200: Loss = -1.999999999900448 for 3 qubits with 10 layers\n",
      "Step 210: Loss = -1.9999999996283813 for 3 qubits with 10 layers\n",
      "Step 220: Loss = -1.9999999999643099 for 3 qubits with 10 layers\n",
      "Step 230: Loss = -1.9999999999486664 for 3 qubits with 10 layers\n",
      "Step 240: Loss = -1.9999999999949116 for 3 qubits with 10 layers\n",
      "Step 250: Loss = -1.999999999995983 for 3 qubits with 10 layers\n",
      "Step 260: Loss = -1.9999999999982057 for 3 qubits with 10 layers\n",
      "Step 270: Loss = -1.9999999999998348 for 3 qubits with 10 layers\n",
      "Step 280: Loss = -1.999999999999905 for 3 qubits with 10 layers\n",
      "Step 290: Loss = -1.9999999999999143 for 3 qubits with 10 layers\n",
      "Step 300: Loss = -1.9999999999999516 for 3 qubits with 10 layers\n",
      "Step 310: Loss = -1.9999999999999614 for 3 qubits with 10 layers\n",
      "Step 320: Loss = -1.9999999999999765 for 3 qubits with 10 layers\n",
      "Step 330: Loss = -1.9999999999999782 for 3 qubits with 10 layers\n",
      "Step 340: Loss = -1.9999999999999782 for 3 qubits with 10 layers\n",
      "Step 350: Loss = -1.9999999999999776 for 3 qubits with 10 layers\n",
      "Step 360: Loss = -1.9999999999999805 for 3 qubits with 10 layers\n",
      "Step 370: Loss = -1.9999999999999762 for 3 qubits with 10 layers\n",
      "Step 380: Loss = -1.9999999999999827 for 3 qubits with 10 layers\n",
      "Step 390: Loss = -1.9999999999999818 for 3 qubits with 10 layers\n",
      "Step 400: Loss = -1.9999999999999827 for 3 qubits with 10 layers\n",
      "Step 410: Loss = -1.9999999999999825 for 3 qubits with 10 layers\n",
      "Step 420: Loss = -1.9999999999999811 for 3 qubits with 10 layers\n",
      "Step 430: Loss = -1.99999999999998 for 3 qubits with 10 layers\n",
      "Step 440: Loss = -1.99999999999998 for 3 qubits with 10 layers\n",
      "Step 450: Loss = -1.9999999999999825 for 3 qubits with 10 layers\n",
      "Step 460: Loss = -1.999999999999981 for 3 qubits with 10 layers\n",
      "Step 470: Loss = -1.999999999999973 for 3 qubits with 10 layers\n",
      "Step 480: Loss = -1.9999999999999818 for 3 qubits with 10 layers\n",
      "Step 490: Loss = -1.9999999999999807 for 3 qubits with 10 layers\n",
      "Running optimization for 3 qubits and 11 layers\n",
      "Step 0: Loss = -0.3913114243448007 for 3 qubits with 11 layers\n",
      "Step 10: Loss = -1.8723190222966557 for 3 qubits with 11 layers\n",
      "Step 20: Loss = -1.9151663335957827 for 3 qubits with 11 layers\n",
      "Step 30: Loss = -1.9876757782345702 for 3 qubits with 11 layers\n",
      "Step 40: Loss = -1.9914669426331357 for 3 qubits with 11 layers\n",
      "Step 50: Loss = -1.995489533904913 for 3 qubits with 11 layers\n",
      "Step 60: Loss = -1.9994960790983902 for 3 qubits with 11 layers\n",
      "Step 70: Loss = -1.9999058387005642 for 3 qubits with 11 layers\n",
      "Step 80: Loss = -1.9998381887291592 for 3 qubits with 11 layers\n",
      "Step 90: Loss = -1.999913922021046 for 3 qubits with 11 layers\n",
      "Step 100: Loss = -1.9999659260803173 for 3 qubits with 11 layers\n",
      "Step 110: Loss = -1.9999915300352598 for 3 qubits with 11 layers\n",
      "Step 120: Loss = -1.9999944776793437 for 3 qubits with 11 layers\n",
      "Step 130: Loss = -1.9999989830648732 for 3 qubits with 11 layers\n",
      "Step 140: Loss = -1.9999993309201056 for 3 qubits with 11 layers\n",
      "Step 150: Loss = -1.999999831394129 for 3 qubits with 11 layers\n",
      "Step 160: Loss = -1.999999922334227 for 3 qubits with 11 layers\n",
      "Step 170: Loss = -1.9999999831116386 for 3 qubits with 11 layers\n",
      "Step 180: Loss = -1.9999999986275605 for 3 qubits with 11 layers\n",
      "Step 190: Loss = -1.9999999987448762 for 3 qubits with 11 layers\n",
      "Step 200: Loss = -1.9999999986397614 for 3 qubits with 11 layers\n",
      "Step 210: Loss = -1.9999999995701374 for 3 qubits with 11 layers\n",
      "Step 220: Loss = -1.9999999998943956 for 3 qubits with 11 layers\n",
      "Step 230: Loss = -1.999999999933595 for 3 qubits with 11 layers\n",
      "Step 240: Loss = -1.9999999999848677 for 3 qubits with 11 layers\n",
      "Step 250: Loss = -1.9999999999925604 for 3 qubits with 11 layers\n",
      "Step 260: Loss = -1.9999999999982792 for 3 qubits with 11 layers\n",
      "Step 270: Loss = -1.9999999999993912 for 3 qubits with 11 layers\n",
      "Step 280: Loss = -1.9999999999998122 for 3 qubits with 11 layers\n",
      "Step 290: Loss = -1.9999999999998428 for 3 qubits with 11 layers\n",
      "Step 300: Loss = -1.99999999999996 for 3 qubits with 11 layers\n",
      "Step 310: Loss = -1.9999999999999634 for 3 qubits with 11 layers\n",
      "Step 320: Loss = -1.999999999999977 for 3 qubits with 11 layers\n",
      "Step 330: Loss = -1.9999999999999671 for 3 qubits with 11 layers\n",
      "Step 340: Loss = -1.9999999999999765 for 3 qubits with 11 layers\n",
      "Step 350: Loss = -1.9999999999999787 for 3 qubits with 11 layers\n",
      "Step 360: Loss = -1.9999999999999782 for 3 qubits with 11 layers\n",
      "Step 370: Loss = -1.9999999999999822 for 3 qubits with 11 layers\n",
      "Step 380: Loss = -1.9999999999999771 for 3 qubits with 11 layers\n",
      "Step 390: Loss = -1.9999999999999785 for 3 qubits with 11 layers\n",
      "Step 400: Loss = -1.9999999999999782 for 3 qubits with 11 layers\n",
      "Step 410: Loss = -1.9999999999999754 for 3 qubits with 11 layers\n",
      "Step 420: Loss = -1.999999999999979 for 3 qubits with 11 layers\n",
      "Step 430: Loss = -1.999999999999975 for 3 qubits with 11 layers\n",
      "Step 440: Loss = -1.9999999999999805 for 3 qubits with 11 layers\n",
      "Step 450: Loss = -1.9999999999999756 for 3 qubits with 11 layers\n",
      "Step 460: Loss = -1.9999999999999756 for 3 qubits with 11 layers\n",
      "Step 470: Loss = -1.9999999999999725 for 3 qubits with 11 layers\n",
      "Step 480: Loss = -1.9999999999999758 for 3 qubits with 11 layers\n",
      "Step 490: Loss = -1.9999999999999756 for 3 qubits with 11 layers\n",
      "Running optimization for 3 qubits and 12 layers\n",
      "Step 0: Loss = -0.675811591703765 for 3 qubits with 12 layers\n",
      "Step 10: Loss = -1.7841028241081887 for 3 qubits with 12 layers\n",
      "Step 20: Loss = -1.9740072501192292 for 3 qubits with 12 layers\n",
      "Step 30: Loss = -1.9927846625016388 for 3 qubits with 12 layers\n",
      "Step 40: Loss = -1.9929536693467325 for 3 qubits with 12 layers\n",
      "Step 50: Loss = -1.9970911780796388 for 3 qubits with 12 layers\n",
      "Step 60: Loss = -1.9992234083375728 for 3 qubits with 12 layers\n",
      "Step 70: Loss = -1.9997700520803274 for 3 qubits with 12 layers\n",
      "Step 80: Loss = -1.9999239567746852 for 3 qubits with 12 layers\n",
      "Step 90: Loss = -1.9999858745852541 for 3 qubits with 12 layers\n",
      "Step 100: Loss = -1.999997824334046 for 3 qubits with 12 layers\n",
      "Step 110: Loss = -1.9999955972088233 for 3 qubits with 12 layers\n",
      "Step 120: Loss = -1.9999982207113658 for 3 qubits with 12 layers\n",
      "Step 130: Loss = -1.99999902136319 for 3 qubits with 12 layers\n",
      "Step 140: Loss = -1.9999996533298672 for 3 qubits with 12 layers\n",
      "Step 150: Loss = -1.9999999429177164 for 3 qubits with 12 layers\n",
      "Step 160: Loss = -1.9999999927757637 for 3 qubits with 12 layers\n",
      "Step 170: Loss = -1.9999999798763282 for 3 qubits with 12 layers\n",
      "Step 180: Loss = -1.9999999966445907 for 3 qubits with 12 layers\n",
      "Step 190: Loss = -1.9999999994508184 for 3 qubits with 12 layers\n",
      "Step 200: Loss = -1.9999999993462112 for 3 qubits with 12 layers\n",
      "Step 210: Loss = -1.999999999902982 for 3 qubits with 12 layers\n",
      "Step 220: Loss = -1.9999999998959694 for 3 qubits with 12 layers\n",
      "Step 230: Loss = -1.9999999999757245 for 3 qubits with 12 layers\n",
      "Step 240: Loss = -1.9999999999931672 for 3 qubits with 12 layers\n",
      "Step 250: Loss = -1.999999999995243 for 3 qubits with 12 layers\n",
      "Step 260: Loss = -1.9999999999994165 for 3 qubits with 12 layers\n",
      "Step 270: Loss = -1.9999999999999045 for 3 qubits with 12 layers\n",
      "Step 280: Loss = -1.9999999999998153 for 3 qubits with 12 layers\n",
      "Step 290: Loss = -1.9999999999999165 for 3 qubits with 12 layers\n",
      "Step 300: Loss = -1.9999999999999405 for 3 qubits with 12 layers\n",
      "Step 310: Loss = -1.9999999999999671 for 3 qubits with 12 layers\n",
      "Step 320: Loss = -1.999999999999973 for 3 qubits with 12 layers\n",
      "Step 330: Loss = -1.9999999999999738 for 3 qubits with 12 layers\n",
      "Step 340: Loss = -1.9999999999999742 for 3 qubits with 12 layers\n",
      "Step 350: Loss = -1.9999999999999765 for 3 qubits with 12 layers\n",
      "Step 360: Loss = -1.9999999999999711 for 3 qubits with 12 layers\n",
      "Step 370: Loss = -1.9999999998652247 for 3 qubits with 12 layers\n",
      "Step 380: Loss = -1.9997390261041676 for 3 qubits with 12 layers\n",
      "Step 390: Loss = -1.999400945577507 for 3 qubits with 12 layers\n",
      "Step 400: Loss = -1.9996874747724862 for 3 qubits with 12 layers\n",
      "Step 410: Loss = -1.9998968720508261 for 3 qubits with 12 layers\n",
      "Step 420: Loss = -1.9999641772388643 for 3 qubits with 12 layers\n",
      "Step 430: Loss = -1.9999837715397804 for 3 qubits with 12 layers\n",
      "Step 440: Loss = -1.9999996890614393 for 3 qubits with 12 layers\n",
      "Step 450: Loss = -1.9999977732216798 for 3 qubits with 12 layers\n",
      "Step 460: Loss = -1.9999990696919296 for 3 qubits with 12 layers\n",
      "Step 470: Loss = -1.9999998889635247 for 3 qubits with 12 layers\n",
      "Step 480: Loss = -1.9999998381069801 for 3 qubits with 12 layers\n",
      "Step 490: Loss = -1.9999999337641734 for 3 qubits with 12 layers\n",
      "Running optimization for 3 qubits and 13 layers\n",
      "Step 0: Loss = -1.503716625995525 for 3 qubits with 13 layers\n",
      "Step 10: Loss = -1.972485032011884 for 3 qubits with 13 layers\n",
      "Step 20: Loss = -1.9907963073020274 for 3 qubits with 13 layers\n",
      "Step 30: Loss = -1.9982878143562628 for 3 qubits with 13 layers\n",
      "Step 40: Loss = -1.9987422293962007 for 3 qubits with 13 layers\n",
      "Step 50: Loss = -1.999747088779929 for 3 qubits with 13 layers\n",
      "Step 60: Loss = -1.9998194468165256 for 3 qubits with 13 layers\n",
      "Step 70: Loss = -1.9998437703116219 for 3 qubits with 13 layers\n",
      "Step 80: Loss = -1.99996967732525 for 3 qubits with 13 layers\n",
      "Step 90: Loss = -1.9999909266595028 for 3 qubits with 13 layers\n",
      "Step 100: Loss = -1.9999938982745458 for 3 qubits with 13 layers\n",
      "Step 110: Loss = -1.999999493250395 for 3 qubits with 13 layers\n",
      "Step 120: Loss = -1.9999990610336642 for 3 qubits with 13 layers\n",
      "Step 130: Loss = -1.9999997469821147 for 3 qubits with 13 layers\n",
      "Step 140: Loss = -1.9999999793725292 for 3 qubits with 13 layers\n",
      "Step 150: Loss = -1.999999964498318 for 3 qubits with 13 layers\n",
      "Step 160: Loss = -1.9999999800108603 for 3 qubits with 13 layers\n",
      "Step 170: Loss = -1.999999995767376 for 3 qubits with 13 layers\n",
      "Step 180: Loss = -1.9999999990680895 for 3 qubits with 13 layers\n",
      "Step 190: Loss = -1.999999999629936 for 3 qubits with 13 layers\n",
      "Step 200: Loss = -1.9999999998698423 for 3 qubits with 13 layers\n",
      "Step 210: Loss = -1.9999999999247717 for 3 qubits with 13 layers\n",
      "Step 220: Loss = -1.999999999963875 for 3 qubits with 13 layers\n",
      "Step 230: Loss = -1.999999999988501 for 3 qubits with 13 layers\n",
      "Step 240: Loss = -1.9999999999998004 for 3 qubits with 13 layers\n",
      "Step 250: Loss = -1.9999999999982006 for 3 qubits with 13 layers\n",
      "Step 260: Loss = -1.9999999999997393 for 3 qubits with 13 layers\n",
      "Step 270: Loss = -1.9999999999998181 for 3 qubits with 13 layers\n",
      "Step 280: Loss = -1.999999999999933 for 3 qubits with 13 layers\n",
      "Step 290: Loss = -1.9999999999999356 for 3 qubits with 13 layers\n",
      "Step 300: Loss = -1.999999999999967 for 3 qubits with 13 layers\n",
      "Step 310: Loss = -1.9999999999999707 for 3 qubits with 13 layers\n",
      "Step 320: Loss = -1.9999999999999698 for 3 qubits with 13 layers\n",
      "Step 330: Loss = -1.9999999999999778 for 3 qubits with 13 layers\n",
      "Step 340: Loss = -1.999999999999972 for 3 qubits with 13 layers\n",
      "Step 350: Loss = -1.9999999999999756 for 3 qubits with 13 layers\n",
      "Step 360: Loss = -1.9999999999999676 for 3 qubits with 13 layers\n",
      "Step 370: Loss = -1.999999999999972 for 3 qubits with 13 layers\n",
      "Step 380: Loss = -1.9999999999999716 for 3 qubits with 13 layers\n",
      "Step 390: Loss = -1.999999999999968 for 3 qubits with 13 layers\n",
      "Step 400: Loss = -1.9999999999999674 for 3 qubits with 13 layers\n",
      "Step 410: Loss = -1.9999999999999676 for 3 qubits with 13 layers\n",
      "Step 420: Loss = -1.9999999999379483 for 3 qubits with 13 layers\n",
      "Step 430: Loss = -1.9998528936778692 for 3 qubits with 13 layers\n",
      "Step 440: Loss = -1.9997474895035827 for 3 qubits with 13 layers\n",
      "Step 450: Loss = -1.9999345727769455 for 3 qubits with 13 layers\n",
      "Step 460: Loss = -1.9999716985132023 for 3 qubits with 13 layers\n",
      "Step 470: Loss = -1.9999979938752785 for 3 qubits with 13 layers\n",
      "Step 480: Loss = -1.9999972129461567 for 3 qubits with 13 layers\n",
      "Step 490: Loss = -1.9999997522900865 for 3 qubits with 13 layers\n",
      "Running optimization for 3 qubits and 14 layers\n",
      "Step 0: Loss = -1.0278926148434664 for 3 qubits with 14 layers\n",
      "Step 10: Loss = -1.8823418170157717 for 3 qubits with 14 layers\n",
      "Step 20: Loss = -1.9721713358361597 for 3 qubits with 14 layers\n",
      "Step 30: Loss = -1.9955414747427112 for 3 qubits with 14 layers\n",
      "Step 40: Loss = -1.999321994708463 for 3 qubits with 14 layers\n",
      "Step 50: Loss = -1.9989246271430705 for 3 qubits with 14 layers\n",
      "Step 60: Loss = -1.9995235385416912 for 3 qubits with 14 layers\n",
      "Step 70: Loss = -1.9998103183509919 for 3 qubits with 14 layers\n",
      "Step 80: Loss = -1.9999650688471817 for 3 qubits with 14 layers\n",
      "Step 90: Loss = -1.9999969800776205 for 3 qubits with 14 layers\n",
      "Step 100: Loss = -1.9999972173858362 for 3 qubits with 14 layers\n",
      "Step 110: Loss = -1.9999957505103714 for 3 qubits with 14 layers\n",
      "Step 120: Loss = -1.9999978081669592 for 3 qubits with 14 layers\n",
      "Step 130: Loss = -1.9999997946447547 for 3 qubits with 14 layers\n",
      "Step 140: Loss = -1.99999989809432 for 3 qubits with 14 layers\n",
      "Step 150: Loss = -1.9999999133036834 for 3 qubits with 14 layers\n",
      "Step 160: Loss = -1.999999991463061 for 3 qubits with 14 layers\n",
      "Step 170: Loss = -1.9999999881514317 for 3 qubits with 14 layers\n",
      "Step 180: Loss = -1.9999999993940774 for 3 qubits with 14 layers\n",
      "Step 190: Loss = -1.9999999985603818 for 3 qubits with 14 layers\n",
      "Step 200: Loss = -1.9999999996030116 for 3 qubits with 14 layers\n",
      "Step 210: Loss = -1.9999999999644174 for 3 qubits with 14 layers\n",
      "Step 220: Loss = -1.9999999999438949 for 3 qubits with 14 layers\n",
      "Step 230: Loss = -1.9999999999747984 for 3 qubits with 14 layers\n",
      "Step 240: Loss = -1.9999999999932754 for 3 qubits with 14 layers\n",
      "Step 250: Loss = -1.9999999999989089 for 3 qubits with 14 layers\n",
      "Step 260: Loss = -1.9999999999997287 for 3 qubits with 14 layers\n",
      "Step 270: Loss = -1.9999999999999165 for 3 qubits with 14 layers\n",
      "Step 280: Loss = -1.999999999999907 for 3 qubits with 14 layers\n",
      "Step 290: Loss = -1.9999999999999343 for 3 qubits with 14 layers\n",
      "Step 300: Loss = -1.9999999999999432 for 3 qubits with 14 layers\n",
      "Step 310: Loss = -1.9999999999999627 for 3 qubits with 14 layers\n",
      "Step 320: Loss = -1.999999999999962 for 3 qubits with 14 layers\n",
      "Step 330: Loss = -1.9999999999999631 for 3 qubits with 14 layers\n",
      "Step 340: Loss = -1.9999999999999698 for 3 qubits with 14 layers\n",
      "Step 350: Loss = -1.999999999999966 for 3 qubits with 14 layers\n",
      "Step 360: Loss = -1.9999999999999658 for 3 qubits with 14 layers\n",
      "Step 370: Loss = -1.9999999998540652 for 3 qubits with 14 layers\n",
      "Step 380: Loss = -1.9998354486539816 for 3 qubits with 14 layers\n",
      "Step 390: Loss = -1.999848044499475 for 3 qubits with 14 layers\n",
      "Step 400: Loss = -1.9998223876415202 for 3 qubits with 14 layers\n",
      "Step 410: Loss = -1.9998864514983596 for 3 qubits with 14 layers\n",
      "Step 420: Loss = -1.999956430193096 for 3 qubits with 14 layers\n",
      "Step 430: Loss = -1.9999912040947945 for 3 qubits with 14 layers\n",
      "Step 440: Loss = -1.9999986269743084 for 3 qubits with 14 layers\n",
      "Step 450: Loss = -1.9999992345904438 for 3 qubits with 14 layers\n",
      "Step 460: Loss = -1.9999995674697155 for 3 qubits with 14 layers\n",
      "Step 470: Loss = -1.9999999864960158 for 3 qubits with 14 layers\n",
      "Step 480: Loss = -1.9999999043385528 for 3 qubits with 14 layers\n",
      "Step 490: Loss = -1.999999988682089 for 3 qubits with 14 layers\n",
      "Running optimization for 3 qubits and 15 layers\n",
      "Step 0: Loss = -0.8712547982473194 for 3 qubits with 15 layers\n",
      "Step 10: Loss = -1.8280986251105444 for 3 qubits with 15 layers\n",
      "Step 20: Loss = -1.9444209235376548 for 3 qubits with 15 layers\n",
      "Step 30: Loss = -1.9818758980518205 for 3 qubits with 15 layers\n",
      "Step 40: Loss = -1.992842076184489 for 3 qubits with 15 layers\n",
      "Step 50: Loss = -1.9975108415289184 for 3 qubits with 15 layers\n",
      "Step 60: Loss = -1.9990729886830807 for 3 qubits with 15 layers\n",
      "Step 70: Loss = -1.99960843225166 for 3 qubits with 15 layers\n",
      "Step 80: Loss = -1.9999236227547574 for 3 qubits with 15 layers\n",
      "Step 90: Loss = -1.9999953374304111 for 3 qubits with 15 layers\n",
      "Step 100: Loss = -1.9999894311618243 for 3 qubits with 15 layers\n",
      "Step 110: Loss = -1.9999936286823183 for 3 qubits with 15 layers\n",
      "Step 120: Loss = -1.9999998901817615 for 3 qubits with 15 layers\n",
      "Step 130: Loss = -1.9999993061198613 for 3 qubits with 15 layers\n",
      "Step 140: Loss = -1.9999999559433363 for 3 qubits with 15 layers\n",
      "Step 150: Loss = -1.999999904033248 for 3 qubits with 15 layers\n",
      "Step 160: Loss = -1.999999997639774 for 3 qubits with 15 layers\n",
      "Step 170: Loss = -1.9999999873303977 for 3 qubits with 15 layers\n",
      "Step 180: Loss = -1.9999999954194574 for 3 qubits with 15 layers\n",
      "Step 190: Loss = -1.9999999998212332 for 3 qubits with 15 layers\n",
      "Step 200: Loss = -1.9999999998358424 for 3 qubits with 15 layers\n",
      "Step 210: Loss = -1.9999999998935678 for 3 qubits with 15 layers\n",
      "Step 220: Loss = -1.9999999999277 for 3 qubits with 15 layers\n",
      "Step 230: Loss = -1.999999999977148 for 3 qubits with 15 layers\n",
      "Step 240: Loss = -1.999999999992078 for 3 qubits with 15 layers\n",
      "Step 250: Loss = -1.9999999999977733 for 3 qubits with 15 layers\n",
      "Step 260: Loss = -1.9999999999996136 for 3 qubits with 15 layers\n",
      "Step 270: Loss = -1.9999999999997753 for 3 qubits with 15 layers\n",
      "Step 280: Loss = -1.999999999999777 for 3 qubits with 15 layers\n",
      "Step 290: Loss = -1.9999999999999267 for 3 qubits with 15 layers\n",
      "Step 300: Loss = -1.9999999999999587 for 3 qubits with 15 layers\n",
      "Step 310: Loss = -1.9999999999999574 for 3 qubits with 15 layers\n",
      "Step 320: Loss = -1.9999999999999665 for 3 qubits with 15 layers\n",
      "Step 330: Loss = -1.999999999999967 for 3 qubits with 15 layers\n",
      "Step 340: Loss = -1.9999999999999705 for 3 qubits with 15 layers\n",
      "Step 350: Loss = -1.999999999999964 for 3 qubits with 15 layers\n",
      "Step 360: Loss = -1.9999999999999618 for 3 qubits with 15 layers\n",
      "Step 370: Loss = -1.9999999999999674 for 3 qubits with 15 layers\n",
      "Step 380: Loss = -1.9999999999999727 for 3 qubits with 15 layers\n",
      "Step 390: Loss = -1.9999999999999705 for 3 qubits with 15 layers\n",
      "Step 400: Loss = -1.9999999999999747 for 3 qubits with 15 layers\n",
      "Step 410: Loss = -1.999999999999973 for 3 qubits with 15 layers\n",
      "Step 420: Loss = -1.999999999999966 for 3 qubits with 15 layers\n",
      "Step 430: Loss = -1.9999999999999614 for 3 qubits with 15 layers\n",
      "Step 440: Loss = -1.999999996450306 for 3 qubits with 15 layers\n",
      "Step 450: Loss = -1.9944950566012967 for 3 qubits with 15 layers\n",
      "Step 460: Loss = -1.9984420391942788 for 3 qubits with 15 layers\n",
      "Step 470: Loss = -1.9999441973083325 for 3 qubits with 15 layers\n",
      "Step 480: Loss = -1.9998607904994938 for 3 qubits with 15 layers\n",
      "Step 490: Loss = -1.9999441317101352 for 3 qubits with 15 layers\n",
      "Running optimization for 4 qubits and 1 layers\n",
      "Step 0: Loss = -1.3427321085695576 for 4 qubits with 1 layers\n",
      "Step 10: Loss = -1.9357915801102166 for 4 qubits with 1 layers\n",
      "Step 20: Loss = -2.4520089043631947 for 4 qubits with 1 layers\n",
      "Step 30: Loss = -2.8013351014205154 for 4 qubits with 1 layers\n",
      "Step 40: Loss = -2.962316287378267 for 4 qubits with 1 layers\n",
      "Step 50: Loss = -2.9993535969325555 for 4 qubits with 1 layers\n",
      "Step 60: Loss = -2.997448642091198 for 4 qubits with 1 layers\n",
      "Step 70: Loss = -2.996757936177948 for 4 qubits with 1 layers\n",
      "Step 80: Loss = -2.999015309689198 for 4 qubits with 1 layers\n",
      "Step 90: Loss = -2.9999752138048184 for 4 qubits with 1 layers\n",
      "Step 100: Loss = -2.999939567886887 for 4 qubits with 1 layers\n",
      "Step 110: Loss = -2.999940851821971 for 4 qubits with 1 layers\n",
      "Step 120: Loss = -2.999993498671057 for 4 qubits with 1 layers\n",
      "Step 130: Loss = -2.999998670530086 for 4 qubits with 1 layers\n",
      "Step 140: Loss = -2.9999972919273903 for 4 qubits with 1 layers\n",
      "Step 150: Loss = -2.999999737906601 for 4 qubits with 1 layers\n",
      "Step 160: Loss = -2.9999998878871255 for 4 qubits with 1 layers\n",
      "Step 170: Loss = -2.999999881044471 for 4 qubits with 1 layers\n",
      "Step 180: Loss = -2.999999999402444 for 4 qubits with 1 layers\n",
      "Step 190: Loss = -2.9999999865447773 for 4 qubits with 1 layers\n",
      "Step 200: Loss = -2.999999997847919 for 4 qubits with 1 layers\n",
      "Step 210: Loss = -2.9999999991422417 for 4 qubits with 1 layers\n",
      "Step 220: Loss = -2.9999999994638067 for 4 qubits with 1 layers\n",
      "Step 230: Loss = -2.9999999999552207 for 4 qubits with 1 layers\n",
      "Step 240: Loss = -2.9999999999179003 for 4 qubits with 1 layers\n",
      "Step 250: Loss = -2.999999999996434 for 4 qubits with 1 layers\n",
      "Step 260: Loss = -2.9999999999895044 for 4 qubits with 1 layers\n",
      "Step 270: Loss = -2.999999999999266 for 4 qubits with 1 layers\n",
      "Step 280: Loss = -2.9999999999988836 for 4 qubits with 1 layers\n",
      "Step 290: Loss = -2.9999999999997553 for 4 qubits with 1 layers\n",
      "Step 300: Loss = -2.999999999999931 for 4 qubits with 1 layers\n",
      "Step 310: Loss = -2.999999999999935 for 4 qubits with 1 layers\n",
      "Step 320: Loss = -2.9999999999999956 for 4 qubits with 1 layers\n",
      "Step 330: Loss = -2.9999999999999876 for 4 qubits with 1 layers\n",
      "Step 340: Loss = -2.999999999999992 for 4 qubits with 1 layers\n",
      "Step 350: Loss = -2.9999999999999947 for 4 qubits with 1 layers\n",
      "Step 360: Loss = -2.999999999999993 for 4 qubits with 1 layers\n",
      "Step 370: Loss = -2.9999999999999925 for 4 qubits with 1 layers\n",
      "Step 380: Loss = -2.999999999999993 for 4 qubits with 1 layers\n",
      "Step 390: Loss = -2.9999999999999933 for 4 qubits with 1 layers\n",
      "Step 400: Loss = -2.999999999999995 for 4 qubits with 1 layers\n",
      "Step 410: Loss = -2.9999999999999947 for 4 qubits with 1 layers\n",
      "Step 420: Loss = -2.999999999999995 for 4 qubits with 1 layers\n",
      "Step 430: Loss = -2.9999999999999964 for 4 qubits with 1 layers\n",
      "Step 440: Loss = -2.999999999999992 for 4 qubits with 1 layers\n",
      "Step 450: Loss = -2.999999999999994 for 4 qubits with 1 layers\n",
      "Step 460: Loss = -2.999999999999993 for 4 qubits with 1 layers\n",
      "Step 470: Loss = -2.999999999999994 for 4 qubits with 1 layers\n",
      "Step 480: Loss = -2.999999999999993 for 4 qubits with 1 layers\n",
      "Step 490: Loss = -2.999999999999993 for 4 qubits with 1 layers\n",
      "Running optimization for 4 qubits and 2 layers\n",
      "Step 0: Loss = -0.8633321418887923 for 4 qubits with 2 layers\n",
      "Step 10: Loss = -2.0663833784522754 for 4 qubits with 2 layers\n",
      "Step 20: Loss = -2.7224468063750558 for 4 qubits with 2 layers\n",
      "Step 30: Loss = -2.946161136332247 for 4 qubits with 2 layers\n",
      "Step 40: Loss = -2.97132157182745 for 4 qubits with 2 layers\n",
      "Step 50: Loss = -2.994616145269295 for 4 qubits with 2 layers\n",
      "Step 60: Loss = -2.997397414101199 for 4 qubits with 2 layers\n",
      "Step 70: Loss = -2.997839914254287 for 4 qubits with 2 layers\n",
      "Step 80: Loss = -2.999801464480562 for 4 qubits with 2 layers\n",
      "Step 90: Loss = -2.999810497866238 for 4 qubits with 2 layers\n",
      "Step 100: Loss = -2.999981879922397 for 4 qubits with 2 layers\n",
      "Step 110: Loss = -2.999962693117357 for 4 qubits with 2 layers\n",
      "Step 120: Loss = -2.9999960244269825 for 4 qubits with 2 layers\n",
      "Step 130: Loss = -2.99999792275809 for 4 qubits with 2 layers\n",
      "Step 140: Loss = -2.99999868411631 for 4 qubits with 2 layers\n",
      "Step 150: Loss = -2.99999980742516 for 4 qubits with 2 layers\n",
      "Step 160: Loss = -2.999999852041779 for 4 qubits with 2 layers\n",
      "Step 170: Loss = -2.999999955270027 for 4 qubits with 2 layers\n",
      "Step 180: Loss = -2.999999991182242 for 4 qubits with 2 layers\n",
      "Step 190: Loss = -2.9999999934426334 for 4 qubits with 2 layers\n",
      "Step 200: Loss = -2.999999996854817 for 4 qubits with 2 layers\n",
      "Step 210: Loss = -2.999999999843387 for 4 qubits with 2 layers\n",
      "Step 220: Loss = -2.9999999998423976 for 4 qubits with 2 layers\n",
      "Step 230: Loss = -2.9999999998570033 for 4 qubits with 2 layers\n",
      "Step 240: Loss = -2.9999999999569416 for 4 qubits with 2 layers\n",
      "Step 250: Loss = -2.9999999999830393 for 4 qubits with 2 layers\n",
      "Step 260: Loss = -2.9999999999988263 for 4 qubits with 2 layers\n",
      "Step 270: Loss = -2.999999999998967 for 4 qubits with 2 layers\n",
      "Step 280: Loss = -2.9999999999999765 for 4 qubits with 2 layers\n",
      "Step 290: Loss = -2.9999999999998397 for 4 qubits with 2 layers\n",
      "Step 300: Loss = -2.999999999999976 for 4 qubits with 2 layers\n",
      "Step 310: Loss = -2.999999999999969 for 4 qubits with 2 layers\n",
      "Step 320: Loss = -2.999999999999986 for 4 qubits with 2 layers\n",
      "Step 330: Loss = -2.999999999999984 for 4 qubits with 2 layers\n",
      "Step 340: Loss = -2.9999999999999885 for 4 qubits with 2 layers\n",
      "Step 350: Loss = -2.999999999999993 for 4 qubits with 2 layers\n",
      "Step 360: Loss = -2.9999999999999933 for 4 qubits with 2 layers\n",
      "Step 370: Loss = -2.9999999999999916 for 4 qubits with 2 layers\n",
      "Step 380: Loss = -2.999999999999986 for 4 qubits with 2 layers\n",
      "Step 390: Loss = -2.9999999999999876 for 4 qubits with 2 layers\n",
      "Step 400: Loss = -2.9999999999999956 for 4 qubits with 2 layers\n",
      "Step 410: Loss = -2.9999999999999916 for 4 qubits with 2 layers\n",
      "Step 420: Loss = -2.9999999999999916 for 4 qubits with 2 layers\n",
      "Step 430: Loss = -2.9999999999999902 for 4 qubits with 2 layers\n",
      "Step 440: Loss = -2.9999999999999964 for 4 qubits with 2 layers\n",
      "Step 450: Loss = -2.999999999999994 for 4 qubits with 2 layers\n",
      "Step 460: Loss = -2.999999999999994 for 4 qubits with 2 layers\n",
      "Step 470: Loss = -2.999999999999998 for 4 qubits with 2 layers\n",
      "Step 480: Loss = -2.9999999999999956 for 4 qubits with 2 layers\n",
      "Step 490: Loss = -2.999999999999994 for 4 qubits with 2 layers\n",
      "Running optimization for 4 qubits and 3 layers\n",
      "Step 0: Loss = -2.584146514022753 for 4 qubits with 3 layers\n",
      "Step 10: Loss = -2.9415613148092943 for 4 qubits with 3 layers\n",
      "Step 20: Loss = -2.993590662326284 for 4 qubits with 3 layers\n",
      "Step 30: Loss = -2.995354532923548 for 4 qubits with 3 layers\n",
      "Step 40: Loss = -2.9969266928075244 for 4 qubits with 3 layers\n",
      "Step 50: Loss = -2.997982042895525 for 4 qubits with 3 layers\n",
      "Step 60: Loss = -2.998504882647049 for 4 qubits with 3 layers\n",
      "Step 70: Loss = -2.9988154074214086 for 4 qubits with 3 layers\n",
      "Step 80: Loss = -2.9989624383185816 for 4 qubits with 3 layers\n",
      "Step 90: Loss = -2.9990266048406027 for 4 qubits with 3 layers\n",
      "Step 100: Loss = -2.9990703332830333 for 4 qubits with 3 layers\n",
      "Step 110: Loss = -2.9991034767538367 for 4 qubits with 3 layers\n",
      "Step 120: Loss = -2.99913100704487 for 4 qubits with 3 layers\n",
      "Step 130: Loss = -2.9991572200526386 for 4 qubits with 3 layers\n",
      "Step 140: Loss = -2.999182662439918 for 4 qubits with 3 layers\n",
      "Step 150: Loss = -2.9992070903082304 for 4 qubits with 3 layers\n",
      "Step 160: Loss = -2.9992307151914863 for 4 qubits with 3 layers\n",
      "Step 170: Loss = -2.9992537322460158 for 4 qubits with 3 layers\n",
      "Step 180: Loss = -2.999276182632716 for 4 qubits with 3 layers\n",
      "Step 190: Loss = -2.999298137800894 for 4 qubits with 3 layers\n",
      "Step 200: Loss = -2.9993196376398092 for 4 qubits with 3 layers\n",
      "Step 210: Loss = -2.999340717139579 for 4 qubits with 3 layers\n",
      "Step 220: Loss = -2.999361400428576 for 4 qubits with 3 layers\n",
      "Step 230: Loss = -2.999381706437261 for 4 qubits with 3 layers\n",
      "Step 240: Loss = -2.999401649501112 for 4 qubits with 3 layers\n",
      "Step 250: Loss = -2.999421240926061 for 4 qubits with 3 layers\n",
      "Step 260: Loss = -2.9994404899827227 for 4 qubits with 3 layers\n",
      "Step 270: Loss = -2.9994594047306844 for 4 qubits with 3 layers\n",
      "Step 280: Loss = -2.999477992741023 for 4 qubits with 3 layers\n",
      "Step 290: Loss = -2.999496261674639 for 4 qubits with 3 layers\n",
      "Step 300: Loss = -2.9995142197353895 for 4 qubits with 3 layers\n",
      "Step 310: Loss = -2.999531876001759 for 4 qubits with 3 layers\n",
      "Step 320: Loss = -2.9995492406161985 for 4 qubits with 3 layers\n",
      "Step 330: Loss = -2.999566324783523 for 4 qubits with 3 layers\n",
      "Step 340: Loss = -2.999583140509431 for 4 qubits with 3 layers\n",
      "Step 350: Loss = -2.9995996999886234 for 4 qubits with 3 layers\n",
      "Step 360: Loss = -2.9996160145369366 for 4 qubits with 3 layers\n",
      "Step 370: Loss = -2.99963209297288 for 4 qubits with 3 layers\n",
      "Step 380: Loss = -2.9996479394155005 for 4 qubits with 3 layers\n",
      "Step 390: Loss = -2.9996635506097524 for 4 qubits with 3 layers\n",
      "Step 400: Loss = -2.999678913141088 for 4 qubits with 3 layers\n",
      "Step 410: Loss = -2.9996940012426756 for 4 qubits with 3 layers\n",
      "Step 420: Loss = -2.9997087762226684 for 4 qubits with 3 layers\n",
      "Step 430: Loss = -2.999723188607882 for 4 qubits with 3 layers\n",
      "Step 440: Loss = -2.999737183597845 for 4 qubits with 3 layers\n",
      "Step 450: Loss = -2.99975070919157 for 4 qubits with 3 layers\n",
      "Step 460: Loss = -2.9997637247520648 for 4 qubits with 3 layers\n",
      "Step 470: Loss = -2.9997762068315215 for 4 qubits with 3 layers\n",
      "Step 480: Loss = -2.9997881498817933 for 4 qubits with 3 layers\n",
      "Step 490: Loss = -2.999799561984099 for 4 qubits with 3 layers\n",
      "Running optimization for 4 qubits and 4 layers\n",
      "Step 0: Loss = -1.4494160563817822 for 4 qubits with 4 layers\n",
      "Step 10: Loss = -2.8722392143798636 for 4 qubits with 4 layers\n",
      "Step 20: Loss = -2.8721060610683393 for 4 qubits with 4 layers\n",
      "Step 30: Loss = -2.974237333809385 for 4 qubits with 4 layers\n",
      "Step 40: Loss = -2.986462490437589 for 4 qubits with 4 layers\n",
      "Step 50: Loss = -2.99825101031401 for 4 qubits with 4 layers\n",
      "Step 60: Loss = -2.998330569005322 for 4 qubits with 4 layers\n",
      "Step 70: Loss = -2.9989456487147734 for 4 qubits with 4 layers\n",
      "Step 80: Loss = -2.9989514419255476 for 4 qubits with 4 layers\n",
      "Step 90: Loss = -2.9991376611997276 for 4 qubits with 4 layers\n",
      "Step 100: Loss = -2.9991759097001296 for 4 qubits with 4 layers\n",
      "Step 110: Loss = -2.9991989121887492 for 4 qubits with 4 layers\n",
      "Step 120: Loss = -2.9992146308231016 for 4 qubits with 4 layers\n",
      "Step 130: Loss = -2.999223436230168 for 4 qubits with 4 layers\n",
      "Step 140: Loss = -2.9992319702605243 for 4 qubits with 4 layers\n",
      "Step 150: Loss = -2.999241152420033 for 4 qubits with 4 layers\n",
      "Step 160: Loss = -2.99925044537731 for 4 qubits with 4 layers\n",
      "Step 170: Loss = -2.9992600721375 for 4 qubits with 4 layers\n",
      "Step 180: Loss = -2.9992701819077516 for 4 qubits with 4 layers\n",
      "Step 190: Loss = -2.9992807279234155 for 4 qubits with 4 layers\n",
      "Step 200: Loss = -2.999291724769587 for 4 qubits with 4 layers\n",
      "Step 210: Loss = -2.999303170271822 for 4 qubits with 4 layers\n",
      "Step 220: Loss = -2.9993150631058025 for 4 qubits with 4 layers\n",
      "Step 230: Loss = -2.999327401454832 for 4 qubits with 4 layers\n",
      "Step 240: Loss = -2.999340182999264 for 4 qubits with 4 layers\n",
      "Step 250: Loss = -2.9993534043155794 for 4 qubits with 4 layers\n",
      "Step 260: Loss = -2.999367061109944 for 4 qubits with 4 layers\n",
      "Step 270: Loss = -2.999381148223394 for 4 qubits with 4 layers\n",
      "Step 280: Loss = -2.999395659693413 for 4 qubits with 4 layers\n",
      "Step 290: Loss = -2.999410588612328 for 4 qubits with 4 layers\n",
      "Step 300: Loss = -2.999425927040498 for 4 qubits with 4 layers\n",
      "Step 310: Loss = -2.999441665876688 for 4 qubits with 4 layers\n",
      "Step 320: Loss = -2.9994577948031846 for 4 qubits with 4 layers\n",
      "Step 330: Loss = -2.9994743021879797 for 4 qubits with 4 layers\n",
      "Step 340: Loss = -2.9994911749602298 for 4 qubits with 4 layers\n",
      "Step 350: Loss = -2.999508398467757 for 4 qubits with 4 layers\n",
      "Step 360: Loss = -2.9995259563157566 for 4 qubits with 4 layers\n",
      "Step 370: Loss = -2.9995438301827444 for 4 qubits with 4 layers\n",
      "Step 380: Loss = -2.999561999602624 for 4 qubits with 4 layers\n",
      "Step 390: Loss = -2.9995804417096226 for 4 qubits with 4 layers\n",
      "Step 400: Loss = -2.999599130945648 for 4 qubits with 4 layers\n",
      "Step 410: Loss = -2.9996180387262914 for 4 qubits with 4 layers\n",
      "Step 420: Loss = -2.9996371330622185 for 4 qubits with 4 layers\n",
      "Step 430: Loss = -2.999656378137638 for 4 qubits with 4 layers\n",
      "Step 440: Loss = -2.9996757338497915 for 4 qubits with 4 layers\n",
      "Step 450: Loss = -2.999695155317779 for 4 qubits with 4 layers\n",
      "Step 460: Loss = -2.99971459237491 for 4 qubits with 4 layers\n",
      "Step 470: Loss = -2.999733989067635 for 4 qubits with 4 layers\n",
      "Step 480: Loss = -2.999753283193454 for 4 qubits with 4 layers\n",
      "Step 490: Loss = -2.9997724059222826 for 4 qubits with 4 layers\n",
      "Running optimization for 4 qubits and 5 layers\n",
      "Step 0: Loss = -1.3970245668066652 for 4 qubits with 5 layers\n",
      "Step 10: Loss = -2.410712204537478 for 4 qubits with 5 layers\n",
      "Step 20: Loss = -2.907530477368682 for 4 qubits with 5 layers\n",
      "Step 30: Loss = -2.928579776925576 for 4 qubits with 5 layers\n",
      "Step 40: Loss = -2.992733101269162 for 4 qubits with 5 layers\n",
      "Step 50: Loss = -2.989349483947315 for 4 qubits with 5 layers\n",
      "Step 60: Loss = -2.998319960623939 for 4 qubits with 5 layers\n",
      "Step 70: Loss = -2.998676301799324 for 4 qubits with 5 layers\n",
      "Step 80: Loss = -2.999446400029254 for 4 qubits with 5 layers\n",
      "Step 90: Loss = -2.9997709852989844 for 4 qubits with 5 layers\n",
      "Step 100: Loss = -2.9998183841089494 for 4 qubits with 5 layers\n",
      "Step 110: Loss = -2.999901870081328 for 4 qubits with 5 layers\n",
      "Step 120: Loss = -2.999945348877964 for 4 qubits with 5 layers\n",
      "Step 130: Loss = -2.99996503349134 for 4 qubits with 5 layers\n",
      "Step 140: Loss = -2.999978629700884 for 4 qubits with 5 layers\n",
      "Step 150: Loss = -2.9999877558645185 for 4 qubits with 5 layers\n",
      "Step 160: Loss = -2.9999932022968294 for 4 qubits with 5 layers\n",
      "Step 170: Loss = -2.9999963747920684 for 4 qubits with 5 layers\n",
      "Step 180: Loss = -2.9999981546425643 for 4 qubits with 5 layers\n",
      "Step 190: Loss = -2.999999118855495 for 4 qubits with 5 layers\n",
      "Step 200: Loss = -2.9999996104680466 for 4 qubits with 5 layers\n",
      "Step 210: Loss = -2.999999842992036 for 4 qubits with 5 layers\n",
      "Step 220: Loss = -2.9999999437176754 for 4 qubits with 5 layers\n",
      "Step 230: Loss = -2.999999982771996 for 4 qubits with 5 layers\n",
      "Step 240: Loss = -2.9999999958284445 for 4 qubits with 5 layers\n",
      "Step 250: Loss = -2.999999999338517 for 4 qubits with 5 layers\n",
      "Step 260: Loss = -2.9999999999691407 for 4 qubits with 5 layers\n",
      "Step 270: Loss = -2.999999999987568 for 4 qubits with 5 layers\n",
      "Step 280: Loss = -2.9999999999668496 for 4 qubits with 5 layers\n",
      "Step 290: Loss = -2.9999999999748677 for 4 qubits with 5 layers\n",
      "Step 300: Loss = -2.9999999999891047 for 4 qubits with 5 layers\n",
      "Step 310: Loss = -2.999999999997167 for 4 qubits with 5 layers\n",
      "Step 320: Loss = -2.999999999999679 for 4 qubits with 5 layers\n",
      "Step 330: Loss = -2.99999999999998 for 4 qubits with 5 layers\n",
      "Step 340: Loss = -2.9999999999999174 for 4 qubits with 5 layers\n",
      "Step 350: Loss = -2.999999999998333 for 4 qubits with 5 layers\n",
      "Step 360: Loss = -2.999999112446724 for 4 qubits with 5 layers\n",
      "Step 370: Loss = -2.9999719303571197 for 4 qubits with 5 layers\n",
      "Step 380: Loss = -2.9999593405273277 for 4 qubits with 5 layers\n",
      "Step 390: Loss = -2.9998994598777564 for 4 qubits with 5 layers\n",
      "Step 400: Loss = -2.9999774845681917 for 4 qubits with 5 layers\n",
      "Step 410: Loss = -2.9999955381425565 for 4 qubits with 5 layers\n",
      "Step 420: Loss = -2.999996488009911 for 4 qubits with 5 layers\n",
      "Step 430: Loss = -2.999998417597606 for 4 qubits with 5 layers\n",
      "Step 440: Loss = -2.9999996037692283 for 4 qubits with 5 layers\n",
      "Step 450: Loss = -2.9999999406848064 for 4 qubits with 5 layers\n",
      "Step 460: Loss = -2.9999999006723543 for 4 qubits with 5 layers\n",
      "Step 470: Loss = -2.999999998729666 for 4 qubits with 5 layers\n",
      "Step 480: Loss = -2.9999999999746234 for 4 qubits with 5 layers\n",
      "Step 490: Loss = -2.9999999878122905 for 4 qubits with 5 layers\n",
      "Running optimization for 4 qubits and 6 layers\n",
      "Step 0: Loss = -0.2517671877486145 for 4 qubits with 6 layers\n",
      "Step 10: Loss = -2.1673820284412635 for 4 qubits with 6 layers\n",
      "Step 20: Loss = -2.508080084620513 for 4 qubits with 6 layers\n",
      "Step 30: Loss = -2.9073129146536694 for 4 qubits with 6 layers\n",
      "Step 40: Loss = -2.971799798341456 for 4 qubits with 6 layers\n",
      "Step 50: Loss = -2.9927372295550394 for 4 qubits with 6 layers\n",
      "Step 60: Loss = -2.9940046828442326 for 4 qubits with 6 layers\n",
      "Step 70: Loss = -2.998913733903922 for 4 qubits with 6 layers\n",
      "Step 80: Loss = -2.999341948261839 for 4 qubits with 6 layers\n",
      "Step 90: Loss = -2.999676305292455 for 4 qubits with 6 layers\n",
      "Step 100: Loss = -2.9999806969239122 for 4 qubits with 6 layers\n",
      "Step 110: Loss = -2.999974241127169 for 4 qubits with 6 layers\n",
      "Step 120: Loss = -2.9999941715945573 for 4 qubits with 6 layers\n",
      "Step 130: Loss = -2.9999959031307513 for 4 qubits with 6 layers\n",
      "Step 140: Loss = -2.999998707162776 for 4 qubits with 6 layers\n",
      "Step 150: Loss = -2.999999428491149 for 4 qubits with 6 layers\n",
      "Step 160: Loss = -2.999999552124062 for 4 qubits with 6 layers\n",
      "Step 170: Loss = -2.9999996050589717 for 4 qubits with 6 layers\n",
      "Step 180: Loss = -2.9999996431102716 for 4 qubits with 6 layers\n",
      "Step 190: Loss = -2.999999667929733 for 4 qubits with 6 layers\n",
      "Step 200: Loss = -2.999999688133199 for 4 qubits with 6 layers\n",
      "Step 210: Loss = -2.9999996998077223 for 4 qubits with 6 layers\n",
      "Step 220: Loss = -2.9999997122134663 for 4 qubits with 6 layers\n",
      "Step 230: Loss = -2.999999723908559 for 4 qubits with 6 layers\n",
      "Step 240: Loss = -2.99999973570194 for 4 qubits with 6 layers\n",
      "Step 250: Loss = -2.999999747544997 for 4 qubits with 6 layers\n",
      "Step 260: Loss = -2.999999759438161 for 4 qubits with 6 layers\n",
      "Step 270: Loss = -2.999999771372149 for 4 qubits with 6 layers\n",
      "Step 280: Loss = -2.9999997833152463 for 4 qubits with 6 layers\n",
      "Step 290: Loss = -2.9999997952242468 for 4 qubits with 6 layers\n",
      "Step 300: Loss = -2.9999998070635856 for 4 qubits with 6 layers\n",
      "Step 310: Loss = -2.999999818793979 for 4 qubits with 6 layers\n",
      "Step 320: Loss = -2.999999830376008 for 4 qubits with 6 layers\n",
      "Step 330: Loss = -2.999999841770404 for 4 qubits with 6 layers\n",
      "Step 340: Loss = -2.999999852938407 for 4 qubits with 6 layers\n",
      "Step 350: Loss = -2.9999998638420564 for 4 qubits with 6 layers\n",
      "Step 360: Loss = -2.9999998744446614 for 4 qubits with 6 layers\n",
      "Step 370: Loss = -2.9999998847111815 for 4 qubits with 6 layers\n",
      "Step 380: Loss = -2.9999998946086563 for 4 qubits with 6 layers\n",
      "Step 390: Loss = -2.999999904106633 for 4 qubits with 6 layers\n",
      "Step 400: Loss = -2.9999999131775446 for 4 qubits with 6 layers\n",
      "Step 410: Loss = -2.999999921797128 for 4 qubits with 6 layers\n",
      "Step 420: Loss = -2.9999999299447606 for 4 qubits with 6 layers\n",
      "Step 430: Loss = -2.9999999376038176 for 4 qubits with 6 layers\n",
      "Step 440: Loss = -2.9999999447618855 for 4 qubits with 6 layers\n",
      "Step 450: Loss = -2.9999999514109934 for 4 qubits with 6 layers\n",
      "Step 460: Loss = -2.9999999575477863 for 4 qubits with 6 layers\n",
      "Step 470: Loss = -2.999999963173523 for 4 qubits with 6 layers\n",
      "Step 480: Loss = -2.999999968294092 for 4 qubits with 6 layers\n",
      "Step 490: Loss = -2.999999972919924 for 4 qubits with 6 layers\n",
      "Running optimization for 4 qubits and 7 layers\n",
      "Step 0: Loss = -1.1204980971577494 for 4 qubits with 7 layers\n",
      "Step 10: Loss = -2.7713142325120206 for 4 qubits with 7 layers\n",
      "Step 20: Loss = -2.9856399982283337 for 4 qubits with 7 layers\n",
      "Step 30: Loss = -2.977404744429962 for 4 qubits with 7 layers\n",
      "Step 40: Loss = -2.9880312774216606 for 4 qubits with 7 layers\n",
      "Step 50: Loss = -2.9943758329900136 for 4 qubits with 7 layers\n",
      "Step 60: Loss = -2.9969546070346196 for 4 qubits with 7 layers\n",
      "Step 70: Loss = -2.9976095495865733 for 4 qubits with 7 layers\n",
      "Step 80: Loss = -2.998077449844097 for 4 qubits with 7 layers\n",
      "Step 90: Loss = -2.9984626577716322 for 4 qubits with 7 layers\n",
      "Step 100: Loss = -2.9987610820509474 for 4 qubits with 7 layers\n",
      "Step 110: Loss = -2.9990082354302987 for 4 qubits with 7 layers\n",
      "Step 120: Loss = -2.9992138783829936 for 4 qubits with 7 layers\n",
      "Step 130: Loss = -2.9993910653685196 for 4 qubits with 7 layers\n",
      "Step 140: Loss = -2.9995409336960197 for 4 qubits with 7 layers\n",
      "Step 150: Loss = -2.999664532992378 for 4 qubits with 7 layers\n",
      "Step 160: Loss = -2.999763858215375 for 4 qubits with 7 layers\n",
      "Step 170: Loss = -2.999841095440588 for 4 qubits with 7 layers\n",
      "Step 180: Loss = -2.9998986151839757 for 4 qubits with 7 layers\n",
      "Step 190: Loss = -2.9999392299884744 for 4 qubits with 7 layers\n",
      "Step 200: Loss = -2.999966138782489 for 4 qubits with 7 layers\n",
      "Step 210: Loss = -2.9999826815568245 for 4 qubits with 7 layers\n",
      "Step 220: Loss = -2.9999919993263338 for 4 qubits with 7 layers\n",
      "Step 230: Loss = -2.9999967336953373 for 4 qubits with 7 layers\n",
      "Step 240: Loss = -2.999998859900756 for 4 qubits with 7 layers\n",
      "Step 250: Loss = -2.9999996790367933 for 4 qubits with 7 layers\n",
      "Step 260: Loss = -2.9999999359039844 for 4 qubits with 7 layers\n",
      "Step 270: Loss = -2.999999994067585 for 4 qubits with 7 layers\n",
      "Step 280: Loss = -2.9999999999475273 for 4 qubits with 7 layers\n",
      "Step 290: Loss = -2.9999999986919863 for 4 qubits with 7 layers\n",
      "Step 300: Loss = -2.9999999986636703 for 4 qubits with 7 layers\n",
      "Step 310: Loss = -2.9999999993322883 for 4 qubits with 7 layers\n",
      "Step 320: Loss = -2.999999999808128 for 4 qubits with 7 layers\n",
      "Step 330: Loss = -2.9999999999757447 for 4 qubits with 7 layers\n",
      "Step 340: Loss = -2.9999999999999667 for 4 qubits with 7 layers\n",
      "Step 350: Loss = -2.9999999999965894 for 4 qubits with 7 layers\n",
      "Step 360: Loss = -2.999999999996751 for 4 qubits with 7 layers\n",
      "Step 370: Loss = -2.99999999999884 for 4 qubits with 7 layers\n",
      "Step 380: Loss = -2.999999999999849 for 4 qubits with 7 layers\n",
      "Step 390: Loss = -2.9999999999999556 for 4 qubits with 7 layers\n",
      "Step 400: Loss = -2.9999999999999427 for 4 qubits with 7 layers\n",
      "Step 410: Loss = -2.9999999999999503 for 4 qubits with 7 layers\n",
      "Step 420: Loss = -2.999999999999966 for 4 qubits with 7 layers\n",
      "Step 430: Loss = -2.9999999999999716 for 4 qubits with 7 layers\n",
      "Step 440: Loss = -2.99999999999997 for 4 qubits with 7 layers\n",
      "Step 450: Loss = -2.999999999999964 for 4 qubits with 7 layers\n",
      "Step 460: Loss = -2.9999999999999654 for 4 qubits with 7 layers\n",
      "Step 470: Loss = -2.999999999999969 for 4 qubits with 7 layers\n",
      "Step 480: Loss = -2.99999999999997 for 4 qubits with 7 layers\n",
      "Step 490: Loss = -2.9999999999999614 for 4 qubits with 7 layers\n",
      "Running optimization for 4 qubits and 8 layers\n",
      "Step 0: Loss = -1.2329155824908304 for 4 qubits with 8 layers\n",
      "Step 10: Loss = -2.920309455085392 for 4 qubits with 8 layers\n",
      "Step 20: Loss = -2.9022414030937944 for 4 qubits with 8 layers\n",
      "Step 30: Loss = -2.9770435711691947 for 4 qubits with 8 layers\n",
      "Step 40: Loss = -2.9910644079458413 for 4 qubits with 8 layers\n",
      "Step 50: Loss = -2.9942168573672485 for 4 qubits with 8 layers\n",
      "Step 60: Loss = -2.999777576389695 for 4 qubits with 8 layers\n",
      "Step 70: Loss = -2.999415561639699 for 4 qubits with 8 layers\n",
      "Step 80: Loss = -2.9997820926008294 for 4 qubits with 8 layers\n",
      "Step 90: Loss = -2.999935005345474 for 4 qubits with 8 layers\n",
      "Step 100: Loss = -2.9999896931456966 for 4 qubits with 8 layers\n",
      "Step 110: Loss = -2.9999979993796697 for 4 qubits with 8 layers\n",
      "Step 120: Loss = -2.999996829835932 for 4 qubits with 8 layers\n",
      "Step 130: Loss = -2.999998795801562 for 4 qubits with 8 layers\n",
      "Step 140: Loss = -2.9999996055856872 for 4 qubits with 8 layers\n",
      "Step 150: Loss = -2.9999998114758424 for 4 qubits with 8 layers\n",
      "Step 160: Loss = -2.9999999212577855 for 4 qubits with 8 layers\n",
      "Step 170: Loss = -2.999999980334148 for 4 qubits with 8 layers\n",
      "Step 180: Loss = -2.9999999967339663 for 4 qubits with 8 layers\n",
      "Step 190: Loss = -2.999999999577088 for 4 qubits with 8 layers\n",
      "Step 200: Loss = -2.9999999997133022 for 4 qubits with 8 layers\n",
      "Step 210: Loss = -2.9999999996746682 for 4 qubits with 8 layers\n",
      "Step 220: Loss = -2.99999999981501 for 4 qubits with 8 layers\n",
      "Step 230: Loss = -2.9999999999535145 for 4 qubits with 8 layers\n",
      "Step 240: Loss = -2.9999999999923004 for 4 qubits with 8 layers\n",
      "Step 250: Loss = -2.999999999991653 for 4 qubits with 8 layers\n",
      "Step 260: Loss = -2.999999999997867 for 4 qubits with 8 layers\n",
      "Step 270: Loss = -2.9999999999994484 for 4 qubits with 8 layers\n",
      "Step 280: Loss = -2.999999999999618 for 4 qubits with 8 layers\n",
      "Step 290: Loss = -2.9999999999999476 for 4 qubits with 8 layers\n",
      "Step 300: Loss = -2.9999999999999236 for 4 qubits with 8 layers\n",
      "Step 310: Loss = -2.9999999999999543 for 4 qubits with 8 layers\n",
      "Step 320: Loss = -2.9999999999999627 for 4 qubits with 8 layers\n",
      "Step 330: Loss = -2.9999999999999667 for 4 qubits with 8 layers\n",
      "Step 340: Loss = -2.999999999999962 for 4 qubits with 8 layers\n",
      "Step 350: Loss = -2.999999999999968 for 4 qubits with 8 layers\n",
      "Step 360: Loss = -2.9999999999999734 for 4 qubits with 8 layers\n",
      "Step 370: Loss = -2.9999999999999627 for 4 qubits with 8 layers\n",
      "Step 380: Loss = -2.999999999999961 for 4 qubits with 8 layers\n",
      "Step 390: Loss = -2.9999999999999627 for 4 qubits with 8 layers\n",
      "Step 400: Loss = -2.99999999999976 for 4 qubits with 8 layers\n",
      "Step 410: Loss = -2.9999997966904397 for 4 qubits with 8 layers\n",
      "Step 420: Loss = -2.9989415587987436 for 4 qubits with 8 layers\n",
      "Step 430: Loss = -2.999804187443379 for 4 qubits with 8 layers\n",
      "Step 440: Loss = -2.999980444951279 for 4 qubits with 8 layers\n",
      "Step 450: Loss = -2.9999883197386006 for 4 qubits with 8 layers\n",
      "Step 460: Loss = -2.9999843371407895 for 4 qubits with 8 layers\n",
      "Step 470: Loss = -2.999994164710057 for 4 qubits with 8 layers\n",
      "Step 480: Loss = -2.9999976666417276 for 4 qubits with 8 layers\n",
      "Step 490: Loss = -2.999999375523803 for 4 qubits with 8 layers\n",
      "Running optimization for 4 qubits and 9 layers\n",
      "Step 0: Loss = -0.8086071799781542 for 4 qubits with 9 layers\n",
      "Step 10: Loss = -2.8597359072652853 for 4 qubits with 9 layers\n",
      "Step 20: Loss = -2.8586017257291196 for 4 qubits with 9 layers\n",
      "Step 30: Loss = -2.9683877260035936 for 4 qubits with 9 layers\n",
      "Step 40: Loss = -2.9905585420875287 for 4 qubits with 9 layers\n",
      "Step 50: Loss = -2.9969913048337937 for 4 qubits with 9 layers\n",
      "Step 60: Loss = -2.9989129222280777 for 4 qubits with 9 layers\n",
      "Step 70: Loss = -2.999772717551903 for 4 qubits with 9 layers\n",
      "Step 80: Loss = -2.9997482487165863 for 4 qubits with 9 layers\n",
      "Step 90: Loss = -2.9999290031590844 for 4 qubits with 9 layers\n",
      "Step 100: Loss = -2.9999622359640448 for 4 qubits with 9 layers\n",
      "Step 110: Loss = -2.999992455913067 for 4 qubits with 9 layers\n",
      "Step 120: Loss = -2.999999907008855 for 4 qubits with 9 layers\n",
      "Step 130: Loss = -2.9999989804808793 for 4 qubits with 9 layers\n",
      "Step 140: Loss = -2.9999995799493635 for 4 qubits with 9 layers\n",
      "Step 150: Loss = -2.9999999220163653 for 4 qubits with 9 layers\n",
      "Step 160: Loss = -2.999999947246191 for 4 qubits with 9 layers\n",
      "Step 170: Loss = -2.999999994639594 for 4 qubits with 9 layers\n",
      "Step 180: Loss = -2.999999993898518 for 4 qubits with 9 layers\n",
      "Step 190: Loss = -2.9999999971890787 for 4 qubits with 9 layers\n",
      "Step 200: Loss = -2.999999999051281 for 4 qubits with 9 layers\n",
      "Step 210: Loss = -2.9999999995859388 for 4 qubits with 9 layers\n",
      "Step 220: Loss = -2.9999999998460556 for 4 qubits with 9 layers\n",
      "Step 230: Loss = -2.999999999988059 for 4 qubits with 9 layers\n",
      "Step 240: Loss = -2.999999999986943 for 4 qubits with 9 layers\n",
      "Step 250: Loss = -2.9999999999959557 for 4 qubits with 9 layers\n",
      "Step 260: Loss = -2.999999999999445 for 4 qubits with 9 layers\n",
      "Step 270: Loss = -2.9999999999995155 for 4 qubits with 9 layers\n",
      "Step 280: Loss = -2.9999999999998965 for 4 qubits with 9 layers\n",
      "Step 290: Loss = -2.9999999999998685 for 4 qubits with 9 layers\n",
      "Step 300: Loss = -2.9999999999999396 for 4 qubits with 9 layers\n",
      "Step 310: Loss = -2.9999999999999556 for 4 qubits with 9 layers\n",
      "Step 320: Loss = -2.99999999999996 for 4 qubits with 9 layers\n",
      "Step 330: Loss = -2.9999999999999667 for 4 qubits with 9 layers\n",
      "Step 340: Loss = -2.9999999999999583 for 4 qubits with 9 layers\n",
      "Step 350: Loss = -2.9999999999999654 for 4 qubits with 9 layers\n",
      "Step 360: Loss = -2.999999999999968 for 4 qubits with 9 layers\n",
      "Step 370: Loss = -2.999999999999976 for 4 qubits with 9 layers\n",
      "Step 380: Loss = -2.9999999999999507 for 4 qubits with 9 layers\n",
      "Step 390: Loss = -2.999999999999953 for 4 qubits with 9 layers\n",
      "Step 400: Loss = -2.999999999999958 for 4 qubits with 9 layers\n",
      "Step 410: Loss = -2.9999999999999547 for 4 qubits with 9 layers\n",
      "Step 420: Loss = -2.9999999999999623 for 4 qubits with 9 layers\n",
      "Step 430: Loss = -2.9999999999999547 for 4 qubits with 9 layers\n",
      "Step 440: Loss = -2.999999999999975 for 4 qubits with 9 layers\n",
      "Step 450: Loss = -2.999999999999968 for 4 qubits with 9 layers\n",
      "Step 460: Loss = -2.9999999999999725 for 4 qubits with 9 layers\n",
      "Step 470: Loss = -2.999999999999967 for 4 qubits with 9 layers\n",
      "Step 480: Loss = -2.9999999999999694 for 4 qubits with 9 layers\n",
      "Step 490: Loss = -2.9999999999999583 for 4 qubits with 9 layers\n",
      "Running optimization for 4 qubits and 10 layers\n",
      "Step 0: Loss = -1.1004298404391561 for 4 qubits with 10 layers\n",
      "Step 10: Loss = -2.942123709500724 for 4 qubits with 10 layers\n",
      "Step 20: Loss = -2.973302274126586 for 4 qubits with 10 layers\n",
      "Step 30: Loss = -2.988930990224393 for 4 qubits with 10 layers\n",
      "Step 40: Loss = -2.9971783269038554 for 4 qubits with 10 layers\n",
      "Step 50: Loss = -2.998845331697566 for 4 qubits with 10 layers\n",
      "Step 60: Loss = -2.999776707535104 for 4 qubits with 10 layers\n",
      "Step 70: Loss = -2.999716257761362 for 4 qubits with 10 layers\n",
      "Step 80: Loss = -2.999931196500029 for 4 qubits with 10 layers\n",
      "Step 90: Loss = -2.9999805273357554 for 4 qubits with 10 layers\n",
      "Step 100: Loss = -2.9999887443101496 for 4 qubits with 10 layers\n",
      "Step 110: Loss = -2.999994586618678 for 4 qubits with 10 layers\n",
      "Step 120: Loss = -2.99999908611518 for 4 qubits with 10 layers\n",
      "Step 130: Loss = -2.999999656703651 for 4 qubits with 10 layers\n",
      "Step 140: Loss = -2.999999884411 for 4 qubits with 10 layers\n",
      "Step 150: Loss = -2.9999999645059834 for 4 qubits with 10 layers\n",
      "Step 160: Loss = -2.9999999886060644 for 4 qubits with 10 layers\n",
      "Step 170: Loss = -2.9999999954199525 for 4 qubits with 10 layers\n",
      "Step 180: Loss = -2.9999999971513445 for 4 qubits with 10 layers\n",
      "Step 190: Loss = -2.9999999994972653 for 4 qubits with 10 layers\n",
      "Step 200: Loss = -2.9999999997944142 for 4 qubits with 10 layers\n",
      "Step 210: Loss = -2.9999999999305826 for 4 qubits with 10 layers\n",
      "Step 220: Loss = -2.9999999999513953 for 4 qubits with 10 layers\n",
      "Step 230: Loss = -2.999999999991748 for 4 qubits with 10 layers\n",
      "Step 240: Loss = -2.9999999999938955 for 4 qubits with 10 layers\n",
      "Step 250: Loss = -2.999999999996116 for 4 qubits with 10 layers\n",
      "Step 260: Loss = -2.9999999999993987 for 4 qubits with 10 layers\n",
      "Step 270: Loss = -2.9999999999997566 for 4 qubits with 10 layers\n",
      "Step 280: Loss = -2.9999999999997975 for 4 qubits with 10 layers\n",
      "Step 290: Loss = -2.9999999999999245 for 4 qubits with 10 layers\n",
      "Step 300: Loss = -2.9999999999999543 for 4 qubits with 10 layers\n",
      "Step 310: Loss = -2.9999999999999565 for 4 qubits with 10 layers\n",
      "Step 320: Loss = -2.9999999999999547 for 4 qubits with 10 layers\n",
      "Step 330: Loss = -2.9999999999999525 for 4 qubits with 10 layers\n",
      "Step 340: Loss = -2.9999999999999543 for 4 qubits with 10 layers\n",
      "Step 350: Loss = -2.99999999999996 for 4 qubits with 10 layers\n",
      "Step 360: Loss = -2.999999999999949 for 4 qubits with 10 layers\n",
      "Step 370: Loss = -2.999999999999953 for 4 qubits with 10 layers\n",
      "Step 380: Loss = -2.999999999999951 for 4 qubits with 10 layers\n",
      "Step 390: Loss = -2.99999999999996 for 4 qubits with 10 layers\n",
      "Step 400: Loss = -2.9999999999999543 for 4 qubits with 10 layers\n",
      "Step 410: Loss = -2.999999999999969 for 4 qubits with 10 layers\n",
      "Step 420: Loss = -2.9999999999999583 for 4 qubits with 10 layers\n",
      "Step 430: Loss = -2.999999999997126 for 4 qubits with 10 layers\n",
      "Step 440: Loss = -2.9999914552418137 for 4 qubits with 10 layers\n",
      "Step 450: Loss = -2.998783501308994 for 4 qubits with 10 layers\n",
      "Step 460: Loss = -2.9999310716767815 for 4 qubits with 10 layers\n",
      "Step 470: Loss = -2.999896053679029 for 4 qubits with 10 layers\n",
      "Step 480: Loss = -2.9999515779735337 for 4 qubits with 10 layers\n",
      "Step 490: Loss = -2.999993120880475 for 4 qubits with 10 layers\n",
      "Running optimization for 4 qubits and 11 layers\n",
      "Step 0: Loss = -1.3938750636301283 for 4 qubits with 11 layers\n",
      "Step 10: Loss = -2.954160723906035 for 4 qubits with 11 layers\n",
      "Step 20: Loss = -2.957760887274312 for 4 qubits with 11 layers\n",
      "Step 30: Loss = -2.9912514946010287 for 4 qubits with 11 layers\n",
      "Step 40: Loss = -2.995009995471893 for 4 qubits with 11 layers\n",
      "Step 50: Loss = -2.9993463644909824 for 4 qubits with 11 layers\n",
      "Step 60: Loss = -2.999353698354917 for 4 qubits with 11 layers\n",
      "Step 70: Loss = -2.9998848845273995 for 4 qubits with 11 layers\n",
      "Step 80: Loss = -2.9999263175840687 for 4 qubits with 11 layers\n",
      "Step 90: Loss = -2.9999712969932073 for 4 qubits with 11 layers\n",
      "Step 100: Loss = -2.99999719402285 for 4 qubits with 11 layers\n",
      "Step 110: Loss = -2.9999948625828807 for 4 qubits with 11 layers\n",
      "Step 120: Loss = -2.999999474951834 for 4 qubits with 11 layers\n",
      "Step 130: Loss = -2.999999695708289 for 4 qubits with 11 layers\n",
      "Step 140: Loss = -2.99999974856423 for 4 qubits with 11 layers\n",
      "Step 150: Loss = -2.999999956885911 for 4 qubits with 11 layers\n",
      "Step 160: Loss = -2.999999997991099 for 4 qubits with 11 layers\n",
      "Step 170: Loss = -2.9999999928926604 for 4 qubits with 11 layers\n",
      "Step 180: Loss = -2.9999999955287153 for 4 qubits with 11 layers\n",
      "Step 190: Loss = -2.9999999989037702 for 4 qubits with 11 layers\n",
      "Step 200: Loss = -2.9999999998457008 for 4 qubits with 11 layers\n",
      "Step 210: Loss = -2.9999999999614264 for 4 qubits with 11 layers\n",
      "Step 220: Loss = -2.9999999999840736 for 4 qubits with 11 layers\n",
      "Step 230: Loss = -2.9999999999909863 for 4 qubits with 11 layers\n",
      "Step 240: Loss = -2.99999999995525 for 4 qubits with 11 layers\n",
      "Step 250: Loss = -2.9999988623539813 for 4 qubits with 11 layers\n",
      "Step 260: Loss = -2.998517256048054 for 4 qubits with 11 layers\n",
      "Step 270: Loss = -2.99975965809067 for 4 qubits with 11 layers\n",
      "Step 280: Loss = -2.999765779568858 for 4 qubits with 11 layers\n",
      "Step 290: Loss = -2.9999882673927525 for 4 qubits with 11 layers\n",
      "Step 300: Loss = -2.9999937445276297 for 4 qubits with 11 layers\n",
      "Step 310: Loss = -2.9999988062955616 for 4 qubits with 11 layers\n",
      "Step 320: Loss = -2.9999961185494888 for 4 qubits with 11 layers\n",
      "Step 330: Loss = -2.999999769223697 for 4 qubits with 11 layers\n",
      "Step 340: Loss = -2.999998718860953 for 4 qubits with 11 layers\n",
      "Step 350: Loss = -2.9999983664554244 for 4 qubits with 11 layers\n",
      "Step 360: Loss = -2.99979547455597 for 4 qubits with 11 layers\n",
      "Step 370: Loss = -2.9996452158497338 for 4 qubits with 11 layers\n",
      "Step 380: Loss = -2.9996335996700765 for 4 qubits with 11 layers\n",
      "Step 390: Loss = -2.999998393626198 for 4 qubits with 11 layers\n",
      "Step 400: Loss = -2.9999485364407104 for 4 qubits with 11 layers\n",
      "Step 410: Loss = -2.9999812941913517 for 4 qubits with 11 layers\n",
      "Step 420: Loss = -2.9999876699557806 for 4 qubits with 11 layers\n",
      "Step 430: Loss = -2.999975556524762 for 4 qubits with 11 layers\n",
      "Step 440: Loss = -2.999371417187557 for 4 qubits with 11 layers\n",
      "Step 450: Loss = -2.9999751464254887 for 4 qubits with 11 layers\n",
      "Step 460: Loss = -2.9998781977324485 for 4 qubits with 11 layers\n",
      "Step 470: Loss = -2.9999270497057386 for 4 qubits with 11 layers\n",
      "Step 480: Loss = -2.999997796741747 for 4 qubits with 11 layers\n",
      "Step 490: Loss = -2.9999914920789337 for 4 qubits with 11 layers\n",
      "Running optimization for 4 qubits and 12 layers\n",
      "Step 0: Loss = -0.9866318550575179 for 4 qubits with 12 layers\n",
      "Step 10: Loss = -2.826863416649891 for 4 qubits with 12 layers\n",
      "Step 20: Loss = -2.900329586526432 for 4 qubits with 12 layers\n",
      "Step 30: Loss = -2.9658603021149395 for 4 qubits with 12 layers\n",
      "Step 40: Loss = -2.99554521770205 for 4 qubits with 12 layers\n",
      "Step 50: Loss = -2.9978810024942377 for 4 qubits with 12 layers\n",
      "Step 60: Loss = -2.9985141625844793 for 4 qubits with 12 layers\n",
      "Step 70: Loss = -2.9992861202233483 for 4 qubits with 12 layers\n",
      "Step 80: Loss = -2.9998413305508977 for 4 qubits with 12 layers\n",
      "Step 90: Loss = -2.999985484386176 for 4 qubits with 12 layers\n",
      "Step 100: Loss = -2.999980850182973 for 4 qubits with 12 layers\n",
      "Step 110: Loss = -2.999997180316939 for 4 qubits with 12 layers\n",
      "Step 120: Loss = -2.9999971021533614 for 4 qubits with 12 layers\n",
      "Step 130: Loss = -2.9999996669082267 for 4 qubits with 12 layers\n",
      "Step 140: Loss = -2.999999791025009 for 4 qubits with 12 layers\n",
      "Step 150: Loss = -2.999999853506945 for 4 qubits with 12 layers\n",
      "Step 160: Loss = -2.999999956343845 for 4 qubits with 12 layers\n",
      "Step 170: Loss = -2.9999999821481653 for 4 qubits with 12 layers\n",
      "Step 180: Loss = -2.9999999926332492 for 4 qubits with 12 layers\n",
      "Step 190: Loss = -2.9999999989151926 for 4 qubits with 12 layers\n",
      "Step 200: Loss = -2.9999999995480215 for 4 qubits with 12 layers\n",
      "Step 210: Loss = -2.9999999996210773 for 4 qubits with 12 layers\n",
      "Step 220: Loss = -2.9999999999441354 for 4 qubits with 12 layers\n",
      "Step 230: Loss = -2.9999999999592823 for 4 qubits with 12 layers\n",
      "Step 240: Loss = -2.9999999999808864 for 4 qubits with 12 layers\n",
      "Step 250: Loss = -2.9999999999971836 for 4 qubits with 12 layers\n",
      "Step 260: Loss = -2.9999999999979465 for 4 qubits with 12 layers\n",
      "Step 270: Loss = -2.999999999999141 for 4 qubits with 12 layers\n",
      "Step 280: Loss = -2.9999999999997726 for 4 qubits with 12 layers\n",
      "Step 290: Loss = -2.999999999999836 for 4 qubits with 12 layers\n",
      "Step 300: Loss = -2.9999999999999165 for 4 qubits with 12 layers\n",
      "Step 310: Loss = -2.999999999999957 for 4 qubits with 12 layers\n",
      "Step 320: Loss = -2.999999999999947 for 4 qubits with 12 layers\n",
      "Step 330: Loss = -2.999999999999943 for 4 qubits with 12 layers\n",
      "Step 340: Loss = -2.9999999999999445 for 4 qubits with 12 layers\n",
      "Step 350: Loss = -2.999999999999942 for 4 qubits with 12 layers\n",
      "Step 360: Loss = -2.9999999999999494 for 4 qubits with 12 layers\n",
      "Step 370: Loss = -2.9999999999999516 for 4 qubits with 12 layers\n",
      "Step 380: Loss = -2.9999999999999503 for 4 qubits with 12 layers\n",
      "Step 390: Loss = -2.9999999999999436 for 4 qubits with 12 layers\n",
      "Step 400: Loss = -2.9999999999997518 for 4 qubits with 12 layers\n",
      "Step 410: Loss = -2.9999999424314474 for 4 qubits with 12 layers\n",
      "Step 420: Loss = -2.9992311539270142 for 4 qubits with 12 layers\n",
      "Step 430: Loss = -2.999025299774416 for 4 qubits with 12 layers\n",
      "Step 440: Loss = -2.9999896052223143 for 4 qubits with 12 layers\n",
      "Step 450: Loss = -2.9998570789092547 for 4 qubits with 12 layers\n",
      "Step 460: Loss = -2.9999909687416104 for 4 qubits with 12 layers\n",
      "Step 470: Loss = -2.999986360770116 for 4 qubits with 12 layers\n",
      "Step 480: Loss = -2.9999967499665434 for 4 qubits with 12 layers\n",
      "Step 490: Loss = -2.999997473085948 for 4 qubits with 12 layers\n",
      "Running optimization for 4 qubits and 13 layers\n",
      "Step 0: Loss = -0.4046142234839861 for 4 qubits with 13 layers\n",
      "Step 10: Loss = -2.712399029294696 for 4 qubits with 13 layers\n",
      "Step 20: Loss = -2.9207718600971404 for 4 qubits with 13 layers\n",
      "Step 30: Loss = -2.9718545267796936 for 4 qubits with 13 layers\n",
      "Step 40: Loss = -2.9850975095726504 for 4 qubits with 13 layers\n",
      "Step 50: Loss = -2.9965959002034 for 4 qubits with 13 layers\n",
      "Step 60: Loss = -2.998850268366435 for 4 qubits with 13 layers\n",
      "Step 70: Loss = -2.999560967488794 for 4 qubits with 13 layers\n",
      "Step 80: Loss = -2.9998231790970173 for 4 qubits with 13 layers\n",
      "Step 90: Loss = -2.9999375239238253 for 4 qubits with 13 layers\n",
      "Step 100: Loss = -2.9999575273965275 for 4 qubits with 13 layers\n",
      "Step 110: Loss = -2.999990844421456 for 4 qubits with 13 layers\n",
      "Step 120: Loss = -2.9999943934298496 for 4 qubits with 13 layers\n",
      "Step 130: Loss = -2.9999978725521066 for 4 qubits with 13 layers\n",
      "Step 140: Loss = -2.999999608258024 for 4 qubits with 13 layers\n",
      "Step 150: Loss = -2.9999997748396376 for 4 qubits with 13 layers\n",
      "Step 160: Loss = -2.9999999245100843 for 4 qubits with 13 layers\n",
      "Step 170: Loss = -2.999999966561302 for 4 qubits with 13 layers\n",
      "Step 180: Loss = -2.999999987891413 for 4 qubits with 13 layers\n",
      "Step 190: Loss = -2.999999996957773 for 4 qubits with 13 layers\n",
      "Step 200: Loss = -2.9999999987986814 for 4 qubits with 13 layers\n",
      "Step 210: Loss = -2.9999999996646367 for 4 qubits with 13 layers\n",
      "Step 220: Loss = -2.999999999883379 for 4 qubits with 13 layers\n",
      "Step 230: Loss = -2.9999999999419065 for 4 qubits with 13 layers\n",
      "Step 240: Loss = -2.999999999972153 for 4 qubits with 13 layers\n",
      "Step 250: Loss = -2.999999999994182 for 4 qubits with 13 layers\n",
      "Step 260: Loss = -2.9999999999977636 for 4 qubits with 13 layers\n",
      "Step 270: Loss = -2.999999999999158 for 4 qubits with 13 layers\n",
      "Step 280: Loss = -2.999999999999659 for 4 qubits with 13 layers\n",
      "Step 290: Loss = -2.9999999999998175 for 4 qubits with 13 layers\n",
      "Step 300: Loss = -2.999999999999906 for 4 qubits with 13 layers\n",
      "Step 310: Loss = -2.99999999999993 for 4 qubits with 13 layers\n",
      "Step 320: Loss = -2.999999999999938 for 4 qubits with 13 layers\n",
      "Step 330: Loss = -2.9999999999999467 for 4 qubits with 13 layers\n",
      "Step 340: Loss = -2.9999999999999405 for 4 qubits with 13 layers\n",
      "Step 350: Loss = -2.9999999999999436 for 4 qubits with 13 layers\n",
      "Step 360: Loss = -2.9999999999999467 for 4 qubits with 13 layers\n",
      "Step 370: Loss = -2.9999999999999445 for 4 qubits with 13 layers\n",
      "Step 380: Loss = -2.9999999999999525 for 4 qubits with 13 layers\n",
      "Step 390: Loss = -2.999999999999954 for 4 qubits with 13 layers\n",
      "Step 400: Loss = -2.999999999999936 for 4 qubits with 13 layers\n",
      "Step 410: Loss = -2.999999999999938 for 4 qubits with 13 layers\n",
      "Step 420: Loss = -2.9999999999999423 for 4 qubits with 13 layers\n",
      "Step 430: Loss = -2.999999999999941 for 4 qubits with 13 layers\n",
      "Step 440: Loss = -2.999999999999954 for 4 qubits with 13 layers\n",
      "Step 450: Loss = -2.9999999999999405 for 4 qubits with 13 layers\n",
      "Step 460: Loss = -2.999999996105543 for 4 qubits with 13 layers\n",
      "Step 470: Loss = -2.995157255196237 for 4 qubits with 13 layers\n",
      "Step 480: Loss = -2.999622975603447 for 4 qubits with 13 layers\n",
      "Step 490: Loss = -2.999836089178168 for 4 qubits with 13 layers\n",
      "Running optimization for 4 qubits and 14 layers\n",
      "Step 0: Loss = -1.5093710136777623 for 4 qubits with 14 layers\n",
      "Step 10: Loss = -2.861155748475881 for 4 qubits with 14 layers\n",
      "Step 20: Loss = -2.9787604243209507 for 4 qubits with 14 layers\n",
      "Step 30: Loss = -2.9923560019221123 for 4 qubits with 14 layers\n",
      "Step 40: Loss = -2.9958477337933065 for 4 qubits with 14 layers\n",
      "Step 50: Loss = -2.997082567945688 for 4 qubits with 14 layers\n",
      "Step 60: Loss = -2.998755900541221 for 4 qubits with 14 layers\n",
      "Step 70: Loss = -2.999754546573712 for 4 qubits with 14 layers\n",
      "Step 80: Loss = -2.999885295768022 for 4 qubits with 14 layers\n",
      "Step 90: Loss = -2.9999806812453054 for 4 qubits with 14 layers\n",
      "Step 100: Loss = -2.999977886007965 for 4 qubits with 14 layers\n",
      "Step 110: Loss = -2.999998869231483 for 4 qubits with 14 layers\n",
      "Step 120: Loss = -2.999997009333577 for 4 qubits with 14 layers\n",
      "Step 130: Loss = -2.999999362246347 for 4 qubits with 14 layers\n",
      "Step 140: Loss = -2.9999997966170673 for 4 qubits with 14 layers\n",
      "Step 150: Loss = -2.999999983546253 for 4 qubits with 14 layers\n",
      "Step 160: Loss = -2.999999977668576 for 4 qubits with 14 layers\n",
      "Step 170: Loss = -2.9999999869856095 for 4 qubits with 14 layers\n",
      "Step 180: Loss = -2.9999999972006433 for 4 qubits with 14 layers\n",
      "Step 190: Loss = -2.9999999995152136 for 4 qubits with 14 layers\n",
      "Step 200: Loss = -2.9999999998513616 for 4 qubits with 14 layers\n",
      "Step 210: Loss = -2.99999999984621 for 4 qubits with 14 layers\n",
      "Step 220: Loss = -2.999999999934776 for 4 qubits with 14 layers\n",
      "Step 230: Loss = -2.9999999999941926 for 4 qubits with 14 layers\n",
      "Step 240: Loss = -2.9999999999867635 for 4 qubits with 14 layers\n",
      "Step 250: Loss = -2.99999999999703 for 4 qubits with 14 layers\n",
      "Step 260: Loss = -2.999999999998341 for 4 qubits with 14 layers\n",
      "Step 270: Loss = -2.999999999999743 for 4 qubits with 14 layers\n",
      "Step 280: Loss = -2.9999999999998095 for 4 qubits with 14 layers\n",
      "Step 290: Loss = -2.999999999999897 for 4 qubits with 14 layers\n",
      "Step 300: Loss = -2.9999999999999085 for 4 qubits with 14 layers\n",
      "Step 310: Loss = -2.999999999999938 for 4 qubits with 14 layers\n",
      "Step 320: Loss = -2.999999999999938 for 4 qubits with 14 layers\n",
      "Step 330: Loss = -2.9999999999999387 for 4 qubits with 14 layers\n",
      "Step 340: Loss = -2.9999999999999485 for 4 qubits with 14 layers\n",
      "Step 350: Loss = -2.99999999999994 for 4 qubits with 14 layers\n",
      "Step 360: Loss = -2.999999999999936 for 4 qubits with 14 layers\n",
      "Step 370: Loss = -2.9999999999999347 for 4 qubits with 14 layers\n",
      "Step 380: Loss = -2.9999999999999476 for 4 qubits with 14 layers\n",
      "Step 390: Loss = -2.9999999999990523 for 4 qubits with 14 layers\n",
      "Step 400: Loss = -2.9999991268412325 for 4 qubits with 14 layers\n",
      "Step 410: Loss = -2.9991426933384537 for 4 qubits with 14 layers\n",
      "Step 420: Loss = -2.9997373080190233 for 4 qubits with 14 layers\n",
      "Step 430: Loss = -2.999943900016229 for 4 qubits with 14 layers\n",
      "Step 440: Loss = -2.9999768286447805 for 4 qubits with 14 layers\n",
      "Step 450: Loss = -2.9999806585621305 for 4 qubits with 14 layers\n",
      "Step 460: Loss = -2.999995492505404 for 4 qubits with 14 layers\n",
      "Step 470: Loss = -2.9999992864402554 for 4 qubits with 14 layers\n",
      "Step 480: Loss = -2.9999994526532348 for 4 qubits with 14 layers\n",
      "Step 490: Loss = -2.999999261685283 for 4 qubits with 14 layers\n",
      "Running optimization for 4 qubits and 15 layers\n",
      "Step 0: Loss = -1.04332790507524 for 4 qubits with 15 layers\n",
      "Step 10: Loss = -2.819219699845825 for 4 qubits with 15 layers\n",
      "Step 20: Loss = -2.968467404297972 for 4 qubits with 15 layers\n",
      "Step 30: Loss = -2.9912086155123117 for 4 qubits with 15 layers\n",
      "Step 40: Loss = -2.9947341191699905 for 4 qubits with 15 layers\n",
      "Step 50: Loss = -2.998628417922261 for 4 qubits with 15 layers\n",
      "Step 60: Loss = -2.9998643966337526 for 4 qubits with 15 layers\n",
      "Step 70: Loss = -2.9997125571482717 for 4 qubits with 15 layers\n",
      "Step 80: Loss = -2.9998421458203204 for 4 qubits with 15 layers\n",
      "Step 90: Loss = -2.999930830656319 for 4 qubits with 15 layers\n",
      "Step 100: Loss = -2.9999698159528267 for 4 qubits with 15 layers\n",
      "Step 110: Loss = -2.999991229711632 for 4 qubits with 15 layers\n",
      "Step 120: Loss = -2.999998475477405 for 4 qubits with 15 layers\n",
      "Step 130: Loss = -2.999999167963563 for 4 qubits with 15 layers\n",
      "Step 140: Loss = -2.999999722715295 for 4 qubits with 15 layers\n",
      "Step 150: Loss = -2.999999877459159 for 4 qubits with 15 layers\n",
      "Step 160: Loss = -2.9999999686510628 for 4 qubits with 15 layers\n",
      "Step 170: Loss = -2.999999983648339 for 4 qubits with 15 layers\n",
      "Step 180: Loss = -2.9999999944137605 for 4 qubits with 15 layers\n",
      "Step 190: Loss = -2.9999999978247236 for 4 qubits with 15 layers\n",
      "Step 200: Loss = -2.9999999992026978 for 4 qubits with 15 layers\n",
      "Step 210: Loss = -2.9999999998361515 for 4 qubits with 15 layers\n",
      "Step 220: Loss = -2.9999999999544444 for 4 qubits with 15 layers\n",
      "Step 230: Loss = -2.999999999972275 for 4 qubits with 15 layers\n",
      "Step 240: Loss = -2.999999999987067 for 4 qubits with 15 layers\n",
      "Step 250: Loss = -2.999999999999071 for 4 qubits with 15 layers\n",
      "Step 260: Loss = -2.9999999999987077 for 4 qubits with 15 layers\n",
      "Step 270: Loss = -2.9999999999997806 for 4 qubits with 15 layers\n",
      "Step 280: Loss = -2.999999999999716 for 4 qubits with 15 layers\n",
      "Step 290: Loss = -2.9999999999998197 for 4 qubits with 15 layers\n",
      "Step 300: Loss = -2.999999999999914 for 4 qubits with 15 layers\n",
      "Step 310: Loss = -2.999999999999923 for 4 qubits with 15 layers\n",
      "Step 320: Loss = -2.9999999999999316 for 4 qubits with 15 layers\n",
      "Step 330: Loss = -2.9999999999999343 for 4 qubits with 15 layers\n",
      "Step 340: Loss = -2.9999999999999276 for 4 qubits with 15 layers\n",
      "Step 350: Loss = -2.999999999999939 for 4 qubits with 15 layers\n",
      "Step 360: Loss = -2.9999999999999267 for 4 qubits with 15 layers\n",
      "Step 370: Loss = -2.999999999999937 for 4 qubits with 15 layers\n",
      "Step 380: Loss = -2.9999999999999476 for 4 qubits with 15 layers\n",
      "Step 390: Loss = -2.999999999999935 for 4 qubits with 15 layers\n",
      "Step 400: Loss = -2.9999999999999414 for 4 qubits with 15 layers\n",
      "Step 410: Loss = -2.999999999999934 for 4 qubits with 15 layers\n",
      "Step 420: Loss = -2.999999999982169 for 4 qubits with 15 layers\n",
      "Step 430: Loss = -2.999977958735459 for 4 qubits with 15 layers\n",
      "Step 440: Loss = -2.9971524541559917 for 4 qubits with 15 layers\n",
      "Step 450: Loss = -2.999646197987986 for 4 qubits with 15 layers\n",
      "Step 460: Loss = -2.999864518090831 for 4 qubits with 15 layers\n",
      "Step 470: Loss = -2.999875926285245 for 4 qubits with 15 layers\n",
      "Step 480: Loss = -2.999965474588402 for 4 qubits with 15 layers\n",
      "Step 490: Loss = -2.9999652546022055 for 4 qubits with 15 layers\n",
      "Running optimization for 5 qubits and 1 layers\n",
      "Step 0: Loss = -1.3476102458493442 for 5 qubits with 1 layers\n",
      "Step 10: Loss = -2.1370667939600545 for 5 qubits with 1 layers\n",
      "Step 20: Loss = -2.903167379761456 for 5 qubits with 1 layers\n",
      "Step 30: Loss = -3.5039363443283076 for 5 qubits with 1 layers\n",
      "Step 40: Loss = -3.8530096295885015 for 5 qubits with 1 layers\n",
      "Step 50: Loss = -3.98158497345152 for 5 qubits with 1 layers\n",
      "Step 60: Loss = -3.9998987551552507 for 5 qubits with 1 layers\n",
      "Step 70: Loss = -3.9961511228747026 for 5 qubits with 1 layers\n",
      "Step 80: Loss = -3.9970425339879156 for 5 qubits with 1 layers\n",
      "Step 90: Loss = -3.999356657513052 for 5 qubits with 1 layers\n",
      "Step 100: Loss = -3.9999982768012163 for 5 qubits with 1 layers\n",
      "Step 110: Loss = -3.9999297104153047 for 5 qubits with 1 layers\n",
      "Step 120: Loss = -3.999951528859822 for 5 qubits with 1 layers\n",
      "Step 130: Loss = -3.99999604332878 for 5 qubits with 1 layers\n",
      "Step 140: Loss = -3.999998573485689 for 5 qubits with 1 layers\n",
      "Step 150: Loss = -3.9999977444394066 for 5 qubits with 1 layers\n",
      "Step 160: Loss = -3.9999997814818733 for 5 qubits with 1 layers\n",
      "Step 170: Loss = -3.9999999172271052 for 5 qubits with 1 layers\n",
      "Step 180: Loss = -3.999999898832958 for 5 qubits with 1 layers\n",
      "Step 190: Loss = -3.999999998066526 for 5 qubits with 1 layers\n",
      "Step 200: Loss = -3.999999990593827 for 5 qubits with 1 layers\n",
      "Step 210: Loss = -3.999999997291467 for 5 qubits with 1 layers\n",
      "Step 220: Loss = -3.999999999667093 for 5 qubits with 1 layers\n",
      "Step 230: Loss = -3.9999999994349213 for 5 qubits with 1 layers\n",
      "Step 240: Loss = -3.9999999999983293 for 5 qubits with 1 layers\n",
      "Step 250: Loss = -3.9999999999231317 for 5 qubits with 1 layers\n",
      "Step 260: Loss = -3.999999999999676 for 5 qubits with 1 layers\n",
      "Step 270: Loss = -3.9999999999903544 for 5 qubits with 1 layers\n",
      "Step 280: Loss = -3.9999999999999485 for 5 qubits with 1 layers\n",
      "Step 290: Loss = -3.999999999998736 for 5 qubits with 1 layers\n",
      "Step 300: Loss = -3.99999999999999 for 5 qubits with 1 layers\n",
      "Step 310: Loss = -3.999999999999848 for 5 qubits with 1 layers\n",
      "Step 320: Loss = -3.99999999999998 for 5 qubits with 1 layers\n",
      "Step 330: Loss = -3.9999999999999853 for 5 qubits with 1 layers\n",
      "Step 340: Loss = -3.9999999999999827 for 5 qubits with 1 layers\n",
      "Step 350: Loss = -3.999999999999986 for 5 qubits with 1 layers\n",
      "Step 360: Loss = -3.9999999999999902 for 5 qubits with 1 layers\n",
      "Step 370: Loss = -3.9999999999999876 for 5 qubits with 1 layers\n",
      "Step 380: Loss = -3.999999999999993 for 5 qubits with 1 layers\n",
      "Step 390: Loss = -3.999999999999987 for 5 qubits with 1 layers\n",
      "Step 400: Loss = -3.9999999999999885 for 5 qubits with 1 layers\n",
      "Step 410: Loss = -3.9999999999999876 for 5 qubits with 1 layers\n",
      "Step 420: Loss = -3.99999999999999 for 5 qubits with 1 layers\n",
      "Step 430: Loss = -3.9999999999999916 for 5 qubits with 1 layers\n",
      "Step 440: Loss = -3.999999999999984 for 5 qubits with 1 layers\n",
      "Step 450: Loss = -3.9999999999999862 for 5 qubits with 1 layers\n",
      "Step 460: Loss = -3.999999999999988 for 5 qubits with 1 layers\n",
      "Step 470: Loss = -3.999999999999989 for 5 qubits with 1 layers\n",
      "Step 480: Loss = -3.9999999999999885 for 5 qubits with 1 layers\n",
      "Step 490: Loss = -3.9999999999999925 for 5 qubits with 1 layers\n",
      "Running optimization for 5 qubits and 2 layers\n",
      "Step 0: Loss = -2.655474149075558 for 5 qubits with 2 layers\n",
      "Step 10: Loss = -3.617048710830706 for 5 qubits with 2 layers\n",
      "Step 20: Loss = -3.996301896537112 for 5 qubits with 2 layers\n",
      "Step 30: Loss = -3.950753838769927 for 5 qubits with 2 layers\n",
      "Step 40: Loss = -3.9918404772088336 for 5 qubits with 2 layers\n",
      "Step 50: Loss = -3.9964203315899383 for 5 qubits with 2 layers\n",
      "Step 60: Loss = -3.9982558455944863 for 5 qubits with 2 layers\n",
      "Step 70: Loss = -3.999548781250176 for 5 qubits with 2 layers\n",
      "Step 80: Loss = -3.9998179120315256 for 5 qubits with 2 layers\n",
      "Step 90: Loss = -3.9999004776055123 for 5 qubits with 2 layers\n",
      "Step 100: Loss = -3.9999954578963797 for 5 qubits with 2 layers\n",
      "Step 110: Loss = -3.999981663631544 for 5 qubits with 2 layers\n",
      "Step 120: Loss = -3.9999985780871716 for 5 qubits with 2 layers\n",
      "Step 130: Loss = -3.9999992574303933 for 5 qubits with 2 layers\n",
      "Step 140: Loss = -3.9999991505967607 for 5 qubits with 2 layers\n",
      "Step 150: Loss = -3.99999988691734 for 5 qubits with 2 layers\n",
      "Step 160: Loss = -3.9999999937820596 for 5 qubits with 2 layers\n",
      "Step 170: Loss = -3.999999973023554 for 5 qubits with 2 layers\n",
      "Step 180: Loss = -3.999999985728183 for 5 qubits with 2 layers\n",
      "Step 190: Loss = -3.999999996858045 for 5 qubits with 2 layers\n",
      "Step 200: Loss = -3.9999999997563886 for 5 qubits with 2 layers\n",
      "Step 210: Loss = -3.999999999988547 for 5 qubits with 2 layers\n",
      "Step 220: Loss = -3.999999999953777 for 5 qubits with 2 layers\n",
      "Step 230: Loss = -3.999999999964025 for 5 qubits with 2 layers\n",
      "Step 240: Loss = -3.99999999998053 for 5 qubits with 2 layers\n",
      "Step 250: Loss = -3.9999999999921316 for 5 qubits with 2 layers\n",
      "Step 260: Loss = -3.9999999999970086 for 5 qubits with 2 layers\n",
      "Step 270: Loss = -3.9999999999990035 for 5 qubits with 2 layers\n",
      "Step 280: Loss = -3.999999999999718 for 5 qubits with 2 layers\n",
      "Step 290: Loss = -3.9999999999999316 for 5 qubits with 2 layers\n",
      "Step 300: Loss = -3.9999999999999813 for 5 qubits with 2 layers\n",
      "Step 310: Loss = -3.9999999999999867 for 5 qubits with 2 layers\n",
      "Step 320: Loss = -3.999999999999988 for 5 qubits with 2 layers\n",
      "Step 330: Loss = -3.99999999999998 for 5 qubits with 2 layers\n",
      "Step 340: Loss = -3.999999999999986 for 5 qubits with 2 layers\n",
      "Step 350: Loss = -3.999999999999987 for 5 qubits with 2 layers\n",
      "Step 360: Loss = -3.9999999999999933 for 5 qubits with 2 layers\n",
      "Step 370: Loss = -3.9999999999999853 for 5 qubits with 2 layers\n",
      "Step 380: Loss = -3.9999999999999822 for 5 qubits with 2 layers\n",
      "Step 390: Loss = -3.9999999999999862 for 5 qubits with 2 layers\n",
      "Step 400: Loss = -3.9999999999999876 for 5 qubits with 2 layers\n",
      "Step 410: Loss = -3.999999999999996 for 5 qubits with 2 layers\n",
      "Step 420: Loss = -3.9999999999999876 for 5 qubits with 2 layers\n",
      "Step 430: Loss = -3.9999999999999876 for 5 qubits with 2 layers\n",
      "Step 440: Loss = -3.999999999999983 for 5 qubits with 2 layers\n",
      "Step 450: Loss = -3.9999999999999805 for 5 qubits with 2 layers\n",
      "Step 460: Loss = -3.9999999999999916 for 5 qubits with 2 layers\n",
      "Step 470: Loss = -3.999999999999985 for 5 qubits with 2 layers\n",
      "Step 480: Loss = -3.999999999999994 for 5 qubits with 2 layers\n",
      "Step 490: Loss = -3.9999999999999947 for 5 qubits with 2 layers\n",
      "Running optimization for 5 qubits and 3 layers\n",
      "Step 0: Loss = -0.4625705018263313 for 5 qubits with 3 layers\n",
      "Step 10: Loss = -1.5860461549297953 for 5 qubits with 3 layers\n",
      "Step 20: Loss = -2.6381394539333867 for 5 qubits with 3 layers\n",
      "Step 30: Loss = -3.549153959079097 for 5 qubits with 3 layers\n",
      "Step 40: Loss = -3.5876989548363825 for 5 qubits with 3 layers\n",
      "Step 50: Loss = -3.7536780002857157 for 5 qubits with 3 layers\n",
      "Step 60: Loss = -3.8738540862227397 for 5 qubits with 3 layers\n",
      "Step 70: Loss = -3.922136997309081 for 5 qubits with 3 layers\n",
      "Step 80: Loss = -3.950441973636217 for 5 qubits with 3 layers\n",
      "Step 90: Loss = -3.9678104813924584 for 5 qubits with 3 layers\n",
      "Step 100: Loss = -3.9773117214012426 for 5 qubits with 3 layers\n",
      "Step 110: Loss = -3.9828584549811192 for 5 qubits with 3 layers\n",
      "Step 120: Loss = -3.98648077707373 for 5 qubits with 3 layers\n",
      "Step 130: Loss = -3.989014386873939 for 5 qubits with 3 layers\n",
      "Step 140: Loss = -3.990870605148788 for 5 qubits with 3 layers\n",
      "Step 150: Loss = -3.9922928563803453 for 5 qubits with 3 layers\n",
      "Step 160: Loss = -3.993417076952653 for 5 qubits with 3 layers\n",
      "Step 170: Loss = -3.994324350847444 for 5 qubits with 3 layers\n",
      "Step 180: Loss = -3.9950685832136 for 5 qubits with 3 layers\n",
      "Step 190: Loss = -3.995687386394007 for 5 qubits with 3 layers\n",
      "Step 200: Loss = -3.996207524176018 for 5 qubits with 3 layers\n",
      "Step 210: Loss = -3.9966487717967967 for 5 qubits with 3 layers\n",
      "Step 220: Loss = -3.9970260807956923 for 5 qubits with 3 layers\n",
      "Step 230: Loss = -3.9973509581632416 for 5 qubits with 3 layers\n",
      "Step 240: Loss = -3.9976324058395845 for 5 qubits with 3 layers\n",
      "Step 250: Loss = -3.9978775583318016 for 5 qubits with 3 layers\n",
      "Step 260: Loss = -3.998092140721 for 5 qubits with 3 layers\n",
      "Step 270: Loss = -3.9982807939117713 for 5 qubits with 3 layers\n",
      "Step 280: Loss = -3.9984473148871813 for 5 qubits with 3 layers\n",
      "Step 290: Loss = -3.998594835854472 for 5 qubits with 3 layers\n",
      "Step 300: Loss = -3.998725959904291 for 5 qubits with 3 layers\n",
      "Step 310: Loss = -3.998842865226499 for 5 qubits with 3 layers\n",
      "Step 320: Loss = -3.998947385947606 for 5 qubits with 3 layers\n",
      "Step 330: Loss = -3.999041075438886 for 5 qubits with 3 layers\n",
      "Step 340: Loss = -3.999125256359096 for 5 qubits with 3 layers\n",
      "Step 350: Loss = -3.999201060531061 for 5 qubits with 3 layers\n",
      "Step 360: Loss = -3.999269460962519 for 5 qubits with 3 layers\n",
      "Step 370: Loss = -3.9993312977468536 for 5 qubits with 3 layers\n",
      "Step 380: Loss = -3.99938729915312 for 5 qubits with 3 layers\n",
      "Step 390: Loss = -3.999438098904511 for 5 qubits with 3 layers\n",
      "Step 400: Loss = -3.999484250415975 for 5 qubits with 3 layers\n",
      "Step 410: Loss = -3.999526238589691 for 5 qubits with 3 layers\n",
      "Step 420: Loss = -3.999564489636876 for 5 qubits with 3 layers\n",
      "Step 430: Loss = -3.9995993792959155 for 5 qubits with 3 layers\n",
      "Step 440: Loss = -3.99963123973962 for 5 qubits with 3 layers\n",
      "Step 450: Loss = -3.999660365405883 for 5 qubits with 3 layers\n",
      "Step 460: Loss = -3.9996870179395163 for 5 qubits with 3 layers\n",
      "Step 470: Loss = -3.9997114303971477 for 5 qubits with 3 layers\n",
      "Step 480: Loss = -3.9997338108380154 for 5 qubits with 3 layers\n",
      "Step 490: Loss = -3.9997543454014526 for 5 qubits with 3 layers\n",
      "Running optimization for 5 qubits and 4 layers\n",
      "Step 0: Loss = -1.7813045250162256 for 5 qubits with 4 layers\n",
      "Step 10: Loss = -3.4252377900297972 for 5 qubits with 4 layers\n",
      "Step 20: Loss = -3.724650858697583 for 5 qubits with 4 layers\n",
      "Step 30: Loss = -3.856583973042287 for 5 qubits with 4 layers\n",
      "Step 40: Loss = -3.860170568366746 for 5 qubits with 4 layers\n",
      "Step 50: Loss = -3.8796552674319735 for 5 qubits with 4 layers\n",
      "Step 60: Loss = -3.8859660400945657 for 5 qubits with 4 layers\n",
      "Step 70: Loss = -3.900031520884296 for 5 qubits with 4 layers\n",
      "Step 80: Loss = -3.9201188614040197 for 5 qubits with 4 layers\n",
      "Step 90: Loss = -3.9534005264510412 for 5 qubits with 4 layers\n",
      "Step 100: Loss = -3.9880103201772714 for 5 qubits with 4 layers\n",
      "Step 110: Loss = -3.9965145229422223 for 5 qubits with 4 layers\n",
      "Step 120: Loss = -3.997979003307222 for 5 qubits with 4 layers\n",
      "Step 130: Loss = -3.9995802574018113 for 5 qubits with 4 layers\n",
      "Step 140: Loss = -3.9996402795370924 for 5 qubits with 4 layers\n",
      "Step 150: Loss = -3.9997970533403007 for 5 qubits with 4 layers\n",
      "Step 160: Loss = -3.9998749031385663 for 5 qubits with 4 layers\n",
      "Step 170: Loss = -3.999898779835919 for 5 qubits with 4 layers\n",
      "Step 180: Loss = -3.9999242457647344 for 5 qubits with 4 layers\n",
      "Step 190: Loss = -3.9999378696061285 for 5 qubits with 4 layers\n",
      "Step 200: Loss = -3.999949176080642 for 5 qubits with 4 layers\n",
      "Step 210: Loss = -3.999957744392576 for 5 qubits with 4 layers\n",
      "Step 220: Loss = -3.999964575806157 for 5 qubits with 4 layers\n",
      "Step 230: Loss = -3.9999701038534488 for 5 qubits with 4 layers\n",
      "Step 240: Loss = -3.9999746174843636 for 5 qubits with 4 layers\n",
      "Step 250: Loss = -3.9999783433204428 for 5 qubits with 4 layers\n",
      "Step 260: Loss = -3.999981439634257 for 5 qubits with 4 layers\n",
      "Step 270: Loss = -3.999984029435299 for 5 qubits with 4 layers\n",
      "Step 280: Loss = -3.999986208523405 for 5 qubits with 4 layers\n",
      "Step 290: Loss = -3.999988051148919 for 5 qubits with 4 layers\n",
      "Step 300: Loss = -3.9999896166977376 for 5 qubits with 4 layers\n",
      "Step 310: Loss = -3.999990952437108 for 5 qubits with 4 layers\n",
      "Step 320: Loss = -3.9999920965586044 for 5 qubits with 4 layers\n",
      "Step 330: Loss = -3.999993080052832 for 5 qubits with 4 layers\n",
      "Step 340: Loss = -3.999993928256472 for 5 qubits with 4 layers\n",
      "Step 350: Loss = -3.9999946620081195 for 5 qubits with 4 layers\n",
      "Step 360: Loss = -3.9999952985387366 for 5 qubits with 4 layers\n",
      "Step 370: Loss = -3.9999958521720727 for 5 qubits with 4 layers\n",
      "Step 380: Loss = -3.9999963348720264 for 5 qubits with 4 layers\n",
      "Step 390: Loss = -3.999996756676061 for 5 qubits with 4 layers\n",
      "Step 400: Loss = -3.999997126040981 for 5 qubits with 4 layers\n",
      "Step 410: Loss = -3.999997450119765 for 5 qubits with 4 layers\n",
      "Step 420: Loss = -3.9999977349846643 for 5 qubits with 4 layers\n",
      "Step 430: Loss = -3.999997985808611 for 5 qubits with 4 layers\n",
      "Step 440: Loss = -3.999998207012452 for 5 qubits with 4 layers\n",
      "Step 450: Loss = -3.999998402385991 for 5 qubits with 4 layers\n",
      "Step 460: Loss = -3.999998575187435 for 5 qubits with 4 layers\n",
      "Step 470: Loss = -3.999998728225508 for 5 qubits with 4 layers\n",
      "Step 480: Loss = -3.9999988639277033 for 5 qubits with 4 layers\n",
      "Step 490: Loss = -3.9999989843970827 for 5 qubits with 4 layers\n",
      "Running optimization for 5 qubits and 5 layers\n",
      "Step 0: Loss = -0.9542354852039028 for 5 qubits with 5 layers\n",
      "Step 10: Loss = -2.4132309492151887 for 5 qubits with 5 layers\n",
      "Step 20: Loss = -3.076615682630683 for 5 qubits with 5 layers\n",
      "Step 30: Loss = -3.2753161204913877 for 5 qubits with 5 layers\n",
      "Step 40: Loss = -3.382879689657993 for 5 qubits with 5 layers\n",
      "Step 50: Loss = -3.586657744341899 for 5 qubits with 5 layers\n",
      "Step 60: Loss = -3.6583202159504284 for 5 qubits with 5 layers\n",
      "Step 70: Loss = -3.7274952786555247 for 5 qubits with 5 layers\n",
      "Step 80: Loss = -3.8057712158572032 for 5 qubits with 5 layers\n",
      "Step 90: Loss = -3.8797178625187017 for 5 qubits with 5 layers\n",
      "Step 100: Loss = -3.9382367856569087 for 5 qubits with 5 layers\n",
      "Step 110: Loss = -3.97564585912499 for 5 qubits with 5 layers\n",
      "Step 120: Loss = -3.993448920798463 for 5 qubits with 5 layers\n",
      "Step 130: Loss = -3.9988077492407017 for 5 qubits with 5 layers\n",
      "Step 140: Loss = -3.9993986677024806 for 5 qubits with 5 layers\n",
      "Step 150: Loss = -3.9993842401852415 for 5 qubits with 5 layers\n",
      "Step 160: Loss = -3.999465044981963 for 5 qubits with 5 layers\n",
      "Step 170: Loss = -3.999590978189677 for 5 qubits with 5 layers\n",
      "Step 180: Loss = -3.9996612926765405 for 5 qubits with 5 layers\n",
      "Step 190: Loss = -3.999700248975693 for 5 qubits with 5 layers\n",
      "Step 200: Loss = -3.9997346241699763 for 5 qubits with 5 layers\n",
      "Step 210: Loss = -3.9997690753451476 for 5 qubits with 5 layers\n",
      "Step 220: Loss = -3.9998014181599237 for 5 qubits with 5 layers\n",
      "Step 230: Loss = -3.9998310260757264 for 5 qubits with 5 layers\n",
      "Step 240: Loss = -3.999858015181707 for 5 qubits with 5 layers\n",
      "Step 250: Loss = -3.9998822891948436 for 5 qubits with 5 layers\n",
      "Step 260: Loss = -3.9999037286396053 for 5 qubits with 5 layers\n",
      "Step 270: Loss = -3.999922365916931 for 5 qubits with 5 layers\n",
      "Step 280: Loss = -3.9999383117355887 for 5 qubits with 5 layers\n",
      "Step 290: Loss = -3.9999517242973313 for 5 qubits with 5 layers\n",
      "Step 300: Loss = -3.999962810625389 for 5 qubits with 5 layers\n",
      "Step 310: Loss = -3.999971814610209 for 5 qubits with 5 layers\n",
      "Step 320: Loss = -3.9999789984409557 for 5 qubits with 5 layers\n",
      "Step 330: Loss = -3.9999846272199213 for 5 qubits with 5 layers\n",
      "Step 340: Loss = -3.9999889563314195 for 5 qubits with 5 layers\n",
      "Step 350: Loss = -3.999992222278504 for 5 qubits with 5 layers\n",
      "Step 360: Loss = -3.999994637043763 for 5 qubits with 5 layers\n",
      "Step 370: Loss = -3.9999963850252813 for 5 qubits with 5 layers\n",
      "Step 380: Loss = -3.9999976221833924 for 5 qubits with 5 layers\n",
      "Step 390: Loss = -3.9999984769444574 for 5 qubits with 5 layers\n",
      "Step 400: Loss = -3.999999052329377 for 5 qubits with 5 layers\n",
      "Step 410: Loss = -3.9999994288353937 for 5 qubits with 5 layers\n",
      "Step 420: Loss = -3.9999996676751297 for 5 qubits with 5 layers\n",
      "Step 430: Loss = -3.9999998140830275 for 5 qubits with 5 layers\n",
      "Step 440: Loss = -3.9999999004760336 for 5 qubits with 5 layers\n",
      "Step 450: Loss = -3.9999999493234846 for 5 qubits with 5 layers\n",
      "Step 460: Loss = -3.9999999756383477 for 5 qubits with 5 layers\n",
      "Step 470: Loss = -3.999999989050183 for 5 qubits with 5 layers\n",
      "Step 480: Loss = -3.999999995458626 for 5 qubits with 5 layers\n",
      "Step 490: Loss = -3.9999999982945225 for 5 qubits with 5 layers\n",
      "Running optimization for 5 qubits and 6 layers\n",
      "Step 0: Loss = -1.4977908220244396 for 5 qubits with 6 layers\n",
      "Step 10: Loss = -3.367310313957433 for 5 qubits with 6 layers\n",
      "Step 20: Loss = -3.869265678116257 for 5 qubits with 6 layers\n",
      "Step 30: Loss = -3.89966821279876 for 5 qubits with 6 layers\n",
      "Step 40: Loss = -3.982192794793811 for 5 qubits with 6 layers\n",
      "Step 50: Loss = -3.988743415404228 for 5 qubits with 6 layers\n",
      "Step 60: Loss = -3.996723143971494 for 5 qubits with 6 layers\n",
      "Step 70: Loss = -3.999343370007266 for 5 qubits with 6 layers\n",
      "Step 80: Loss = -3.9996077914341956 for 5 qubits with 6 layers\n",
      "Step 90: Loss = -3.999746109722878 for 5 qubits with 6 layers\n",
      "Step 100: Loss = -3.9999090024819504 for 5 qubits with 6 layers\n",
      "Step 110: Loss = -3.999978333160831 for 5 qubits with 6 layers\n",
      "Step 120: Loss = -3.999989194913752 for 5 qubits with 6 layers\n",
      "Step 130: Loss = -3.9999954991244446 for 5 qubits with 6 layers\n",
      "Step 140: Loss = -3.9999982769563287 for 5 qubits with 6 layers\n",
      "Step 150: Loss = -3.9999994171060944 for 5 qubits with 6 layers\n",
      "Step 160: Loss = -3.9999997758604002 for 5 qubits with 6 layers\n",
      "Step 170: Loss = -3.9999999530816925 for 5 qubits with 6 layers\n",
      "Step 180: Loss = -3.9999999883652055 for 5 qubits with 6 layers\n",
      "Step 190: Loss = -3.9999999943205005 for 5 qubits with 6 layers\n",
      "Step 200: Loss = -3.9999999978846548 for 5 qubits with 6 layers\n",
      "Step 210: Loss = -3.999999998662984 for 5 qubits with 6 layers\n",
      "Step 220: Loss = -3.9999999995239968 for 5 qubits with 6 layers\n",
      "Step 230: Loss = -3.9999999998952016 for 5 qubits with 6 layers\n",
      "Step 240: Loss = -3.999999999973639 for 5 qubits with 6 layers\n",
      "Step 250: Loss = -3.9999999999869074 for 5 qubits with 6 layers\n",
      "Step 260: Loss = -3.9999999999939284 for 5 qubits with 6 layers\n",
      "Step 270: Loss = -3.9999999999970086 for 5 qubits with 6 layers\n",
      "Step 280: Loss = -3.9999999999990377 for 5 qubits with 6 layers\n",
      "Step 290: Loss = -3.99999999999978 for 5 qubits with 6 layers\n",
      "Step 300: Loss = -3.9999999999998384 for 5 qubits with 6 layers\n",
      "Step 310: Loss = -3.9999999999999454 for 5 qubits with 6 layers\n",
      "Step 320: Loss = -3.999999999999962 for 5 qubits with 6 layers\n",
      "Step 330: Loss = -3.999999999999954 for 5 qubits with 6 layers\n",
      "Step 340: Loss = -3.999999999999953 for 5 qubits with 6 layers\n",
      "Step 350: Loss = -3.999999999999968 for 5 qubits with 6 layers\n",
      "Step 360: Loss = -3.9999999999999485 for 5 qubits with 6 layers\n",
      "Step 370: Loss = -3.999999999999968 for 5 qubits with 6 layers\n",
      "Step 380: Loss = -3.9999999999999587 for 5 qubits with 6 layers\n",
      "Step 390: Loss = -3.9999999999999534 for 5 qubits with 6 layers\n",
      "Step 400: Loss = -3.9999999999999645 for 5 qubits with 6 layers\n",
      "Step 410: Loss = -3.999999999999962 for 5 qubits with 6 layers\n",
      "Step 420: Loss = -3.9999999999999485 for 5 qubits with 6 layers\n",
      "Step 430: Loss = -3.9999999999999485 for 5 qubits with 6 layers\n",
      "Step 440: Loss = -3.999999999999954 for 5 qubits with 6 layers\n",
      "Step 450: Loss = -3.9999999999999445 for 5 qubits with 6 layers\n",
      "Step 460: Loss = -3.999999999999962 for 5 qubits with 6 layers\n",
      "Step 470: Loss = -3.9999999999999414 for 5 qubits with 6 layers\n",
      "Step 480: Loss = -3.9999999999999636 for 5 qubits with 6 layers\n",
      "Step 490: Loss = -3.999999999999946 for 5 qubits with 6 layers\n",
      "Running optimization for 5 qubits and 7 layers\n",
      "Step 0: Loss = -2.879967190871098 for 5 qubits with 7 layers\n",
      "Step 10: Loss = -3.8429929992261487 for 5 qubits with 7 layers\n",
      "Step 20: Loss = -3.962070229141974 for 5 qubits with 7 layers\n",
      "Step 30: Loss = -3.9763514828542688 for 5 qubits with 7 layers\n",
      "Step 40: Loss = -3.996524660827811 for 5 qubits with 7 layers\n",
      "Step 50: Loss = -3.9971675459726534 for 5 qubits with 7 layers\n",
      "Step 60: Loss = -3.9983811621298226 for 5 qubits with 7 layers\n",
      "Step 70: Loss = -3.999224986408505 for 5 qubits with 7 layers\n",
      "Step 80: Loss = -3.9994942825449638 for 5 qubits with 7 layers\n",
      "Step 90: Loss = -3.999671828317623 for 5 qubits with 7 layers\n",
      "Step 100: Loss = -3.999786102989127 for 5 qubits with 7 layers\n",
      "Step 110: Loss = -3.9998584018376384 for 5 qubits with 7 layers\n",
      "Step 120: Loss = -3.999906049964716 for 5 qubits with 7 layers\n",
      "Step 130: Loss = -3.999938789802183 for 5 qubits with 7 layers\n",
      "Step 140: Loss = -3.999961020104319 for 5 qubits with 7 layers\n",
      "Step 150: Loss = -3.999975848209897 for 5 qubits with 7 layers\n",
      "Step 160: Loss = -3.999985548591663 for 5 qubits with 7 layers\n",
      "Step 170: Loss = -3.999991691286124 for 5 qubits with 7 layers\n",
      "Step 180: Loss = -3.999995423902993 for 5 qubits with 7 layers\n",
      "Step 190: Loss = -3.999997602950149 for 5 qubits with 7 layers\n",
      "Step 200: Loss = -3.999998814761785 for 5 qubits with 7 layers\n",
      "Step 210: Loss = -3.999999452138367 for 5 qubits with 7 layers\n",
      "Step 220: Loss = -3.999999766350606 for 5 qubits with 7 layers\n",
      "Step 230: Loss = -3.999999909790785 for 5 qubits with 7 layers\n",
      "Step 240: Loss = -3.999999969356169 for 5 qubits with 7 layers\n",
      "Step 250: Loss = -3.9999999912756303 for 5 qubits with 7 layers\n",
      "Step 260: Loss = -3.999999998117832 for 5 qubits with 7 layers\n",
      "Step 270: Loss = -3.9999999997682765 for 5 qubits with 7 layers\n",
      "Step 280: Loss = -3.9999999999981988 for 5 qubits with 7 layers\n",
      "Step 290: Loss = -3.999999999982537 for 5 qubits with 7 layers\n",
      "Step 300: Loss = -3.999999999974391 for 5 qubits with 7 layers\n",
      "Step 310: Loss = -3.9999999999841496 for 5 qubits with 7 layers\n",
      "Step 320: Loss = -3.999999999992795 for 5 qubits with 7 layers\n",
      "Step 330: Loss = -3.9999996025359414 for 5 qubits with 7 layers\n",
      "Step 340: Loss = -3.9982404365740365 for 5 qubits with 7 layers\n",
      "Step 350: Loss = -3.9997809002806326 for 5 qubits with 7 layers\n",
      "Step 360: Loss = -3.9999871438299612 for 5 qubits with 7 layers\n",
      "Step 370: Loss = -3.9999399959995414 for 5 qubits with 7 layers\n",
      "Step 380: Loss = -3.9999866776702717 for 5 qubits with 7 layers\n",
      "Step 390: Loss = -3.9999866887397926 for 5 qubits with 7 layers\n",
      "Step 400: Loss = -3.9999947448292756 for 5 qubits with 7 layers\n",
      "Step 410: Loss = -3.9999993831538703 for 5 qubits with 7 layers\n",
      "Step 420: Loss = -3.999999399350884 for 5 qubits with 7 layers\n",
      "Step 430: Loss = -3.9999999265888517 for 5 qubits with 7 layers\n",
      "Step 440: Loss = -3.9999999227921856 for 5 qubits with 7 layers\n",
      "Step 450: Loss = -3.9999870709957057 for 5 qubits with 7 layers\n",
      "Step 460: Loss = -3.999825675700254 for 5 qubits with 7 layers\n",
      "Step 470: Loss = -3.99994522736149 for 5 qubits with 7 layers\n",
      "Step 480: Loss = -3.99992628266928 for 5 qubits with 7 layers\n",
      "Step 490: Loss = -3.999930890895243 for 5 qubits with 7 layers\n",
      "Running optimization for 5 qubits and 8 layers\n",
      "Step 0: Loss = -1.523956259904089 for 5 qubits with 8 layers\n",
      "Step 10: Loss = -3.843016658382612 for 5 qubits with 8 layers\n",
      "Step 20: Loss = -3.9006668640442093 for 5 qubits with 8 layers\n",
      "Step 30: Loss = -3.9604374839117766 for 5 qubits with 8 layers\n",
      "Step 40: Loss = -3.9926549192976175 for 5 qubits with 8 layers\n",
      "Step 50: Loss = -3.997002684817108 for 5 qubits with 8 layers\n",
      "Step 60: Loss = -3.998311224638555 for 5 qubits with 8 layers\n",
      "Step 70: Loss = -3.9996265037482353 for 5 qubits with 8 layers\n",
      "Step 80: Loss = -3.9996350218898273 for 5 qubits with 8 layers\n",
      "Step 90: Loss = -3.9998972770469914 for 5 qubits with 8 layers\n",
      "Step 100: Loss = -3.9999633052779204 for 5 qubits with 8 layers\n",
      "Step 110: Loss = -3.9999878868108603 for 5 qubits with 8 layers\n",
      "Step 120: Loss = -3.99999644995173 for 5 qubits with 8 layers\n",
      "Step 130: Loss = -3.9999989865284995 for 5 qubits with 8 layers\n",
      "Step 140: Loss = -3.9999995242953714 for 5 qubits with 8 layers\n",
      "Step 150: Loss = -3.999999800583268 for 5 qubits with 8 layers\n",
      "Step 160: Loss = -3.999999917168518 for 5 qubits with 8 layers\n",
      "Step 170: Loss = -3.99999997623438 for 5 qubits with 8 layers\n",
      "Step 180: Loss = -3.9999999952820273 for 5 qubits with 8 layers\n",
      "Step 190: Loss = -3.9999999976850473 for 5 qubits with 8 layers\n",
      "Step 200: Loss = -3.9999999991161155 for 5 qubits with 8 layers\n",
      "Step 210: Loss = -3.9999999996626743 for 5 qubits with 8 layers\n",
      "Step 220: Loss = -3.999999999837138 for 5 qubits with 8 layers\n",
      "Step 230: Loss = -3.999999999957349 for 5 qubits with 8 layers\n",
      "Step 240: Loss = -3.999999999988626 for 5 qubits with 8 layers\n",
      "Step 250: Loss = -3.9999999999946785 for 5 qubits with 8 layers\n",
      "Step 260: Loss = -3.999999999998548 for 5 qubits with 8 layers\n",
      "Step 270: Loss = -3.9999999999990545 for 5 qubits with 8 layers\n",
      "Step 280: Loss = -3.9999999999995715 for 5 qubits with 8 layers\n",
      "Step 290: Loss = -3.9999999999997726 for 5 qubits with 8 layers\n",
      "Step 300: Loss = -3.999999999984005 for 5 qubits with 8 layers\n",
      "Step 310: Loss = -3.9999977939569598 for 5 qubits with 8 layers\n",
      "Step 320: Loss = -3.9999877969070097 for 5 qubits with 8 layers\n",
      "Step 330: Loss = -3.999873929609853 for 5 qubits with 8 layers\n",
      "Step 340: Loss = -3.9999267170853425 for 5 qubits with 8 layers\n",
      "Step 350: Loss = -3.9999882315893367 for 5 qubits with 8 layers\n",
      "Step 360: Loss = -3.9999930041250082 for 5 qubits with 8 layers\n",
      "Step 370: Loss = -3.9999996814664316 for 5 qubits with 8 layers\n",
      "Step 380: Loss = -3.999999514231938 for 5 qubits with 8 layers\n",
      "Step 390: Loss = -3.9999994295500203 for 5 qubits with 8 layers\n",
      "Step 400: Loss = -3.9999997003201706 for 5 qubits with 8 layers\n",
      "Step 410: Loss = -3.999999986075167 for 5 qubits with 8 layers\n",
      "Step 420: Loss = -3.9999999716210968 for 5 qubits with 8 layers\n",
      "Step 430: Loss = -3.999998307875006 for 5 qubits with 8 layers\n",
      "Step 440: Loss = -3.9999957438205618 for 5 qubits with 8 layers\n",
      "Step 450: Loss = -3.9999966271424916 for 5 qubits with 8 layers\n",
      "Step 460: Loss = -3.9999762558240493 for 5 qubits with 8 layers\n",
      "Step 470: Loss = -3.9999947008291157 for 5 qubits with 8 layers\n",
      "Step 480: Loss = -3.9999920632000836 for 5 qubits with 8 layers\n",
      "Step 490: Loss = -3.9999996934253836 for 5 qubits with 8 layers\n",
      "Running optimization for 5 qubits and 9 layers\n",
      "Step 0: Loss = -1.2301964952018387 for 5 qubits with 9 layers\n",
      "Step 10: Loss = -3.452071263949108 for 5 qubits with 9 layers\n",
      "Step 20: Loss = -3.9110381318422025 for 5 qubits with 9 layers\n",
      "Step 30: Loss = -3.9738649402596993 for 5 qubits with 9 layers\n",
      "Step 40: Loss = -3.9893550746437962 for 5 qubits with 9 layers\n",
      "Step 50: Loss = -3.9940910995621848 for 5 qubits with 9 layers\n",
      "Step 60: Loss = -3.997759676954422 for 5 qubits with 9 layers\n",
      "Step 70: Loss = -3.9993727262204124 for 5 qubits with 9 layers\n",
      "Step 80: Loss = -3.9997037098857486 for 5 qubits with 9 layers\n",
      "Step 90: Loss = -3.999912760779785 for 5 qubits with 9 layers\n",
      "Step 100: Loss = -3.999976111720052 for 5 qubits with 9 layers\n",
      "Step 110: Loss = -3.9999812897699187 for 5 qubits with 9 layers\n",
      "Step 120: Loss = -3.9999943945810896 for 5 qubits with 9 layers\n",
      "Step 130: Loss = -3.9999980331347755 for 5 qubits with 9 layers\n",
      "Step 140: Loss = -3.9999995408238753 for 5 qubits with 9 layers\n",
      "Step 150: Loss = -3.999999845939928 for 5 qubits with 9 layers\n",
      "Step 160: Loss = -3.9999999359784275 for 5 qubits with 9 layers\n",
      "Step 170: Loss = -3.999999968844385 for 5 qubits with 9 layers\n",
      "Step 180: Loss = -3.9999999920660585 for 5 qubits with 9 layers\n",
      "Step 190: Loss = -3.9999999970650517 for 5 qubits with 9 layers\n",
      "Step 200: Loss = -3.9999999995263043 for 5 qubits with 9 layers\n",
      "Step 210: Loss = -3.999999999529744 for 5 qubits with 9 layers\n",
      "Step 220: Loss = -3.999999999899212 for 5 qubits with 9 layers\n",
      "Step 230: Loss = -3.999999999930353 for 5 qubits with 9 layers\n",
      "Step 240: Loss = -3.999999999975352 for 5 qubits with 9 layers\n",
      "Step 250: Loss = -3.9999999999944844 for 5 qubits with 9 layers\n",
      "Step 260: Loss = -3.999999999998188 for 5 qubits with 9 layers\n",
      "Step 270: Loss = -3.999999999998762 for 5 qubits with 9 layers\n",
      "Step 280: Loss = -3.999999999999439 for 5 qubits with 9 layers\n",
      "Step 290: Loss = -3.9999999999998312 for 5 qubits with 9 layers\n",
      "Step 300: Loss = -3.9999999999998996 for 5 qubits with 9 layers\n",
      "Step 310: Loss = -3.9999999999999334 for 5 qubits with 9 layers\n",
      "Step 320: Loss = -3.9999999999999294 for 5 qubits with 9 layers\n",
      "Step 330: Loss = -3.999999999999936 for 5 qubits with 9 layers\n",
      "Step 340: Loss = -3.999999999999928 for 5 qubits with 9 layers\n",
      "Step 350: Loss = -3.999999999999929 for 5 qubits with 9 layers\n",
      "Step 360: Loss = -3.999999999999938 for 5 qubits with 9 layers\n",
      "Step 370: Loss = -3.999999999999917 for 5 qubits with 9 layers\n",
      "Step 380: Loss = -3.9999999999999316 for 5 qubits with 9 layers\n",
      "Step 390: Loss = -3.9999999999999294 for 5 qubits with 9 layers\n",
      "Step 400: Loss = -3.999999999999944 for 5 qubits with 9 layers\n",
      "Step 410: Loss = -3.9999999999999387 for 5 qubits with 9 layers\n",
      "Step 420: Loss = -3.9999999999999396 for 5 qubits with 9 layers\n",
      "Step 430: Loss = -3.9999999999999534 for 5 qubits with 9 layers\n",
      "Step 440: Loss = -3.999999999998052 for 5 qubits with 9 layers\n",
      "Step 450: Loss = -3.9999983942071076 for 5 qubits with 9 layers\n",
      "Step 460: Loss = -3.9950348291983593 for 5 qubits with 9 layers\n",
      "Step 470: Loss = -3.9997200266692614 for 5 qubits with 9 layers\n",
      "Step 480: Loss = -3.999412053225246 for 5 qubits with 9 layers\n",
      "Step 490: Loss = -3.9997922043289518 for 5 qubits with 9 layers\n",
      "Running optimization for 5 qubits and 10 layers\n",
      "Step 0: Loss = -1.0320109085695917 for 5 qubits with 10 layers\n",
      "Step 10: Loss = -3.6167263588141707 for 5 qubits with 10 layers\n",
      "Step 20: Loss = -3.9443552510483824 for 5 qubits with 10 layers\n",
      "Step 30: Loss = -3.9765649797516343 for 5 qubits with 10 layers\n",
      "Step 40: Loss = -3.9869204667313887 for 5 qubits with 10 layers\n",
      "Step 50: Loss = -3.992206254364617 for 5 qubits with 10 layers\n",
      "Step 60: Loss = -3.997346799420102 for 5 qubits with 10 layers\n",
      "Step 70: Loss = -3.9992396313087526 for 5 qubits with 10 layers\n",
      "Step 80: Loss = -3.9996841576985016 for 5 qubits with 10 layers\n",
      "Step 90: Loss = -3.9998999764724954 for 5 qubits with 10 layers\n",
      "Step 100: Loss = -3.999949765722966 for 5 qubits with 10 layers\n",
      "Step 110: Loss = -3.9999848974937526 for 5 qubits with 10 layers\n",
      "Step 120: Loss = -3.999995413496235 for 5 qubits with 10 layers\n",
      "Step 130: Loss = -3.9999992592458122 for 5 qubits with 10 layers\n",
      "Step 140: Loss = -3.999999550827303 for 5 qubits with 10 layers\n",
      "Step 150: Loss = -3.999999777015111 for 5 qubits with 10 layers\n",
      "Step 160: Loss = -3.999999980951853 for 5 qubits with 10 layers\n",
      "Step 170: Loss = -3.9999999722638924 for 5 qubits with 10 layers\n",
      "Step 180: Loss = -3.999999993708723 for 5 qubits with 10 layers\n",
      "Step 190: Loss = -3.9999999963944024 for 5 qubits with 10 layers\n",
      "Step 200: Loss = -3.9999999991139767 for 5 qubits with 10 layers\n",
      "Step 210: Loss = -3.9999999993582267 for 5 qubits with 10 layers\n",
      "Step 220: Loss = -3.999999999953711 for 5 qubits with 10 layers\n",
      "Step 230: Loss = -3.999999999934865 for 5 qubits with 10 layers\n",
      "Step 240: Loss = -3.9999999999669393 for 5 qubits with 10 layers\n",
      "Step 250: Loss = -3.999999999992557 for 5 qubits with 10 layers\n",
      "Step 260: Loss = -3.999999999998459 for 5 qubits with 10 layers\n",
      "Step 270: Loss = -3.999999999999331 for 5 qubits with 10 layers\n",
      "Step 280: Loss = -3.999999999999704 for 5 qubits with 10 layers\n",
      "Step 290: Loss = -3.9999999999998432 for 5 qubits with 10 layers\n",
      "Step 300: Loss = -3.99999999999992 for 5 qubits with 10 layers\n",
      "Step 310: Loss = -3.999999999999919 for 5 qubits with 10 layers\n",
      "Step 320: Loss = -3.999999999999904 for 5 qubits with 10 layers\n",
      "Step 330: Loss = -3.9999999999999325 for 5 qubits with 10 layers\n",
      "Step 340: Loss = -3.999999999999908 for 5 qubits with 10 layers\n",
      "Step 350: Loss = -3.9999999999999316 for 5 qubits with 10 layers\n",
      "Step 360: Loss = -3.9999999999999156 for 5 qubits with 10 layers\n",
      "Step 370: Loss = -3.999999999999911 for 5 qubits with 10 layers\n",
      "Step 380: Loss = -3.9999999999999076 for 5 qubits with 10 layers\n",
      "Step 390: Loss = -3.9999999999999267 for 5 qubits with 10 layers\n",
      "Step 400: Loss = -3.9999999999999103 for 5 qubits with 10 layers\n",
      "Step 410: Loss = -3.999999999999937 for 5 qubits with 10 layers\n",
      "Step 420: Loss = -3.9999999999999387 for 5 qubits with 10 layers\n",
      "Step 430: Loss = -3.9999999999999227 for 5 qubits with 10 layers\n",
      "Step 440: Loss = -3.9999999999999263 for 5 qubits with 10 layers\n",
      "Step 450: Loss = -3.9999999999999196 for 5 qubits with 10 layers\n",
      "Step 460: Loss = -3.999999999999938 for 5 qubits with 10 layers\n",
      "Step 470: Loss = -3.99999999999994 for 5 qubits with 10 layers\n",
      "Step 480: Loss = -3.9999999999999503 for 5 qubits with 10 layers\n",
      "Step 490: Loss = -3.99999999999992 for 5 qubits with 10 layers\n",
      "Running optimization for 5 qubits and 11 layers\n",
      "Step 0: Loss = -0.7592294997547095 for 5 qubits with 11 layers\n",
      "Step 10: Loss = -3.2168757429705215 for 5 qubits with 11 layers\n",
      "Step 20: Loss = -3.948388557208034 for 5 qubits with 11 layers\n",
      "Step 30: Loss = -3.9399409962534486 for 5 qubits with 11 layers\n",
      "Step 40: Loss = -3.985226968269269 for 5 qubits with 11 layers\n",
      "Step 50: Loss = -3.991663262246756 for 5 qubits with 11 layers\n",
      "Step 60: Loss = -3.9985188498231796 for 5 qubits with 11 layers\n",
      "Step 70: Loss = -3.999016935474578 for 5 qubits with 11 layers\n",
      "Step 80: Loss = -3.9995953635647705 for 5 qubits with 11 layers\n",
      "Step 90: Loss = -3.999961861674672 for 5 qubits with 11 layers\n",
      "Step 100: Loss = -3.9999618196685622 for 5 qubits with 11 layers\n",
      "Step 110: Loss = -3.9999758839209414 for 5 qubits with 11 layers\n",
      "Step 120: Loss = -3.999994551635564 for 5 qubits with 11 layers\n",
      "Step 130: Loss = -3.9999986307400732 for 5 qubits with 11 layers\n",
      "Step 140: Loss = -3.999999599969059 for 5 qubits with 11 layers\n",
      "Step 150: Loss = -3.999999630456604 for 5 qubits with 11 layers\n",
      "Step 160: Loss = -3.9999998859385424 for 5 qubits with 11 layers\n",
      "Step 170: Loss = -3.9999999578529812 for 5 qubits with 11 layers\n",
      "Step 180: Loss = -3.9999999826095447 for 5 qubits with 11 layers\n",
      "Step 190: Loss = -3.9999999942686117 for 5 qubits with 11 layers\n",
      "Step 200: Loss = -3.9999999975605194 for 5 qubits with 11 layers\n",
      "Step 210: Loss = -3.9999999991867354 for 5 qubits with 11 layers\n",
      "Step 220: Loss = -3.9999999997423443 for 5 qubits with 11 layers\n",
      "Step 230: Loss = -3.999999999896197 for 5 qubits with 11 layers\n",
      "Step 240: Loss = -3.999999999971731 for 5 qubits with 11 layers\n",
      "Step 250: Loss = -3.99999999999433 for 5 qubits with 11 layers\n",
      "Step 260: Loss = -3.9999999999989715 for 5 qubits with 11 layers\n",
      "Step 270: Loss = -3.9999999999984377 for 5 qubits with 11 layers\n",
      "Step 280: Loss = -3.9999999999994573 for 5 qubits with 11 layers\n",
      "Step 290: Loss = -3.999999999999842 for 5 qubits with 11 layers\n",
      "Step 300: Loss = -3.999999999999864 for 5 qubits with 11 layers\n",
      "Step 310: Loss = -3.9999999999998828 for 5 qubits with 11 layers\n",
      "Step 320: Loss = -3.999999999999912 for 5 qubits with 11 layers\n",
      "Step 330: Loss = -3.9999999999999134 for 5 qubits with 11 layers\n",
      "Step 340: Loss = -3.9999999999999325 for 5 qubits with 11 layers\n",
      "Step 350: Loss = -3.999999999999913 for 5 qubits with 11 layers\n",
      "Step 360: Loss = -3.999999999999733 for 5 qubits with 11 layers\n",
      "Step 370: Loss = -3.9999999847145578 for 5 qubits with 11 layers\n",
      "Step 380: Loss = -3.9838305436624672 for 5 qubits with 11 layers\n",
      "Step 390: Loss = -3.999676595884244 for 5 qubits with 11 layers\n",
      "Step 400: Loss = -3.9976228576384805 for 5 qubits with 11 layers\n",
      "Step 410: Loss = -3.999879228741678 for 5 qubits with 11 layers\n",
      "Step 420: Loss = -3.999919130020306 for 5 qubits with 11 layers\n",
      "Step 430: Loss = -3.99992900176513 for 5 qubits with 11 layers\n",
      "Step 440: Loss = -3.99998651619252 for 5 qubits with 11 layers\n",
      "Step 450: Loss = -3.999996719856438 for 5 qubits with 11 layers\n",
      "Step 460: Loss = -3.999996299876674 for 5 qubits with 11 layers\n",
      "Step 470: Loss = -3.999997678675303 for 5 qubits with 11 layers\n",
      "Step 480: Loss = -3.9999996183730304 for 5 qubits with 11 layers\n",
      "Step 490: Loss = -3.9999996562179234 for 5 qubits with 11 layers\n",
      "Running optimization for 5 qubits and 12 layers\n",
      "Step 0: Loss = -1.0532071385053123 for 5 qubits with 12 layers\n",
      "Step 10: Loss = -3.599693029204105 for 5 qubits with 12 layers\n",
      "Step 20: Loss = -3.751585167860243 for 5 qubits with 12 layers\n",
      "Step 30: Loss = -3.9178495310211625 for 5 qubits with 12 layers\n",
      "Step 40: Loss = -3.9813950293295677 for 5 qubits with 12 layers\n",
      "Step 50: Loss = -3.9960857975480515 for 5 qubits with 12 layers\n",
      "Step 60: Loss = -3.9979721348277195 for 5 qubits with 12 layers\n",
      "Step 70: Loss = -3.999072386747056 for 5 qubits with 12 layers\n",
      "Step 80: Loss = -3.999701612244377 for 5 qubits with 12 layers\n",
      "Step 90: Loss = -3.999837507102552 for 5 qubits with 12 layers\n",
      "Step 100: Loss = -3.9999457983186106 for 5 qubits with 12 layers\n",
      "Step 110: Loss = -3.9999743898410878 for 5 qubits with 12 layers\n",
      "Step 120: Loss = -3.9999943359524224 for 5 qubits with 12 layers\n",
      "Step 130: Loss = -3.999998285826027 for 5 qubits with 12 layers\n",
      "Step 140: Loss = -3.9999992490150706 for 5 qubits with 12 layers\n",
      "Step 150: Loss = -3.999999774925873 for 5 qubits with 12 layers\n",
      "Step 160: Loss = -3.999999912405802 for 5 qubits with 12 layers\n",
      "Step 170: Loss = -3.999999938553569 for 5 qubits with 12 layers\n",
      "Step 180: Loss = -3.999999988021941 for 5 qubits with 12 layers\n",
      "Step 190: Loss = -3.9999999954525283 for 5 qubits with 12 layers\n",
      "Step 200: Loss = -3.999999997757277 for 5 qubits with 12 layers\n",
      "Step 210: Loss = -3.9999999995474163 for 5 qubits with 12 layers\n",
      "Step 220: Loss = -3.99999999964435 for 5 qubits with 12 layers\n",
      "Step 230: Loss = -3.9999999999376743 for 5 qubits with 12 layers\n",
      "Step 240: Loss = -3.9999999999726366 for 5 qubits with 12 layers\n",
      "Step 250: Loss = -3.9999999999909392 for 5 qubits with 12 layers\n",
      "Step 260: Loss = -3.9999999999976303 for 5 qubits with 12 layers\n",
      "Step 270: Loss = -3.999999999998456 for 5 qubits with 12 layers\n",
      "Step 280: Loss = -3.9999999999994023 for 5 qubits with 12 layers\n",
      "Step 290: Loss = -3.9999999999998037 for 5 qubits with 12 layers\n",
      "Step 300: Loss = -3.999999999999863 for 5 qubits with 12 layers\n",
      "Step 310: Loss = -3.999999999999892 for 5 qubits with 12 layers\n",
      "Step 320: Loss = -3.9999999999998987 for 5 qubits with 12 layers\n",
      "Step 330: Loss = -3.9999999999999147 for 5 qubits with 12 layers\n",
      "Step 340: Loss = -3.9999999999999094 for 5 qubits with 12 layers\n",
      "Step 350: Loss = -3.999999999999911 for 5 qubits with 12 layers\n",
      "Step 360: Loss = -3.999999999999909 for 5 qubits with 12 layers\n",
      "Step 370: Loss = -3.999999999999909 for 5 qubits with 12 layers\n",
      "Step 380: Loss = -3.9999999999999236 for 5 qubits with 12 layers\n",
      "Step 390: Loss = -3.999999999999923 for 5 qubits with 12 layers\n",
      "Step 400: Loss = -3.9999999999999205 for 5 qubits with 12 layers\n",
      "Step 410: Loss = -3.999999999999913 for 5 qubits with 12 layers\n",
      "Step 420: Loss = -3.9999999999999165 for 5 qubits with 12 layers\n",
      "Step 430: Loss = -3.9999999999999076 for 5 qubits with 12 layers\n",
      "Step 440: Loss = -3.9999999999999316 for 5 qubits with 12 layers\n",
      "Step 450: Loss = -3.9999999999999134 for 5 qubits with 12 layers\n",
      "Step 460: Loss = -3.999999998157144 for 5 qubits with 12 layers\n",
      "Step 470: Loss = -3.9909626643760117 for 5 qubits with 12 layers\n",
      "Step 480: Loss = -3.997421228384754 for 5 qubits with 12 layers\n",
      "Step 490: Loss = -3.9998981487683922 for 5 qubits with 12 layers\n",
      "Running optimization for 5 qubits and 13 layers\n",
      "Step 0: Loss = -1.1419084334270584 for 5 qubits with 13 layers\n",
      "Step 10: Loss = -3.7067516733995793 for 5 qubits with 13 layers\n",
      "Step 20: Loss = -3.8687698809797033 for 5 qubits with 13 layers\n",
      "Step 30: Loss = -3.9791263442134954 for 5 qubits with 13 layers\n",
      "Step 40: Loss = -3.9934500709207894 for 5 qubits with 13 layers\n",
      "Step 50: Loss = -3.994783268846989 for 5 qubits with 13 layers\n",
      "Step 60: Loss = -3.998618591340431 for 5 qubits with 13 layers\n",
      "Step 70: Loss = -3.999286134392151 for 5 qubits with 13 layers\n",
      "Step 80: Loss = -3.9997271577753786 for 5 qubits with 13 layers\n",
      "Step 90: Loss = -3.9999279076842527 for 5 qubits with 13 layers\n",
      "Step 100: Loss = -3.9999879131807536 for 5 qubits with 13 layers\n",
      "Step 110: Loss = -3.999991809259641 for 5 qubits with 13 layers\n",
      "Step 120: Loss = -3.999996398219533 for 5 qubits with 13 layers\n",
      "Step 130: Loss = -3.9999978965050715 for 5 qubits with 13 layers\n",
      "Step 140: Loss = -3.9999992026456566 for 5 qubits with 13 layers\n",
      "Step 150: Loss = -3.9999998038668503 for 5 qubits with 13 layers\n",
      "Step 160: Loss = -3.9999999292683692 for 5 qubits with 13 layers\n",
      "Step 170: Loss = -3.999999973957453 for 5 qubits with 13 layers\n",
      "Step 180: Loss = -3.999999991688801 for 5 qubits with 13 layers\n",
      "Step 190: Loss = -3.999999995878108 for 5 qubits with 13 layers\n",
      "Step 200: Loss = -3.9999999985474064 for 5 qubits with 13 layers\n",
      "Step 210: Loss = -3.999999999637289 for 5 qubits with 13 layers\n",
      "Step 220: Loss = -3.9999999998994795 for 5 qubits with 13 layers\n",
      "Step 230: Loss = -3.9999999999719926 for 5 qubits with 13 layers\n",
      "Step 240: Loss = -3.999999999982947 for 5 qubits with 13 layers\n",
      "Step 250: Loss = -3.999999999990962 for 5 qubits with 13 layers\n",
      "Step 260: Loss = -3.999999999997624 for 5 qubits with 13 layers\n",
      "Step 270: Loss = -3.9999999999991394 for 5 qubits with 13 layers\n",
      "Step 280: Loss = -3.9999999999995746 for 5 qubits with 13 layers\n",
      "Step 290: Loss = -3.999999999999817 for 5 qubits with 13 layers\n",
      "Step 300: Loss = -3.999999999999865 for 5 qubits with 13 layers\n",
      "Step 310: Loss = -3.999999999999903 for 5 qubits with 13 layers\n",
      "Step 320: Loss = -3.9999999999999076 for 5 qubits with 13 layers\n",
      "Step 330: Loss = -3.999999999999915 for 5 qubits with 13 layers\n",
      "Step 340: Loss = -3.9999999999999076 for 5 qubits with 13 layers\n",
      "Step 350: Loss = -3.9999999999999223 for 5 qubits with 13 layers\n",
      "Step 360: Loss = -3.9999999999999147 for 5 qubits with 13 layers\n",
      "Step 370: Loss = -3.9999999999999005 for 5 qubits with 13 layers\n",
      "Step 380: Loss = -3.9999999999999054 for 5 qubits with 13 layers\n",
      "Step 390: Loss = -3.999999999999904 for 5 qubits with 13 layers\n",
      "Step 400: Loss = -3.9999999999999147 for 5 qubits with 13 layers\n",
      "Step 410: Loss = -3.9999999999999107 for 5 qubits with 13 layers\n",
      "Step 420: Loss = -3.999999999999919 for 5 qubits with 13 layers\n",
      "Step 430: Loss = -3.999999999999907 for 5 qubits with 13 layers\n",
      "Step 440: Loss = -3.999999999999907 for 5 qubits with 13 layers\n",
      "Step 450: Loss = -3.999999999999871 for 5 qubits with 13 layers\n",
      "Step 460: Loss = -3.9999999850512613 for 5 qubits with 13 layers\n",
      "Step 470: Loss = -3.986279085718993 for 5 qubits with 13 layers\n",
      "Step 480: Loss = -3.9981232792218635 for 5 qubits with 13 layers\n",
      "Step 490: Loss = -3.9985600381537885 for 5 qubits with 13 layers\n",
      "Running optimization for 5 qubits and 14 layers\n",
      "Step 0: Loss = -1.1297346949360427 for 5 qubits with 14 layers\n",
      "Step 10: Loss = -3.722411614533627 for 5 qubits with 14 layers\n",
      "Step 20: Loss = -3.915366110954224 for 5 qubits with 14 layers\n",
      "Step 30: Loss = -3.962043720640331 for 5 qubits with 14 layers\n",
      "Step 40: Loss = -3.986375178798936 for 5 qubits with 14 layers\n",
      "Step 50: Loss = -3.9969599511104774 for 5 qubits with 14 layers\n",
      "Step 60: Loss = -3.998254070936742 for 5 qubits with 14 layers\n",
      "Step 70: Loss = -3.9995071945268967 for 5 qubits with 14 layers\n",
      "Step 80: Loss = -3.999755177960819 for 5 qubits with 14 layers\n",
      "Step 90: Loss = -3.999945331859668 for 5 qubits with 14 layers\n",
      "Step 100: Loss = -3.9999706430176776 for 5 qubits with 14 layers\n",
      "Step 110: Loss = -3.9999933121326787 for 5 qubits with 14 layers\n",
      "Step 120: Loss = -3.999995823114536 for 5 qubits with 14 layers\n",
      "Step 130: Loss = -3.9999992148756873 for 5 qubits with 14 layers\n",
      "Step 140: Loss = -3.999999488168487 for 5 qubits with 14 layers\n",
      "Step 150: Loss = -3.9999998939770807 for 5 qubits with 14 layers\n",
      "Step 160: Loss = -3.9999999488743114 for 5 qubits with 14 layers\n",
      "Step 170: Loss = -3.9999999724954067 for 5 qubits with 14 layers\n",
      "Step 180: Loss = -3.999999994599501 for 5 qubits with 14 layers\n",
      "Step 190: Loss = -3.99999999540183 for 5 qubits with 14 layers\n",
      "Step 200: Loss = -3.9999999989721235 for 5 qubits with 14 layers\n",
      "Step 210: Loss = -3.999999999525686 for 5 qubits with 14 layers\n",
      "Step 220: Loss = -3.9999999998907185 for 5 qubits with 14 layers\n",
      "Step 230: Loss = -3.999999999958913 for 5 qubits with 14 layers\n",
      "Step 240: Loss = -3.9999999999854565 for 5 qubits with 14 layers\n",
      "Step 250: Loss = -3.9999999999929567 for 5 qubits with 14 layers\n",
      "Step 260: Loss = -3.999999999998362 for 5 qubits with 14 layers\n",
      "Step 270: Loss = -3.9999999999991154 for 5 qubits with 14 layers\n",
      "Step 280: Loss = -3.999999999999808 for 5 qubits with 14 layers\n",
      "Step 290: Loss = -3.999999999999737 for 5 qubits with 14 layers\n",
      "Step 300: Loss = -3.999999999999878 for 5 qubits with 14 layers\n",
      "Step 310: Loss = -3.999999999999872 for 5 qubits with 14 layers\n",
      "Step 320: Loss = -3.999999999999884 for 5 qubits with 14 layers\n",
      "Step 330: Loss = -3.9999999999999183 for 5 qubits with 14 layers\n",
      "Step 340: Loss = -3.999999999999888 for 5 qubits with 14 layers\n",
      "Step 350: Loss = -3.999999999999911 for 5 qubits with 14 layers\n",
      "Step 360: Loss = -3.999999999999896 for 5 qubits with 14 layers\n",
      "Step 370: Loss = -3.999999999999911 for 5 qubits with 14 layers\n",
      "Step 380: Loss = -3.9999999999964473 for 5 qubits with 14 layers\n",
      "Step 390: Loss = -3.9999984399529906 for 5 qubits with 14 layers\n",
      "Step 400: Loss = -3.9986860654763223 for 5 qubits with 14 layers\n",
      "Step 410: Loss = -3.999709280644319 for 5 qubits with 14 layers\n",
      "Step 420: Loss = -3.9997844074308273 for 5 qubits with 14 layers\n",
      "Step 430: Loss = -3.9998146335618934 for 5 qubits with 14 layers\n",
      "Step 440: Loss = -3.9999445170377737 for 5 qubits with 14 layers\n",
      "Step 450: Loss = -3.9999906923275756 for 5 qubits with 14 layers\n",
      "Step 460: Loss = -3.99999369961299 for 5 qubits with 14 layers\n",
      "Step 470: Loss = -3.9999947254715966 for 5 qubits with 14 layers\n",
      "Step 480: Loss = -3.9999933533587244 for 5 qubits with 14 layers\n",
      "Step 490: Loss = -3.9990013782344302 for 5 qubits with 14 layers\n",
      "Running optimization for 5 qubits and 15 layers\n",
      "Step 0: Loss = -2.8945526310318126 for 5 qubits with 15 layers\n",
      "Step 10: Loss = -3.8941350729394353 for 5 qubits with 15 layers\n",
      "Step 20: Loss = -3.9727366218933855 for 5 qubits with 15 layers\n",
      "Step 30: Loss = -3.995949995747747 for 5 qubits with 15 layers\n",
      "Step 40: Loss = -3.997931676657138 for 5 qubits with 15 layers\n",
      "Step 50: Loss = -3.9983918962960465 for 5 qubits with 15 layers\n",
      "Step 60: Loss = -3.9989873844957895 for 5 qubits with 15 layers\n",
      "Step 70: Loss = -3.999922428553332 for 5 qubits with 15 layers\n",
      "Step 80: Loss = -3.999910106233844 for 5 qubits with 15 layers\n",
      "Step 90: Loss = -3.999967054856893 for 5 qubits with 15 layers\n",
      "Step 100: Loss = -3.999988675041468 for 5 qubits with 15 layers\n",
      "Step 110: Loss = -3.9999972440265474 for 5 qubits with 15 layers\n",
      "Step 120: Loss = -3.999997646677982 for 5 qubits with 15 layers\n",
      "Step 130: Loss = -3.999999473845513 for 5 qubits with 15 layers\n",
      "Step 140: Loss = -3.999999929084189 for 5 qubits with 15 layers\n",
      "Step 150: Loss = -3.9999999588513324 for 5 qubits with 15 layers\n",
      "Step 160: Loss = -3.9999999760375258 for 5 qubits with 15 layers\n",
      "Step 170: Loss = -3.9999999888258784 for 5 qubits with 15 layers\n",
      "Step 180: Loss = -3.999999996902789 for 5 qubits with 15 layers\n",
      "Step 190: Loss = -3.999999999323537 for 5 qubits with 15 layers\n",
      "Step 200: Loss = -3.9999999998053055 for 5 qubits with 15 layers\n",
      "Step 210: Loss = -3.999999999887221 for 5 qubits with 15 layers\n",
      "Step 220: Loss = -3.9999999999175966 for 5 qubits with 15 layers\n",
      "Step 230: Loss = -3.999999999992223 for 5 qubits with 15 layers\n",
      "Step 240: Loss = -3.9999999999933147 for 5 qubits with 15 layers\n",
      "Step 250: Loss = -3.999999999998125 for 5 qubits with 15 layers\n",
      "Step 260: Loss = -3.999999999998639 for 5 qubits with 15 layers\n",
      "Step 270: Loss = -3.999999999999745 for 5 qubits with 15 layers\n",
      "Step 280: Loss = -3.9999999999997957 for 5 qubits with 15 layers\n",
      "Step 290: Loss = -3.9999999999998086 for 5 qubits with 15 layers\n",
      "Step 300: Loss = -3.9999999999998472 for 5 qubits with 15 layers\n",
      "Step 310: Loss = -3.999999999999865 for 5 qubits with 15 layers\n",
      "Step 320: Loss = -3.9999999999999227 for 5 qubits with 15 layers\n",
      "Step 330: Loss = -3.999999999999499 for 5 qubits with 15 layers\n",
      "Step 340: Loss = -3.9999999609978074 for 5 qubits with 15 layers\n",
      "Step 350: Loss = -3.9993619507433196 for 5 qubits with 15 layers\n",
      "Step 360: Loss = -3.9998413370814787 for 5 qubits with 15 layers\n",
      "Step 370: Loss = -3.999871142809432 for 5 qubits with 15 layers\n",
      "Step 380: Loss = -3.9998871388938713 for 5 qubits with 15 layers\n",
      "Step 390: Loss = -3.9997949105300274 for 5 qubits with 15 layers\n",
      "Step 400: Loss = -3.9999972156337544 for 5 qubits with 15 layers\n",
      "Step 410: Loss = -3.9999963275668056 for 5 qubits with 15 layers\n",
      "Step 420: Loss = -3.9996513602197665 for 5 qubits with 15 layers\n",
      "Step 430: Loss = -3.9999420138451596 for 5 qubits with 15 layers\n",
      "Step 440: Loss = -3.9988429346674845 for 5 qubits with 15 layers\n",
      "Step 450: Loss = -3.9996151026594355 for 5 qubits with 15 layers\n",
      "Step 460: Loss = -3.999944324324956 for 5 qubits with 15 layers\n",
      "Step 470: Loss = -3.999932176170425 for 5 qubits with 15 layers\n",
      "Step 480: Loss = -3.9998420201219176 for 5 qubits with 15 layers\n",
      "Step 490: Loss = -3.9998158156074908 for 5 qubits with 15 layers\n",
      "Running optimization for 6 qubits and 1 layers\n",
      "Step 0: Loss = -4.8142464841659525 for 6 qubits with 1 layers\n",
      "Step 10: Loss = -4.999581328789666 for 6 qubits with 1 layers\n",
      "Step 20: Loss = -4.985683393796051 for 6 qubits with 1 layers\n",
      "Step 30: Loss = -4.999999999151069 for 6 qubits with 1 layers\n",
      "Step 40: Loss = -4.998102548873725 for 6 qubits with 1 layers\n",
      "Step 50: Loss = -4.9998240654242245 for 6 qubits with 1 layers\n",
      "Step 60: Loss = -4.999909084055239 for 6 qubits with 1 layers\n",
      "Step 70: Loss = -4.999901819468633 for 6 qubits with 1 layers\n",
      "Step 80: Loss = -4.999988582446669 for 6 qubits with 1 layers\n",
      "Step 90: Loss = -4.99999948419374 for 6 qubits with 1 layers\n",
      "Step 100: Loss = -4.999997116457207 for 6 qubits with 1 layers\n",
      "Step 110: Loss = -4.999998292157591 for 6 qubits with 1 layers\n",
      "Step 120: Loss = -4.999999473120109 for 6 qubits with 1 layers\n",
      "Step 130: Loss = -4.999999888964798 for 6 qubits with 1 layers\n",
      "Step 140: Loss = -4.999999982018954 for 6 qubits with 1 layers\n",
      "Step 150: Loss = -4.999999997430807 for 6 qubits with 1 layers\n",
      "Step 160: Loss = -4.999999999535216 for 6 qubits with 1 layers\n",
      "Step 170: Loss = -4.999999999814943 for 6 qubits with 1 layers\n",
      "Step 180: Loss = -4.999999999852593 for 6 qubits with 1 layers\n",
      "Step 190: Loss = -4.999999999866439 for 6 qubits with 1 layers\n",
      "Step 200: Loss = -4.999999999897797 for 6 qubits with 1 layers\n",
      "Step 210: Loss = -4.999999999941849 for 6 qubits with 1 layers\n",
      "Step 220: Loss = -4.999999999978806 for 6 qubits with 1 layers\n",
      "Step 230: Loss = -4.999999999996789 for 6 qubits with 1 layers\n",
      "Step 240: Loss = -4.9999999999999725 for 6 qubits with 1 layers\n",
      "Step 250: Loss = -4.9999999999993205 for 6 qubits with 1 layers\n",
      "Step 260: Loss = -4.999999999999646 for 6 qubits with 1 layers\n",
      "Step 270: Loss = -4.9999999999999805 for 6 qubits with 1 layers\n",
      "Step 280: Loss = -4.999999999999943 for 6 qubits with 1 layers\n",
      "Step 290: Loss = -4.99999999999998 for 6 qubits with 1 layers\n",
      "Step 300: Loss = -4.999999999999979 for 6 qubits with 1 layers\n",
      "Step 310: Loss = -4.999999999999992 for 6 qubits with 1 layers\n",
      "Step 320: Loss = -4.999999999999993 for 6 qubits with 1 layers\n",
      "Step 330: Loss = -4.999999999999978 for 6 qubits with 1 layers\n",
      "Step 340: Loss = -4.999999999999979 for 6 qubits with 1 layers\n",
      "Step 350: Loss = -4.999999999999993 for 6 qubits with 1 layers\n",
      "Step 360: Loss = -4.9999999999999805 for 6 qubits with 1 layers\n",
      "Step 370: Loss = -4.999999999999989 for 6 qubits with 1 layers\n",
      "Step 380: Loss = -4.99999999999999 for 6 qubits with 1 layers\n",
      "Step 390: Loss = -4.999999999999977 for 6 qubits with 1 layers\n",
      "Step 400: Loss = -4.999999999999988 for 6 qubits with 1 layers\n",
      "Step 410: Loss = -4.999999999999986 for 6 qubits with 1 layers\n",
      "Step 420: Loss = -4.99999999999999 for 6 qubits with 1 layers\n",
      "Step 430: Loss = -4.99999999999999 for 6 qubits with 1 layers\n",
      "Step 440: Loss = -4.999999999999989 for 6 qubits with 1 layers\n",
      "Step 450: Loss = -4.999999999999981 for 6 qubits with 1 layers\n",
      "Step 460: Loss = -4.999999999999981 for 6 qubits with 1 layers\n",
      "Step 470: Loss = -4.999999999999994 for 6 qubits with 1 layers\n",
      "Step 480: Loss = -4.999999999999983 for 6 qubits with 1 layers\n",
      "Step 490: Loss = -4.999999999999983 for 6 qubits with 1 layers\n",
      "Running optimization for 6 qubits and 2 layers\n",
      "Step 0: Loss = -0.5184735664004669 for 6 qubits with 2 layers\n",
      "Step 10: Loss = -2.475436319166408 for 6 qubits with 2 layers\n",
      "Step 20: Loss = -4.302595803163193 for 6 qubits with 2 layers\n",
      "Step 30: Loss = -4.974024589648851 for 6 qubits with 2 layers\n",
      "Step 40: Loss = -4.91129089474864 for 6 qubits with 2 layers\n",
      "Step 50: Loss = -4.964874689828698 for 6 qubits with 2 layers\n",
      "Step 60: Loss = -4.998329302014334 for 6 qubits with 2 layers\n",
      "Step 70: Loss = -4.994021046140444 for 6 qubits with 2 layers\n",
      "Step 80: Loss = -4.999716385822975 for 6 qubits with 2 layers\n",
      "Step 90: Loss = -4.999416375908564 for 6 qubits with 2 layers\n",
      "Step 100: Loss = -4.999896901657484 for 6 qubits with 2 layers\n",
      "Step 110: Loss = -4.999928996719362 for 6 qubits with 2 layers\n",
      "Step 120: Loss = -4.99998945641253 for 6 qubits with 2 layers\n",
      "Step 130: Loss = -4.999988654971556 for 6 qubits with 2 layers\n",
      "Step 140: Loss = -4.999999610224207 for 6 qubits with 2 layers\n",
      "Step 150: Loss = -4.99999824950677 for 6 qubits with 2 layers\n",
      "Step 160: Loss = -4.999999926584712 for 6 qubits with 2 layers\n",
      "Step 170: Loss = -4.999999860002551 for 6 qubits with 2 layers\n",
      "Step 180: Loss = -4.999999937607917 for 6 qubits with 2 layers\n",
      "Step 190: Loss = -4.999999997640358 for 6 qubits with 2 layers\n",
      "Step 200: Loss = -4.999999992148341 for 6 qubits with 2 layers\n",
      "Step 210: Loss = -4.999999996920587 for 6 qubits with 2 layers\n",
      "Step 220: Loss = -4.999999999879328 for 6 qubits with 2 layers\n",
      "Step 230: Loss = -4.999999999876819 for 6 qubits with 2 layers\n",
      "Step 240: Loss = -4.999999999846299 for 6 qubits with 2 layers\n",
      "Step 250: Loss = -4.999999999945455 for 6 qubits with 2 layers\n",
      "Step 260: Loss = -4.999999999991472 for 6 qubits with 2 layers\n",
      "Step 270: Loss = -4.99999999999966 for 6 qubits with 2 layers\n",
      "Step 280: Loss = -4.999999999999591 for 6 qubits with 2 layers\n",
      "Step 290: Loss = -4.99999999999944 for 6 qubits with 2 layers\n",
      "Step 300: Loss = -4.999999999999721 for 6 qubits with 2 layers\n",
      "Step 310: Loss = -4.999999999999877 for 6 qubits with 2 layers\n",
      "Step 320: Loss = -4.999999999999936 for 6 qubits with 2 layers\n",
      "Step 330: Loss = -4.9999999999999645 for 6 qubits with 2 layers\n",
      "Step 340: Loss = -4.999999999999961 for 6 qubits with 2 layers\n",
      "Step 350: Loss = -4.999999999999986 for 6 qubits with 2 layers\n",
      "Step 360: Loss = -4.999999999999977 for 6 qubits with 2 layers\n",
      "Step 370: Loss = -4.9999999999999725 for 6 qubits with 2 layers\n",
      "Step 380: Loss = -4.9999999999999805 for 6 qubits with 2 layers\n",
      "Step 390: Loss = -4.9999999999999725 for 6 qubits with 2 layers\n",
      "Step 400: Loss = -4.99999999999997 for 6 qubits with 2 layers\n",
      "Step 410: Loss = -4.9999999999999645 for 6 qubits with 2 layers\n",
      "Step 420: Loss = -4.999999999999972 for 6 qubits with 2 layers\n",
      "Step 430: Loss = -4.9999999999999645 for 6 qubits with 2 layers\n",
      "Step 440: Loss = -4.999999999999989 for 6 qubits with 2 layers\n",
      "Step 450: Loss = -4.999999999999969 for 6 qubits with 2 layers\n",
      "Step 460: Loss = -4.999999999999979 for 6 qubits with 2 layers\n",
      "Step 470: Loss = -4.9999999999999725 for 6 qubits with 2 layers\n",
      "Step 480: Loss = -4.999999999999976 for 6 qubits with 2 layers\n",
      "Step 490: Loss = -4.999999999999993 for 6 qubits with 2 layers\n",
      "Running optimization for 6 qubits and 3 layers\n",
      "Step 0: Loss = -0.9782580764719798 for 6 qubits with 3 layers\n",
      "Step 10: Loss = -2.2033008496754034 for 6 qubits with 3 layers\n",
      "Step 20: Loss = -2.6394581604261314 for 6 qubits with 3 layers\n",
      "Step 30: Loss = -3.7632905044739062 for 6 qubits with 3 layers\n",
      "Step 40: Loss = -4.425220137221459 for 6 qubits with 3 layers\n",
      "Step 50: Loss = -4.8358561170378795 for 6 qubits with 3 layers\n",
      "Step 60: Loss = -4.902868703576324 for 6 qubits with 3 layers\n",
      "Step 70: Loss = -4.96279210489027 for 6 qubits with 3 layers\n",
      "Step 80: Loss = -4.985485155064301 for 6 qubits with 3 layers\n",
      "Step 90: Loss = -4.994993647978165 for 6 qubits with 3 layers\n",
      "Step 100: Loss = -4.998463204154681 for 6 qubits with 3 layers\n",
      "Step 110: Loss = -4.999248434429092 for 6 qubits with 3 layers\n",
      "Step 120: Loss = -4.999588550022201 for 6 qubits with 3 layers\n",
      "Step 130: Loss = -4.999747790487231 for 6 qubits with 3 layers\n",
      "Step 140: Loss = -4.999818203191115 for 6 qubits with 3 layers\n",
      "Step 150: Loss = -4.999860426650039 for 6 qubits with 3 layers\n",
      "Step 160: Loss = -4.9998900537296915 for 6 qubits with 3 layers\n",
      "Step 170: Loss = -4.999911540402197 for 6 qubits with 3 layers\n",
      "Step 180: Loss = -4.999927635615742 for 6 qubits with 3 layers\n",
      "Step 190: Loss = -4.999940126673563 for 6 qubits with 3 layers\n",
      "Step 200: Loss = -4.999950049820888 for 6 qubits with 3 layers\n",
      "Step 210: Loss = -4.999958049850105 for 6 qubits with 3 layers\n",
      "Step 220: Loss = -4.999964565254663 for 6 qubits with 3 layers\n",
      "Step 230: Loss = -4.999969916767035 for 6 qubits with 3 layers\n",
      "Step 240: Loss = -4.999974344319624 for 6 qubits with 3 layers\n",
      "Step 250: Loss = -4.999978030422677 for 6 qubits with 3 layers\n",
      "Step 260: Loss = -4.999981116574313 for 6 qubits with 3 layers\n",
      "Step 270: Loss = -4.999983713722847 for 6 qubits with 3 layers\n",
      "Step 280: Loss = -4.999985909633341 for 6 qubits with 3 layers\n",
      "Step 290: Loss = -4.999987774316598 for 6 qubits with 3 layers\n",
      "Step 300: Loss = -4.999989364032109 for 6 qubits with 3 layers\n",
      "Step 310: Loss = -4.999990724313572 for 6 qubits with 3 layers\n",
      "Step 320: Loss = -4.999991892237984 for 6 qubits with 3 layers\n",
      "Step 330: Loss = -4.999992898174774 for 6 qubits with 3 layers\n",
      "Step 340: Loss = -4.999993767134986 for 6 qubits with 3 layers\n",
      "Step 350: Loss = -4.999994519821923 for 6 qubits with 3 layers\n",
      "Step 360: Loss = -4.9999951734546055 for 6 qubits with 3 layers\n",
      "Step 370: Loss = -4.999995742417756 for 6 qubits with 3 layers\n",
      "Step 380: Loss = -4.999996238779094 for 6 qubits with 3 layers\n",
      "Step 390: Loss = -4.999996672702226 for 6 qubits with 3 layers\n",
      "Step 400: Loss = -4.999997052778951 for 6 qubits with 3 layers\n",
      "Step 410: Loss = -4.999997386297651 for 6 qubits with 3 layers\n",
      "Step 420: Loss = -4.999997679461478 for 6 qubits with 3 layers\n",
      "Step 430: Loss = -4.999997937566407 for 6 qubits with 3 layers\n",
      "Step 440: Loss = -4.999998165147175 for 6 qubits with 3 layers\n",
      "Step 450: Loss = -4.999998366097533 for 6 qubits with 3 layers\n",
      "Step 460: Loss = -4.999998543769697 for 6 qubits with 3 layers\n",
      "Step 470: Loss = -4.999998701056827 for 6 qubits with 3 layers\n",
      "Step 480: Loss = -4.999998840461752 for 6 qubits with 3 layers\n",
      "Step 490: Loss = -4.99999896415461 for 6 qubits with 3 layers\n",
      "Running optimization for 6 qubits and 4 layers\n",
      "Step 0: Loss = -1.2260952001960774 for 6 qubits with 4 layers\n",
      "Step 10: Loss = -2.987382747529867 for 6 qubits with 4 layers\n",
      "Step 20: Loss = -3.600444964151811 for 6 qubits with 4 layers\n",
      "Step 30: Loss = -3.919666803780764 for 6 qubits with 4 layers\n",
      "Step 40: Loss = -4.0384454064392 for 6 qubits with 4 layers\n",
      "Step 50: Loss = -4.165539716621719 for 6 qubits with 4 layers\n",
      "Step 60: Loss = -4.382604346859203 for 6 qubits with 4 layers\n",
      "Step 70: Loss = -4.631083294823198 for 6 qubits with 4 layers\n",
      "Step 80: Loss = -4.808771535840061 for 6 qubits with 4 layers\n",
      "Step 90: Loss = -4.874810510406881 for 6 qubits with 4 layers\n",
      "Step 100: Loss = -4.926060935152583 for 6 qubits with 4 layers\n",
      "Step 110: Loss = -4.970857745686633 for 6 qubits with 4 layers\n",
      "Step 120: Loss = -4.992176593253105 for 6 qubits with 4 layers\n",
      "Step 130: Loss = -4.998153843709162 for 6 qubits with 4 layers\n",
      "Step 140: Loss = -4.999380243400437 for 6 qubits with 4 layers\n",
      "Step 150: Loss = -4.9997019244962875 for 6 qubits with 4 layers\n",
      "Step 160: Loss = -4.999816477730076 for 6 qubits with 4 layers\n",
      "Step 170: Loss = -4.9998703626821905 for 6 qubits with 4 layers\n",
      "Step 180: Loss = -4.999900631976786 for 6 qubits with 4 layers\n",
      "Step 190: Loss = -4.999919969948714 for 6 qubits with 4 layers\n",
      "Step 200: Loss = -4.999933704530461 for 6 qubits with 4 layers\n",
      "Step 210: Loss = -4.999944191836123 for 6 qubits with 4 layers\n",
      "Step 220: Loss = -4.99995255992518 for 6 qubits with 4 layers\n",
      "Step 230: Loss = -4.999959408766079 for 6 qubits with 4 layers\n",
      "Step 240: Loss = -4.999965100786574 for 6 qubits with 4 layers\n",
      "Step 250: Loss = -4.99996987808083 for 6 qubits with 4 layers\n",
      "Step 260: Loss = -4.999973915956273 for 6 qubits with 4 layers\n",
      "Step 270: Loss = -4.999977347228686 for 6 qubits with 4 layers\n",
      "Step 280: Loss = -4.999980276062056 for 6 qubits with 4 layers\n",
      "Step 290: Loss = -4.9999827856167585 for 6 qubits with 4 layers\n",
      "Step 300: Loss = -4.999984943226114 for 6 qubits with 4 layers\n",
      "Step 310: Loss = -4.999986803943665 for 6 qubits with 4 layers\n",
      "Step 320: Loss = -4.999988413137249 for 6 qubits with 4 layers\n",
      "Step 330: Loss = -4.999989808430557 for 6 qubits with 4 layers\n",
      "Step 340: Loss = -4.9999910211806595 for 6 qubits with 4 layers\n",
      "Step 350: Loss = -4.999992077644708 for 6 qubits with 4 layers\n",
      "Step 360: Loss = -4.999992999902812 for 6 qubits with 4 layers\n",
      "Step 370: Loss = -4.999993806593059 for 6 qubits with 4 layers\n",
      "Step 380: Loss = -4.999994513504458 for 6 qubits with 4 layers\n",
      "Step 390: Loss = -4.999995134056592 for 6 qubits with 4 layers\n",
      "Step 400: Loss = -4.999995679691041 for 6 qubits with 4 layers\n",
      "Step 410: Loss = -4.9999961601914125 for 6 qubits with 4 layers\n",
      "Step 420: Loss = -4.999996583946612 for 6 qubits with 4 layers\n",
      "Step 430: Loss = -4.999996958168975 for 6 qubits with 4 layers\n",
      "Step 440: Loss = -4.9999972890742 for 6 qubits with 4 layers\n",
      "Step 450: Loss = -4.999997582032032 for 6 qubits with 4 layers\n",
      "Step 460: Loss = -4.999997841691679 for 6 qubits with 4 layers\n",
      "Step 470: Loss = -4.999998072087012 for 6 qubits with 4 layers\n",
      "Step 480: Loss = -4.999998276725191 for 6 qubits with 4 layers\n",
      "Step 490: Loss = -4.999998458661224 for 6 qubits with 4 layers\n",
      "Running optimization for 6 qubits and 5 layers\n",
      "Step 0: Loss = -1.0163054463325176 for 6 qubits with 5 layers\n",
      "Step 10: Loss = -3.847286167380884 for 6 qubits with 5 layers\n",
      "Step 20: Loss = -3.975375093123819 for 6 qubits with 5 layers\n",
      "Step 30: Loss = -4.418918533897006 for 6 qubits with 5 layers\n",
      "Step 40: Loss = -4.81373810651451 for 6 qubits with 5 layers\n",
      "Step 50: Loss = -4.972538931812795 for 6 qubits with 5 layers\n",
      "Step 60: Loss = -4.973472924582016 for 6 qubits with 5 layers\n",
      "Step 70: Loss = -4.98360844285121 for 6 qubits with 5 layers\n",
      "Step 80: Loss = -4.991485467299945 for 6 qubits with 5 layers\n",
      "Step 90: Loss = -4.99192012334143 for 6 qubits with 5 layers\n",
      "Step 100: Loss = -4.992783474763444 for 6 qubits with 5 layers\n",
      "Step 110: Loss = -4.992962754075098 for 6 qubits with 5 layers\n",
      "Step 120: Loss = -4.99299362522746 for 6 qubits with 5 layers\n",
      "Step 130: Loss = -4.993031688328571 for 6 qubits with 5 layers\n",
      "Step 140: Loss = -4.993045929110145 for 6 qubits with 5 layers\n",
      "Step 150: Loss = -4.99304556956824 for 6 qubits with 5 layers\n",
      "Step 160: Loss = -4.99304765451864 for 6 qubits with 5 layers\n",
      "Step 170: Loss = -4.9930479605082665 for 6 qubits with 5 layers\n",
      "Step 180: Loss = -4.993048105562834 for 6 qubits with 5 layers\n",
      "Step 190: Loss = -4.993048125198149 for 6 qubits with 5 layers\n",
      "Step 200: Loss = -4.993048144734167 for 6 qubits with 5 layers\n",
      "Step 210: Loss = -4.993048151187092 for 6 qubits with 5 layers\n",
      "Step 220: Loss = -4.993048155123558 for 6 qubits with 5 layers\n",
      "Step 230: Loss = -4.993048156048592 for 6 qubits with 5 layers\n",
      "Step 240: Loss = -4.993048156622478 for 6 qubits with 5 layers\n",
      "Step 250: Loss = -4.993048156662162 for 6 qubits with 5 layers\n",
      "Step 260: Loss = -4.993048156728742 for 6 qubits with 5 layers\n",
      "Step 270: Loss = -4.993048156746083 for 6 qubits with 5 layers\n",
      "Step 280: Loss = -4.993048156755011 for 6 qubits with 5 layers\n",
      "Step 290: Loss = -4.993048156758384 for 6 qubits with 5 layers\n",
      "Step 300: Loss = -4.9930481567585 for 6 qubits with 5 layers\n",
      "Step 310: Loss = -4.99304815675872 for 6 qubits with 5 layers\n",
      "Step 320: Loss = -4.993048156758829 for 6 qubits with 5 layers\n",
      "Step 330: Loss = -4.993048156758843 for 6 qubits with 5 layers\n",
      "Step 340: Loss = -4.993048156758878 for 6 qubits with 5 layers\n",
      "Step 350: Loss = -4.993048156758876 for 6 qubits with 5 layers\n",
      "Step 360: Loss = -4.993048156758858 for 6 qubits with 5 layers\n",
      "Step 370: Loss = -4.993048156758881 for 6 qubits with 5 layers\n",
      "Step 380: Loss = -4.993048156758869 for 6 qubits with 5 layers\n",
      "Step 390: Loss = -4.993048156758862 for 6 qubits with 5 layers\n",
      "Step 400: Loss = -4.9930481567588645 for 6 qubits with 5 layers\n",
      "Step 410: Loss = -4.993048156758864 for 6 qubits with 5 layers\n",
      "Step 420: Loss = -4.993048156758877 for 6 qubits with 5 layers\n",
      "Step 430: Loss = -4.99304815675888 for 6 qubits with 5 layers\n",
      "Step 440: Loss = -4.9930481567588725 for 6 qubits with 5 layers\n",
      "Step 450: Loss = -4.99304815675886 for 6 qubits with 5 layers\n",
      "Step 460: Loss = -4.993048156758871 for 6 qubits with 5 layers\n",
      "Step 470: Loss = -4.993048156758832 for 6 qubits with 5 layers\n",
      "Step 480: Loss = -4.993048156758881 for 6 qubits with 5 layers\n",
      "Step 490: Loss = -4.99304815675886 for 6 qubits with 5 layers\n",
      "Running optimization for 6 qubits and 6 layers\n",
      "Step 0: Loss = -2.118778489790674 for 6 qubits with 6 layers\n",
      "Step 10: Loss = -4.638653434783886 for 6 qubits with 6 layers\n",
      "Step 20: Loss = -4.687640219747682 for 6 qubits with 6 layers\n",
      "Step 30: Loss = -4.864563773357569 for 6 qubits with 6 layers\n",
      "Step 40: Loss = -4.883792487377293 for 6 qubits with 6 layers\n",
      "Step 50: Loss = -4.92380451000189 for 6 qubits with 6 layers\n",
      "Step 60: Loss = -4.9581554092383096 for 6 qubits with 6 layers\n",
      "Step 70: Loss = -4.980410515711636 for 6 qubits with 6 layers\n",
      "Step 80: Loss = -4.994197018920157 for 6 qubits with 6 layers\n",
      "Step 90: Loss = -4.998051945674553 for 6 qubits with 6 layers\n",
      "Step 100: Loss = -4.9990869472728185 for 6 qubits with 6 layers\n",
      "Step 110: Loss = -4.999521765372522 for 6 qubits with 6 layers\n",
      "Step 120: Loss = -4.99959434803506 for 6 qubits with 6 layers\n",
      "Step 130: Loss = -4.999731765426825 for 6 qubits with 6 layers\n",
      "Step 140: Loss = -4.999791033216902 for 6 qubits with 6 layers\n",
      "Step 150: Loss = -4.999843303053001 for 6 qubits with 6 layers\n",
      "Step 160: Loss = -4.999878130769017 for 6 qubits with 6 layers\n",
      "Step 170: Loss = -4.999906969861427 for 6 qubits with 6 layers\n",
      "Step 180: Loss = -4.999929713941784 for 6 qubits with 6 layers\n",
      "Step 190: Loss = -4.99994780900695 for 6 qubits with 6 layers\n",
      "Step 200: Loss = -4.999961885711133 for 6 qubits with 6 layers\n",
      "Step 210: Loss = -4.9999726751986024 for 6 qubits with 6 layers\n",
      "Step 220: Loss = -4.999980808814288 for 6 qubits with 6 layers\n",
      "Step 230: Loss = -4.9999868126245905 for 6 qubits with 6 layers\n",
      "Step 240: Loss = -4.9999911510505415 for 6 qubits with 6 layers\n",
      "Step 250: Loss = -4.999994214383649 for 6 qubits with 6 layers\n",
      "Step 260: Loss = -4.999996323097619 for 6 qubits with 6 layers\n",
      "Step 270: Loss = -4.999997734892559 for 6 qubits with 6 layers\n",
      "Step 280: Loss = -4.999998651772852 for 6 qubits with 6 layers\n",
      "Step 290: Loss = -4.999999227599265 for 6 qubits with 6 layers\n",
      "Step 300: Loss = -4.999999576012659 for 6 qubits with 6 layers\n",
      "Step 310: Loss = -4.999999778238067 for 6 qubits with 6 layers\n",
      "Step 320: Loss = -4.999999890238734 for 6 qubits with 6 layers\n",
      "Step 330: Loss = -4.99999994904329 for 6 qubits with 6 layers\n",
      "Step 340: Loss = -4.99999997807053 for 6 qubits with 6 layers\n",
      "Step 350: Loss = -4.999999991394813 for 6 qubits with 6 layers\n",
      "Step 360: Loss = -4.999999996956268 for 6 qubits with 6 layers\n",
      "Step 370: Loss = -4.999979504124332 for 6 qubits with 6 layers\n",
      "Step 380: Loss = -4.998423440198337 for 6 qubits with 6 layers\n",
      "Step 390: Loss = -4.999537145771762 for 6 qubits with 6 layers\n",
      "Step 400: Loss = -4.9998362068914215 for 6 qubits with 6 layers\n",
      "Step 410: Loss = -4.999976724764707 for 6 qubits with 6 layers\n",
      "Step 420: Loss = -4.999990817356731 for 6 qubits with 6 layers\n",
      "Step 430: Loss = -4.999994594976425 for 6 qubits with 6 layers\n",
      "Step 440: Loss = -4.99999606769118 for 6 qubits with 6 layers\n",
      "Step 450: Loss = -4.999999531696865 for 6 qubits with 6 layers\n",
      "Step 460: Loss = -4.999999263498294 for 6 qubits with 6 layers\n",
      "Step 470: Loss = -4.99999965929354 for 6 qubits with 6 layers\n",
      "Step 480: Loss = -4.999999999777361 for 6 qubits with 6 layers\n",
      "Step 490: Loss = -4.999999098865713 for 6 qubits with 6 layers\n",
      "Running optimization for 6 qubits and 7 layers\n",
      "Step 0: Loss = -1.5607223609257976 for 6 qubits with 7 layers\n",
      "Step 10: Loss = -4.244221886855095 for 6 qubits with 7 layers\n",
      "Step 20: Loss = -4.5779588618877485 for 6 qubits with 7 layers\n",
      "Step 30: Loss = -4.916707693661885 for 6 qubits with 7 layers\n",
      "Step 40: Loss = -4.936121776479117 for 6 qubits with 7 layers\n",
      "Step 50: Loss = -4.968910486962011 for 6 qubits with 7 layers\n",
      "Step 60: Loss = -4.990202510705372 for 6 qubits with 7 layers\n",
      "Step 70: Loss = -4.995421997452834 for 6 qubits with 7 layers\n",
      "Step 80: Loss = -4.997825740156367 for 6 qubits with 7 layers\n",
      "Step 90: Loss = -4.998806309889259 for 6 qubits with 7 layers\n",
      "Step 100: Loss = -4.999382647688394 for 6 qubits with 7 layers\n",
      "Step 110: Loss = -4.999645423414015 for 6 qubits with 7 layers\n",
      "Step 120: Loss = -4.99977785256194 for 6 qubits with 7 layers\n",
      "Step 130: Loss = -4.999861029799375 for 6 qubits with 7 layers\n",
      "Step 140: Loss = -4.999908263092847 for 6 qubits with 7 layers\n",
      "Step 150: Loss = -4.999940030987395 for 6 qubits with 7 layers\n",
      "Step 160: Loss = -4.999960617934908 for 6 qubits with 7 layers\n",
      "Step 170: Loss = -4.999974180731538 for 6 qubits with 7 layers\n",
      "Step 180: Loss = -4.999983270032386 for 6 qubits with 7 layers\n",
      "Step 190: Loss = -4.9999893076989625 for 6 qubits with 7 layers\n",
      "Step 200: Loss = -4.9999932798370255 for 6 qubits with 7 layers\n",
      "Step 210: Loss = -4.999995864939305 for 6 qubits with 7 layers\n",
      "Step 220: Loss = -4.9999975186781045 for 6 qubits with 7 layers\n",
      "Step 230: Loss = -4.9999985532484414 for 6 qubits with 7 layers\n",
      "Step 240: Loss = -4.999999184028141 for 6 qubits with 7 layers\n",
      "Step 250: Loss = -4.999999557096981 for 6 qubits with 7 layers\n",
      "Step 260: Loss = -4.999999762602435 for 6 qubits with 7 layers\n",
      "Step 270: Loss = -4.998672758541228 for 6 qubits with 7 layers\n",
      "Step 280: Loss = -4.999414259740216 for 6 qubits with 7 layers\n",
      "Step 290: Loss = -4.999968136079574 for 6 qubits with 7 layers\n",
      "Step 300: Loss = -4.999880443648001 for 6 qubits with 7 layers\n",
      "Step 310: Loss = -4.999981469824853 for 6 qubits with 7 layers\n",
      "Step 320: Loss = -4.999996620874278 for 6 qubits with 7 layers\n",
      "Step 330: Loss = -4.999993405264538 for 6 qubits with 7 layers\n",
      "Step 340: Loss = -4.999999990210553 for 6 qubits with 7 layers\n",
      "Step 350: Loss = -4.999999815803644 for 6 qubits with 7 layers\n",
      "Step 360: Loss = -4.999999790816778 for 6 qubits with 7 layers\n",
      "Step 370: Loss = -4.999987803763058 for 6 qubits with 7 layers\n",
      "Step 380: Loss = -4.999742885852305 for 6 qubits with 7 layers\n",
      "Step 390: Loss = -4.999639362361781 for 6 qubits with 7 layers\n",
      "Step 400: Loss = -4.999984159984879 for 6 qubits with 7 layers\n",
      "Step 410: Loss = -4.9999589522068755 for 6 qubits with 7 layers\n",
      "Step 420: Loss = -4.9999826060031305 for 6 qubits with 7 layers\n",
      "Step 430: Loss = -4.999988212604605 for 6 qubits with 7 layers\n",
      "Step 440: Loss = -4.999961982739078 for 6 qubits with 7 layers\n",
      "Step 450: Loss = -4.9993559932804095 for 6 qubits with 7 layers\n",
      "Step 460: Loss = -4.999636235664495 for 6 qubits with 7 layers\n",
      "Step 470: Loss = -4.999909956889416 for 6 qubits with 7 layers\n",
      "Step 480: Loss = -4.999931215949962 for 6 qubits with 7 layers\n",
      "Step 490: Loss = -4.9998744505583685 for 6 qubits with 7 layers\n",
      "Running optimization for 6 qubits and 8 layers\n",
      "Step 0: Loss = -1.495824717994922 for 6 qubits with 8 layers\n",
      "Step 10: Loss = -4.300132573124989 for 6 qubits with 8 layers\n",
      "Step 20: Loss = -4.8172863607401695 for 6 qubits with 8 layers\n",
      "Step 30: Loss = -4.8927618889090985 for 6 qubits with 8 layers\n",
      "Step 40: Loss = -4.93349809819108 for 6 qubits with 8 layers\n",
      "Step 50: Loss = -4.959347781808177 for 6 qubits with 8 layers\n",
      "Step 60: Loss = -4.967148714866481 for 6 qubits with 8 layers\n",
      "Step 70: Loss = -4.980077255511709 for 6 qubits with 8 layers\n",
      "Step 80: Loss = -4.990345819110946 for 6 qubits with 8 layers\n",
      "Step 90: Loss = -4.997489504670137 for 6 qubits with 8 layers\n",
      "Step 100: Loss = -4.999713042904238 for 6 qubits with 8 layers\n",
      "Step 110: Loss = -4.999698053394846 for 6 qubits with 8 layers\n",
      "Step 120: Loss = -4.999883159905722 for 6 qubits with 8 layers\n",
      "Step 130: Loss = -4.999983224506991 for 6 qubits with 8 layers\n",
      "Step 140: Loss = -4.99998322316615 for 6 qubits with 8 layers\n",
      "Step 150: Loss = -4.9999966903325745 for 6 qubits with 8 layers\n",
      "Step 160: Loss = -4.999998486155551 for 6 qubits with 8 layers\n",
      "Step 170: Loss = -4.999999352803166 for 6 qubits with 8 layers\n",
      "Step 180: Loss = -4.999999852321916 for 6 qubits with 8 layers\n",
      "Step 190: Loss = -4.999999910097748 for 6 qubits with 8 layers\n",
      "Step 200: Loss = -4.999999985664534 for 6 qubits with 8 layers\n",
      "Step 210: Loss = -4.999999985241054 for 6 qubits with 8 layers\n",
      "Step 220: Loss = -4.999999998964739 for 6 qubits with 8 layers\n",
      "Step 230: Loss = -4.9999999980793195 for 6 qubits with 8 layers\n",
      "Step 240: Loss = -4.999999999892442 for 6 qubits with 8 layers\n",
      "Step 250: Loss = -4.9999999997348725 for 6 qubits with 8 layers\n",
      "Step 260: Loss = -4.999999999978518 for 6 qubits with 8 layers\n",
      "Step 270: Loss = -4.999999999971232 for 6 qubits with 8 layers\n",
      "Step 280: Loss = -4.999999999993276 for 6 qubits with 8 layers\n",
      "Step 290: Loss = -4.999999999998416 for 6 qubits with 8 layers\n",
      "Step 300: Loss = -4.999999999998429 for 6 qubits with 8 layers\n",
      "Step 310: Loss = -4.9999999999997575 for 6 qubits with 8 layers\n",
      "Step 320: Loss = -4.999999999999787 for 6 qubits with 8 layers\n",
      "Step 330: Loss = -4.999999999999842 for 6 qubits with 8 layers\n",
      "Step 340: Loss = -4.999999999999908 for 6 qubits with 8 layers\n",
      "Step 350: Loss = -4.9999999999999 for 6 qubits with 8 layers\n",
      "Step 360: Loss = -4.999999999999897 for 6 qubits with 8 layers\n",
      "Step 370: Loss = -4.999999999999909 for 6 qubits with 8 layers\n",
      "Step 380: Loss = -4.999999999999931 for 6 qubits with 8 layers\n",
      "Step 390: Loss = -4.999999999999924 for 6 qubits with 8 layers\n",
      "Step 400: Loss = -4.999999999999898 for 6 qubits with 8 layers\n",
      "Step 410: Loss = -4.999999999999912 for 6 qubits with 8 layers\n",
      "Step 420: Loss = -4.999999999999901 for 6 qubits with 8 layers\n",
      "Step 430: Loss = -4.99999999999991 for 6 qubits with 8 layers\n",
      "Step 440: Loss = -4.999999999999881 for 6 qubits with 8 layers\n",
      "Step 450: Loss = -4.999999999999906 for 6 qubits with 8 layers\n",
      "Step 460: Loss = -4.999999999999942 for 6 qubits with 8 layers\n",
      "Step 470: Loss = -4.99999999999992 for 6 qubits with 8 layers\n",
      "Step 480: Loss = -4.999999999999898 for 6 qubits with 8 layers\n",
      "Step 490: Loss = -4.99999996679404 for 6 qubits with 8 layers\n",
      "Running optimization for 6 qubits and 9 layers\n",
      "Step 0: Loss = -1.7691051443335 for 6 qubits with 9 layers\n",
      "Step 10: Loss = -4.16512953512556 for 6 qubits with 9 layers\n",
      "Step 20: Loss = -4.5634814467264535 for 6 qubits with 9 layers\n",
      "Step 30: Loss = -4.775945733880384 for 6 qubits with 9 layers\n",
      "Step 40: Loss = -4.947417758925944 for 6 qubits with 9 layers\n",
      "Step 50: Loss = -4.975706321940154 for 6 qubits with 9 layers\n",
      "Step 60: Loss = -4.988233658638802 for 6 qubits with 9 layers\n",
      "Step 70: Loss = -4.994423899156274 for 6 qubits with 9 layers\n",
      "Step 80: Loss = -4.997633429356353 for 6 qubits with 9 layers\n",
      "Step 90: Loss = -4.999303771374192 for 6 qubits with 9 layers\n",
      "Step 100: Loss = -4.999844903028115 for 6 qubits with 9 layers\n",
      "Step 110: Loss = -4.999961512085736 for 6 qubits with 9 layers\n",
      "Step 120: Loss = -4.9999799088848516 for 6 qubits with 9 layers\n",
      "Step 130: Loss = -4.999989222122477 for 6 qubits with 9 layers\n",
      "Step 140: Loss = -4.999995617683183 for 6 qubits with 9 layers\n",
      "Step 150: Loss = -4.9999990210303125 for 6 qubits with 9 layers\n",
      "Step 160: Loss = -4.99999986327037 for 6 qubits with 9 layers\n",
      "Step 170: Loss = -4.999999867234366 for 6 qubits with 9 layers\n",
      "Step 180: Loss = -4.999999943974146 for 6 qubits with 9 layers\n",
      "Step 190: Loss = -4.999999982793999 for 6 qubits with 9 layers\n",
      "Step 200: Loss = -4.999999997914995 for 6 qubits with 9 layers\n",
      "Step 210: Loss = -4.999999997068712 for 6 qubits with 9 layers\n",
      "Step 220: Loss = -4.9999999989006065 for 6 qubits with 9 layers\n",
      "Step 230: Loss = -4.999999999790959 for 6 qubits with 9 layers\n",
      "Step 240: Loss = -4.9999999999015285 for 6 qubits with 9 layers\n",
      "Step 250: Loss = -4.999999999947952 for 6 qubits with 9 layers\n",
      "Step 260: Loss = -4.9999999999917115 for 6 qubits with 9 layers\n",
      "Step 270: Loss = -4.999999999997141 for 6 qubits with 9 layers\n",
      "Step 280: Loss = -4.999999999997494 for 6 qubits with 9 layers\n",
      "Step 290: Loss = -4.99999999999967 for 6 qubits with 9 layers\n",
      "Step 300: Loss = -4.999999999999635 for 6 qubits with 9 layers\n",
      "Step 310: Loss = -4.999999999999822 for 6 qubits with 9 layers\n",
      "Step 320: Loss = -4.999999999999859 for 6 qubits with 9 layers\n",
      "Step 330: Loss = -4.999999999999898 for 6 qubits with 9 layers\n",
      "Step 340: Loss = -4.999999999999919 for 6 qubits with 9 layers\n",
      "Step 350: Loss = -4.999999999999906 for 6 qubits with 9 layers\n",
      "Step 360: Loss = -4.999999999999884 for 6 qubits with 9 layers\n",
      "Step 370: Loss = -4.999999999999913 for 6 qubits with 9 layers\n",
      "Step 380: Loss = -4.999999999999897 for 6 qubits with 9 layers\n",
      "Step 390: Loss = -4.999999999999924 for 6 qubits with 9 layers\n",
      "Step 400: Loss = -4.999999999999858 for 6 qubits with 9 layers\n",
      "Step 410: Loss = -4.999999999999916 for 6 qubits with 9 layers\n",
      "Step 420: Loss = -4.999999999999896 for 6 qubits with 9 layers\n",
      "Step 430: Loss = -4.999999999999927 for 6 qubits with 9 layers\n",
      "Step 440: Loss = -4.999999999577666 for 6 qubits with 9 layers\n",
      "Step 450: Loss = -4.998915220587375 for 6 qubits with 9 layers\n",
      "Step 460: Loss = -4.998461549910199 for 6 qubits with 9 layers\n",
      "Step 470: Loss = -4.999425796812131 for 6 qubits with 9 layers\n",
      "Step 480: Loss = -4.999790409660901 for 6 qubits with 9 layers\n",
      "Step 490: Loss = -4.99996938741173 for 6 qubits with 9 layers\n",
      "Running optimization for 6 qubits and 10 layers\n",
      "Step 0: Loss = -1.9223416192204597 for 6 qubits with 10 layers\n",
      "Step 10: Loss = -4.313561507389703 for 6 qubits with 10 layers\n",
      "Step 20: Loss = -4.762398915445606 for 6 qubits with 10 layers\n",
      "Step 30: Loss = -4.907473679500086 for 6 qubits with 10 layers\n",
      "Step 40: Loss = -4.9670499531239765 for 6 qubits with 10 layers\n",
      "Step 50: Loss = -4.989667948994716 for 6 qubits with 10 layers\n",
      "Step 60: Loss = -4.996225904331996 for 6 qubits with 10 layers\n",
      "Step 70: Loss = -4.998645441981749 for 6 qubits with 10 layers\n",
      "Step 80: Loss = -4.9994881974579926 for 6 qubits with 10 layers\n",
      "Step 90: Loss = -4.999797142435798 for 6 qubits with 10 layers\n",
      "Step 100: Loss = -4.999959894321744 for 6 qubits with 10 layers\n",
      "Step 110: Loss = -4.99999479078842 for 6 qubits with 10 layers\n",
      "Step 120: Loss = -4.999992644810131 for 6 qubits with 10 layers\n",
      "Step 130: Loss = -4.999997388876154 for 6 qubits with 10 layers\n",
      "Step 140: Loss = -4.999999430457938 for 6 qubits with 10 layers\n",
      "Step 150: Loss = -4.999999614876693 for 6 qubits with 10 layers\n",
      "Step 160: Loss = -4.999999919446711 for 6 qubits with 10 layers\n",
      "Step 170: Loss = -4.999999959413401 for 6 qubits with 10 layers\n",
      "Step 180: Loss = -4.99999999038973 for 6 qubits with 10 layers\n",
      "Step 190: Loss = -4.99999999183138 for 6 qubits with 10 layers\n",
      "Step 200: Loss = -4.999999998191171 for 6 qubits with 10 layers\n",
      "Step 210: Loss = -4.999999999624488 for 6 qubits with 10 layers\n",
      "Step 220: Loss = -4.999999999665423 for 6 qubits with 10 layers\n",
      "Step 230: Loss = -4.999999999847876 for 6 qubits with 10 layers\n",
      "Step 240: Loss = -4.999999999959067 for 6 qubits with 10 layers\n",
      "Step 250: Loss = -4.999999999983573 for 6 qubits with 10 layers\n",
      "Step 260: Loss = -4.999999999996445 for 6 qubits with 10 layers\n",
      "Step 270: Loss = -4.999999999998419 for 6 qubits with 10 layers\n",
      "Step 280: Loss = -4.999999999999276 for 6 qubits with 10 layers\n",
      "Step 290: Loss = -4.999999999999598 for 6 qubits with 10 layers\n",
      "Step 300: Loss = -4.999999999999831 for 6 qubits with 10 layers\n",
      "Step 310: Loss = -4.999999999999852 for 6 qubits with 10 layers\n",
      "Step 320: Loss = -4.999999999999896 for 6 qubits with 10 layers\n",
      "Step 330: Loss = -4.99999999999229 for 6 qubits with 10 layers\n",
      "Step 340: Loss = -4.9999983898008 for 6 qubits with 10 layers\n",
      "Step 350: Loss = -4.98950791407704 for 6 qubits with 10 layers\n",
      "Step 360: Loss = -4.99626424655275 for 6 qubits with 10 layers\n",
      "Step 370: Loss = -4.999883667053186 for 6 qubits with 10 layers\n",
      "Step 380: Loss = -4.999407366083911 for 6 qubits with 10 layers\n",
      "Step 390: Loss = -4.999949883573302 for 6 qubits with 10 layers\n",
      "Step 400: Loss = -4.999998824270777 for 6 qubits with 10 layers\n",
      "Step 410: Loss = -4.999996851875108 for 6 qubits with 10 layers\n",
      "Step 420: Loss = -4.999989381224494 for 6 qubits with 10 layers\n",
      "Step 430: Loss = -4.999999921989722 for 6 qubits with 10 layers\n",
      "Step 440: Loss = -4.999999492386372 for 6 qubits with 10 layers\n",
      "Step 450: Loss = -4.999999990159708 for 6 qubits with 10 layers\n",
      "Step 460: Loss = -4.999998948448495 for 6 qubits with 10 layers\n",
      "Step 470: Loss = -4.994379920865737 for 6 qubits with 10 layers\n",
      "Step 480: Loss = -4.9993549207816255 for 6 qubits with 10 layers\n",
      "Step 490: Loss = -4.999410445219774 for 6 qubits with 10 layers\n",
      "Running optimization for 6 qubits and 11 layers\n",
      "Step 0: Loss = -1.9967594090858682 for 6 qubits with 11 layers\n",
      "Step 10: Loss = -4.099231749290993 for 6 qubits with 11 layers\n",
      "Step 20: Loss = -4.744162192631149 for 6 qubits with 11 layers\n",
      "Step 30: Loss = -4.9756173982503205 for 6 qubits with 11 layers\n",
      "Step 40: Loss = -4.986924582931232 for 6 qubits with 11 layers\n",
      "Step 50: Loss = -4.996483275487739 for 6 qubits with 11 layers\n",
      "Step 60: Loss = -4.998313392702897 for 6 qubits with 11 layers\n",
      "Step 70: Loss = -4.999608515835295 for 6 qubits with 11 layers\n",
      "Step 80: Loss = -4.999811834420779 for 6 qubits with 11 layers\n",
      "Step 90: Loss = -4.999897972218426 for 6 qubits with 11 layers\n",
      "Step 100: Loss = -4.999987707461361 for 6 qubits with 11 layers\n",
      "Step 110: Loss = -4.999990276086175 for 6 qubits with 11 layers\n",
      "Step 120: Loss = -4.999996111869457 for 6 qubits with 11 layers\n",
      "Step 130: Loss = -4.9999980888638795 for 6 qubits with 11 layers\n",
      "Step 140: Loss = -4.999999681112338 for 6 qubits with 11 layers\n",
      "Step 150: Loss = -4.999999919326965 for 6 qubits with 11 layers\n",
      "Step 160: Loss = -4.999999929447254 for 6 qubits with 11 layers\n",
      "Step 170: Loss = -4.9999999772651975 for 6 qubits with 11 layers\n",
      "Step 180: Loss = -4.999999991353994 for 6 qubits with 11 layers\n",
      "Step 190: Loss = -4.999999995097566 for 6 qubits with 11 layers\n",
      "Step 200: Loss = -4.999999967134176 for 6 qubits with 11 layers\n",
      "Step 210: Loss = -4.99874294003139 for 6 qubits with 11 layers\n",
      "Step 220: Loss = -4.991719046064801 for 6 qubits with 11 layers\n",
      "Step 230: Loss = -4.996357793279526 for 6 qubits with 11 layers\n",
      "Step 240: Loss = -4.999217804484908 for 6 qubits with 11 layers\n",
      "Step 250: Loss = -4.999981310064468 for 6 qubits with 11 layers\n",
      "Step 260: Loss = -4.9998273152536425 for 6 qubits with 11 layers\n",
      "Step 270: Loss = -4.999921701669499 for 6 qubits with 11 layers\n",
      "Step 280: Loss = -4.999973777662202 for 6 qubits with 11 layers\n",
      "Step 290: Loss = -4.999995746256292 for 6 qubits with 11 layers\n",
      "Step 300: Loss = -4.999997732829129 for 6 qubits with 11 layers\n",
      "Step 310: Loss = -4.999999105466655 for 6 qubits with 11 layers\n",
      "Step 320: Loss = -4.999977294521707 for 6 qubits with 11 layers\n",
      "Step 330: Loss = -4.986167171831778 for 6 qubits with 11 layers\n",
      "Step 340: Loss = -4.999956692091968 for 6 qubits with 11 layers\n",
      "Step 350: Loss = -4.999750970029202 for 6 qubits with 11 layers\n",
      "Step 360: Loss = -4.999406262571861 for 6 qubits with 11 layers\n",
      "Step 370: Loss = -4.999999398470644 for 6 qubits with 11 layers\n",
      "Step 380: Loss = -4.999997171146585 for 6 qubits with 11 layers\n",
      "Step 390: Loss = -4.999935150916004 for 6 qubits with 11 layers\n",
      "Step 400: Loss = -4.999940637156208 for 6 qubits with 11 layers\n",
      "Step 410: Loss = -4.990189100567707 for 6 qubits with 11 layers\n",
      "Step 420: Loss = -4.997156928646001 for 6 qubits with 11 layers\n",
      "Step 430: Loss = -4.99998525934236 for 6 qubits with 11 layers\n",
      "Step 440: Loss = -4.999926330888722 for 6 qubits with 11 layers\n",
      "Step 450: Loss = -4.999974126315433 for 6 qubits with 11 layers\n",
      "Step 460: Loss = -4.9999876287681015 for 6 qubits with 11 layers\n",
      "Step 470: Loss = -4.998465064276331 for 6 qubits with 11 layers\n",
      "Step 480: Loss = -4.997510855253697 for 6 qubits with 11 layers\n",
      "Step 490: Loss = -4.999983763962863 for 6 qubits with 11 layers\n",
      "Running optimization for 6 qubits and 12 layers\n",
      "Step 0: Loss = -3.6569431101698906 for 6 qubits with 12 layers\n",
      "Step 10: Loss = -4.867821320452098 for 6 qubits with 12 layers\n",
      "Step 20: Loss = -4.975868786522127 for 6 qubits with 12 layers\n",
      "Step 30: Loss = -4.988771893789375 for 6 qubits with 12 layers\n",
      "Step 40: Loss = -4.994914177374168 for 6 qubits with 12 layers\n",
      "Step 50: Loss = -4.999389700944688 for 6 qubits with 12 layers\n",
      "Step 60: Loss = -4.999194047261171 for 6 qubits with 12 layers\n",
      "Step 70: Loss = -4.999565698070347 for 6 qubits with 12 layers\n",
      "Step 80: Loss = -4.999911270425539 for 6 qubits with 12 layers\n",
      "Step 90: Loss = -4.999948388776838 for 6 qubits with 12 layers\n",
      "Step 100: Loss = -4.999991997055453 for 6 qubits with 12 layers\n",
      "Step 110: Loss = -4.999998198908369 for 6 qubits with 12 layers\n",
      "Step 120: Loss = -4.999998899862145 for 6 qubits with 12 layers\n",
      "Step 130: Loss = -4.999999579268554 for 6 qubits with 12 layers\n",
      "Step 140: Loss = -4.999999936043447 for 6 qubits with 12 layers\n",
      "Step 150: Loss = -4.999999961486755 for 6 qubits with 12 layers\n",
      "Step 160: Loss = -4.99999996709885 for 6 qubits with 12 layers\n",
      "Step 170: Loss = -4.999999991625996 for 6 qubits with 12 layers\n",
      "Step 180: Loss = -4.999999993463553 for 6 qubits with 12 layers\n",
      "Step 190: Loss = -4.999999998675715 for 6 qubits with 12 layers\n",
      "Step 200: Loss = -4.999999999806148 for 6 qubits with 12 layers\n",
      "Step 210: Loss = -4.999999999875539 for 6 qubits with 12 layers\n",
      "Step 220: Loss = -4.999999999953857 for 6 qubits with 12 layers\n",
      "Step 230: Loss = -4.99999999997561 for 6 qubits with 12 layers\n",
      "Step 240: Loss = -4.999999999989138 for 6 qubits with 12 layers\n",
      "Step 250: Loss = -4.999999999996718 for 6 qubits with 12 layers\n",
      "Step 260: Loss = -4.99999999999877 for 6 qubits with 12 layers\n",
      "Step 270: Loss = -4.999999999916603 for 6 qubits with 12 layers\n",
      "Step 280: Loss = -4.999992040232161 for 6 qubits with 12 layers\n",
      "Step 290: Loss = -4.992305522867085 for 6 qubits with 12 layers\n",
      "Step 300: Loss = -4.998979427817063 for 6 qubits with 12 layers\n",
      "Step 310: Loss = -4.999812264734352 for 6 qubits with 12 layers\n",
      "Step 320: Loss = -4.999646429195251 for 6 qubits with 12 layers\n",
      "Step 330: Loss = -4.999961181662804 for 6 qubits with 12 layers\n",
      "Step 340: Loss = -4.999969714348457 for 6 qubits with 12 layers\n",
      "Step 350: Loss = -4.999989575569357 for 6 qubits with 12 layers\n",
      "Step 360: Loss = -4.999994366412683 for 6 qubits with 12 layers\n",
      "Step 370: Loss = -4.999998709177206 for 6 qubits with 12 layers\n",
      "Step 380: Loss = -4.99999816183689 for 6 qubits with 12 layers\n",
      "Step 390: Loss = -4.999998877367137 for 6 qubits with 12 layers\n",
      "Step 400: Loss = -4.999427481841073 for 6 qubits with 12 layers\n",
      "Step 410: Loss = -4.999452913363831 for 6 qubits with 12 layers\n",
      "Step 420: Loss = -4.9996653948176295 for 6 qubits with 12 layers\n",
      "Step 430: Loss = -4.999864483952829 for 6 qubits with 12 layers\n",
      "Step 440: Loss = -4.99983273557002 for 6 qubits with 12 layers\n",
      "Step 450: Loss = -4.999968887757566 for 6 qubits with 12 layers\n",
      "Step 460: Loss = -4.999938462331724 for 6 qubits with 12 layers\n",
      "Step 470: Loss = -4.99956055033531 for 6 qubits with 12 layers\n",
      "Step 480: Loss = -4.999349689569591 for 6 qubits with 12 layers\n",
      "Step 490: Loss = -4.999587031308877 for 6 qubits with 12 layers\n",
      "Running optimization for 6 qubits and 13 layers\n",
      "Step 0: Loss = -2.1992517806400107 for 6 qubits with 13 layers\n",
      "Step 10: Loss = -4.678354562101299 for 6 qubits with 13 layers\n",
      "Step 20: Loss = -4.89968621451499 for 6 qubits with 13 layers\n",
      "Step 30: Loss = -4.975638076453179 for 6 qubits with 13 layers\n",
      "Step 40: Loss = -4.992084393478196 for 6 qubits with 13 layers\n",
      "Step 50: Loss = -4.997998754439547 for 6 qubits with 13 layers\n",
      "Step 60: Loss = -4.998815752496876 for 6 qubits with 13 layers\n",
      "Step 70: Loss = -4.9994231751840275 for 6 qubits with 13 layers\n",
      "Step 80: Loss = -4.999818158120535 for 6 qubits with 13 layers\n",
      "Step 90: Loss = -4.999933630311206 for 6 qubits with 13 layers\n",
      "Step 100: Loss = -4.9999765677036025 for 6 qubits with 13 layers\n",
      "Step 110: Loss = -4.999991839523923 for 6 qubits with 13 layers\n",
      "Step 120: Loss = -4.999997411353804 for 6 qubits with 13 layers\n",
      "Step 130: Loss = -4.999998709502736 for 6 qubits with 13 layers\n",
      "Step 140: Loss = -4.999999484554807 for 6 qubits with 13 layers\n",
      "Step 150: Loss = -4.999999877484928 for 6 qubits with 13 layers\n",
      "Step 160: Loss = -4.999999973737879 for 6 qubits with 13 layers\n",
      "Step 170: Loss = -4.9999999871242755 for 6 qubits with 13 layers\n",
      "Step 180: Loss = -4.999999995760116 for 6 qubits with 13 layers\n",
      "Step 190: Loss = -4.999999996618605 for 6 qubits with 13 layers\n",
      "Step 200: Loss = -4.999999999278604 for 6 qubits with 13 layers\n",
      "Step 210: Loss = -4.999999999566795 for 6 qubits with 13 layers\n",
      "Step 220: Loss = -4.999999999928921 for 6 qubits with 13 layers\n",
      "Step 230: Loss = -4.999999999930421 for 6 qubits with 13 layers\n",
      "Step 240: Loss = -4.9999999999818145 for 6 qubits with 13 layers\n",
      "Step 250: Loss = -4.999999999995429 for 6 qubits with 13 layers\n",
      "Step 260: Loss = -4.99999999999743 for 6 qubits with 13 layers\n",
      "Step 270: Loss = -4.999999999998787 for 6 qubits with 13 layers\n",
      "Step 280: Loss = -4.999999999999536 for 6 qubits with 13 layers\n",
      "Step 290: Loss = -4.999999999999689 for 6 qubits with 13 layers\n",
      "Step 300: Loss = -4.999999999999827 for 6 qubits with 13 layers\n",
      "Step 310: Loss = -4.99999999999986 for 6 qubits with 13 layers\n",
      "Step 320: Loss = -4.999999999999866 for 6 qubits with 13 layers\n",
      "Step 330: Loss = -4.999999999999871 for 6 qubits with 13 layers\n",
      "Step 340: Loss = -4.999999999999837 for 6 qubits with 13 layers\n",
      "Step 350: Loss = -4.999999999993348 for 6 qubits with 13 layers\n",
      "Step 360: Loss = -4.999996814104389 for 6 qubits with 13 layers\n",
      "Step 370: Loss = -4.985153043640031 for 6 qubits with 13 layers\n",
      "Step 380: Loss = -4.999179620541336 for 6 qubits with 13 layers\n",
      "Step 390: Loss = -4.998104961836294 for 6 qubits with 13 layers\n",
      "Step 400: Loss = -4.999978519432193 for 6 qubits with 13 layers\n",
      "Step 410: Loss = -4.999806651136054 for 6 qubits with 13 layers\n",
      "Step 420: Loss = -4.999904675559648 for 6 qubits with 13 layers\n",
      "Step 430: Loss = -4.999963762465479 for 6 qubits with 13 layers\n",
      "Step 440: Loss = -4.999990533092492 for 6 qubits with 13 layers\n",
      "Step 450: Loss = -4.999998763404485 for 6 qubits with 13 layers\n",
      "Step 460: Loss = -4.99999983201762 for 6 qubits with 13 layers\n",
      "Step 470: Loss = -4.9999999723181645 for 6 qubits with 13 layers\n",
      "Step 480: Loss = -4.999999725797382 for 6 qubits with 13 layers\n",
      "Step 490: Loss = -4.999999961735853 for 6 qubits with 13 layers\n",
      "Running optimization for 6 qubits and 14 layers\n",
      "Step 0: Loss = -1.8903457314515921 for 6 qubits with 14 layers\n",
      "Step 10: Loss = -4.676342191291646 for 6 qubits with 14 layers\n",
      "Step 20: Loss = -4.932476402767883 for 6 qubits with 14 layers\n",
      "Step 30: Loss = -4.978470977495509 for 6 qubits with 14 layers\n",
      "Step 40: Loss = -4.992058673716775 for 6 qubits with 14 layers\n",
      "Step 50: Loss = -4.995527866245053 for 6 qubits with 14 layers\n",
      "Step 60: Loss = -4.999067938872148 for 6 qubits with 14 layers\n",
      "Step 70: Loss = -4.999689259051225 for 6 qubits with 14 layers\n",
      "Step 80: Loss = -4.999905889265922 for 6 qubits with 14 layers\n",
      "Step 90: Loss = -4.999963584091029 for 6 qubits with 14 layers\n",
      "Step 100: Loss = -4.999983922089191 for 6 qubits with 14 layers\n",
      "Step 110: Loss = -4.9999915251455205 for 6 qubits with 14 layers\n",
      "Step 120: Loss = -4.999996838169604 for 6 qubits with 14 layers\n",
      "Step 130: Loss = -4.999999494607212 for 6 qubits with 14 layers\n",
      "Step 140: Loss = -4.999999427333018 for 6 qubits with 14 layers\n",
      "Step 150: Loss = -4.999999922773158 for 6 qubits with 14 layers\n",
      "Step 160: Loss = -4.999999951906115 for 6 qubits with 14 layers\n",
      "Step 170: Loss = -4.999999972907006 for 6 qubits with 14 layers\n",
      "Step 180: Loss = -4.999999993521389 for 6 qubits with 14 layers\n",
      "Step 190: Loss = -4.999999998275886 for 6 qubits with 14 layers\n",
      "Step 200: Loss = -4.999999999046895 for 6 qubits with 14 layers\n",
      "Step 210: Loss = -4.99999999957138 for 6 qubits with 14 layers\n",
      "Step 220: Loss = -4.999999999827602 for 6 qubits with 14 layers\n",
      "Step 230: Loss = -4.999999999971966 for 6 qubits with 14 layers\n",
      "Step 240: Loss = -4.999999999984263 for 6 qubits with 14 layers\n",
      "Step 250: Loss = -4.999999999995592 for 6 qubits with 14 layers\n",
      "Step 260: Loss = -4.999999999996742 for 6 qubits with 14 layers\n",
      "Step 270: Loss = -4.999999999999082 for 6 qubits with 14 layers\n",
      "Step 280: Loss = -4.999999999999743 for 6 qubits with 14 layers\n",
      "Step 290: Loss = -4.999999999998392 for 6 qubits with 14 layers\n",
      "Step 300: Loss = -4.999999898129362 for 6 qubits with 14 layers\n",
      "Step 310: Loss = -4.977903002064095 for 6 qubits with 14 layers\n",
      "Step 320: Loss = -4.997186839423637 for 6 qubits with 14 layers\n",
      "Step 330: Loss = -4.998448801393318 for 6 qubits with 14 layers\n",
      "Step 340: Loss = -4.999307943841823 for 6 qubits with 14 layers\n",
      "Step 350: Loss = -4.999839097695615 for 6 qubits with 14 layers\n",
      "Step 360: Loss = -4.999992983291761 for 6 qubits with 14 layers\n",
      "Step 370: Loss = -4.999958034561357 for 6 qubits with 14 layers\n",
      "Step 380: Loss = -4.999981192579412 for 6 qubits with 14 layers\n",
      "Step 390: Loss = -4.999993107107092 for 6 qubits with 14 layers\n",
      "Step 400: Loss = -4.999999461598717 for 6 qubits with 14 layers\n",
      "Step 410: Loss = -4.99999918788816 for 6 qubits with 14 layers\n",
      "Step 420: Loss = -4.997604513134582 for 6 qubits with 14 layers\n",
      "Step 430: Loss = -4.999889554384819 for 6 qubits with 14 layers\n",
      "Step 440: Loss = -4.998651770289177 for 6 qubits with 14 layers\n",
      "Step 450: Loss = -4.999693247553579 for 6 qubits with 14 layers\n",
      "Step 460: Loss = -4.999649148860719 for 6 qubits with 14 layers\n",
      "Step 470: Loss = -4.998667794677108 for 6 qubits with 14 layers\n",
      "Step 480: Loss = -4.999585468461701 for 6 qubits with 14 layers\n",
      "Step 490: Loss = -4.9997013155111425 for 6 qubits with 14 layers\n",
      "Running optimization for 6 qubits and 15 layers\n",
      "Step 0: Loss = -1.8452386959919562 for 6 qubits with 15 layers\n",
      "Step 10: Loss = -4.747527742104989 for 6 qubits with 15 layers\n",
      "Step 20: Loss = -4.822177251293236 for 6 qubits with 15 layers\n",
      "Step 30: Loss = -4.9649760658461854 for 6 qubits with 15 layers\n",
      "Step 40: Loss = -4.980461572610983 for 6 qubits with 15 layers\n",
      "Step 50: Loss = -4.99496261887519 for 6 qubits with 15 layers\n",
      "Step 60: Loss = -4.997568823821986 for 6 qubits with 15 layers\n",
      "Step 70: Loss = -4.999030209967128 for 6 qubits with 15 layers\n",
      "Step 80: Loss = -4.999828964755082 for 6 qubits with 15 layers\n",
      "Step 90: Loss = -4.999899241471972 for 6 qubits with 15 layers\n",
      "Step 100: Loss = -4.999937518671316 for 6 qubits with 15 layers\n",
      "Step 110: Loss = -4.999983075783645 for 6 qubits with 15 layers\n",
      "Step 120: Loss = -4.999996637316352 for 6 qubits with 15 layers\n",
      "Step 130: Loss = -4.999997499934912 for 6 qubits with 15 layers\n",
      "Step 140: Loss = -4.99999915469062 for 6 qubits with 15 layers\n",
      "Step 150: Loss = -4.999999669049542 for 6 qubits with 15 layers\n",
      "Step 160: Loss = -4.999999879296773 for 6 qubits with 15 layers\n",
      "Step 170: Loss = -4.9999999743283 for 6 qubits with 15 layers\n",
      "Step 180: Loss = -4.999999992792451 for 6 qubits with 15 layers\n",
      "Step 190: Loss = -4.999999997442742 for 6 qubits with 15 layers\n",
      "Step 200: Loss = -4.999999998517978 for 6 qubits with 15 layers\n",
      "Step 210: Loss = -4.99999999969192 for 6 qubits with 15 layers\n",
      "Step 220: Loss = -4.9999999998204085 for 6 qubits with 15 layers\n",
      "Step 230: Loss = -4.999999999916083 for 6 qubits with 15 layers\n",
      "Step 240: Loss = -4.999999999977271 for 6 qubits with 15 layers\n",
      "Step 250: Loss = -4.99999999998628 for 6 qubits with 15 layers\n",
      "Step 260: Loss = -4.999999999997398 for 6 qubits with 15 layers\n",
      "Step 270: Loss = -4.999999999997259 for 6 qubits with 15 layers\n",
      "Step 280: Loss = -4.999999999997902 for 6 qubits with 15 layers\n",
      "Step 290: Loss = -4.999999998987212 for 6 qubits with 15 layers\n",
      "Step 300: Loss = -4.9999359602232545 for 6 qubits with 15 layers\n",
      "Step 310: Loss = -4.98312922445987 for 6 qubits with 15 layers\n",
      "Step 320: Loss = -4.996844380314792 for 6 qubits with 15 layers\n",
      "Step 330: Loss = -4.999386819105686 for 6 qubits with 15 layers\n",
      "Step 340: Loss = -4.998677131017793 for 6 qubits with 15 layers\n",
      "Step 350: Loss = -4.999411718502156 for 6 qubits with 15 layers\n",
      "Step 360: Loss = -4.999837918271404 for 6 qubits with 15 layers\n",
      "Step 370: Loss = -4.999998801814386 for 6 qubits with 15 layers\n",
      "Step 380: Loss = -4.999972468524656 for 6 qubits with 15 layers\n",
      "Step 390: Loss = -4.9999871259627735 for 6 qubits with 15 layers\n",
      "Step 400: Loss = -4.999997472311204 for 6 qubits with 15 layers\n",
      "Step 410: Loss = -4.999998208888225 for 6 qubits with 15 layers\n",
      "Step 420: Loss = -4.999999043840825 for 6 qubits with 15 layers\n",
      "Step 430: Loss = -4.999999948082646 for 6 qubits with 15 layers\n",
      "Step 440: Loss = -4.999996449864984 for 6 qubits with 15 layers\n",
      "Step 450: Loss = -4.991147664736052 for 6 qubits with 15 layers\n",
      "Step 460: Loss = -4.988562602121409 for 6 qubits with 15 layers\n",
      "Step 470: Loss = -4.999157621762209 for 6 qubits with 15 layers\n",
      "Step 480: Loss = -4.999583891483258 for 6 qubits with 15 layers\n",
      "Step 490: Loss = -4.999564886995403 for 6 qubits with 15 layers\n",
      "Running optimization for 7 qubits and 1 layers\n",
      "Step 0: Loss = -3.426109901563752 for 7 qubits with 1 layers\n",
      "Step 10: Loss = -4.5451135234225175 for 7 qubits with 1 layers\n",
      "Step 20: Loss = -5.3948960957301315 for 7 qubits with 1 layers\n",
      "Step 30: Loss = -5.855971515782556 for 7 qubits with 1 layers\n",
      "Step 40: Loss = -5.993032827243292 for 7 qubits with 1 layers\n",
      "Step 50: Loss = -5.995323901503974 for 7 qubits with 1 layers\n",
      "Step 60: Loss = -5.9910375975154215 for 7 qubits with 1 layers\n",
      "Step 70: Loss = -5.996970734715762 for 7 qubits with 1 layers\n",
      "Step 80: Loss = -5.999928925591627 for 7 qubits with 1 layers\n",
      "Step 90: Loss = -5.999794588252376 for 7 qubits with 1 layers\n",
      "Step 100: Loss = -5.9998263865192945 for 7 qubits with 1 layers\n",
      "Step 110: Loss = -5.999989570898558 for 7 qubits with 1 layers\n",
      "Step 120: Loss = -5.999991382121814 for 7 qubits with 1 layers\n",
      "Step 130: Loss = -5.999992220756124 for 7 qubits with 1 layers\n",
      "Step 140: Loss = -5.999999908938532 for 7 qubits with 1 layers\n",
      "Step 150: Loss = -5.999999216272222 for 7 qubits with 1 layers\n",
      "Step 160: Loss = -5.999999794081441 for 7 qubits with 1 layers\n",
      "Step 170: Loss = -5.99999997127666 for 7 qubits with 1 layers\n",
      "Step 180: Loss = -5.999999955107054 for 7 qubits with 1 layers\n",
      "Step 190: Loss = -5.999999999818983 for 7 qubits with 1 layers\n",
      "Step 200: Loss = -5.999999993817008 for 7 qubits with 1 layers\n",
      "Step 210: Loss = -5.9999999999836175 for 7 qubits with 1 layers\n",
      "Step 220: Loss = -5.999999999217556 for 7 qubits with 1 layers\n",
      "Step 230: Loss = -5.9999999999980815 for 7 qubits with 1 layers\n",
      "Step 240: Loss = -5.999999999898698 for 7 qubits with 1 layers\n",
      "Step 250: Loss = -5.999999999999456 for 7 qubits with 1 layers\n",
      "Step 260: Loss = -5.999999999988158 for 7 qubits with 1 layers\n",
      "Step 270: Loss = -5.999999999998806 for 7 qubits with 1 layers\n",
      "Step 280: Loss = -5.999999999999192 for 7 qubits with 1 layers\n",
      "Step 290: Loss = -5.999999999999489 for 7 qubits with 1 layers\n",
      "Step 300: Loss = -5.99999999999999 for 7 qubits with 1 layers\n",
      "Step 310: Loss = -5.9999999999999245 for 7 qubits with 1 layers\n",
      "Step 320: Loss = -5.999999999999972 for 7 qubits with 1 layers\n",
      "Step 330: Loss = -5.999999999999988 for 7 qubits with 1 layers\n",
      "Step 340: Loss = -5.999999999999985 for 7 qubits with 1 layers\n",
      "Step 350: Loss = -5.999999999999966 for 7 qubits with 1 layers\n",
      "Step 360: Loss = -5.999999999999983 for 7 qubits with 1 layers\n",
      "Step 370: Loss = -5.999999999999988 for 7 qubits with 1 layers\n",
      "Step 380: Loss = -5.999999999999979 for 7 qubits with 1 layers\n",
      "Step 390: Loss = -5.999999999999974 for 7 qubits with 1 layers\n",
      "Step 400: Loss = -5.99999999999997 for 7 qubits with 1 layers\n",
      "Step 410: Loss = -5.999999999999972 for 7 qubits with 1 layers\n",
      "Step 420: Loss = -5.9999999999999885 for 7 qubits with 1 layers\n",
      "Step 430: Loss = -5.999999999999963 for 7 qubits with 1 layers\n",
      "Step 440: Loss = -5.999999999999983 for 7 qubits with 1 layers\n",
      "Step 450: Loss = -5.99999999999999 for 7 qubits with 1 layers\n",
      "Step 460: Loss = -5.999999999999967 for 7 qubits with 1 layers\n",
      "Step 470: Loss = -5.999999999999993 for 7 qubits with 1 layers\n",
      "Step 480: Loss = -5.99999999999998 for 7 qubits with 1 layers\n",
      "Step 490: Loss = -5.999999999999978 for 7 qubits with 1 layers\n",
      "Running optimization for 7 qubits and 2 layers\n",
      "Step 0: Loss = -0.2380927318640425 for 7 qubits with 2 layers\n",
      "Step 10: Loss = -1.8534792395730435 for 7 qubits with 2 layers\n",
      "Step 20: Loss = -3.465547312244715 for 7 qubits with 2 layers\n",
      "Step 30: Loss = -4.098472946083899 for 7 qubits with 2 layers\n",
      "Step 40: Loss = -4.792726272732273 for 7 qubits with 2 layers\n",
      "Step 50: Loss = -5.479534625508757 for 7 qubits with 2 layers\n",
      "Step 60: Loss = -5.809949376622807 for 7 qubits with 2 layers\n",
      "Step 70: Loss = -5.946711160143924 for 7 qubits with 2 layers\n",
      "Step 80: Loss = -5.978747558816571 for 7 qubits with 2 layers\n",
      "Step 90: Loss = -5.992037384125664 for 7 qubits with 2 layers\n",
      "Step 100: Loss = -5.995405793987104 for 7 qubits with 2 layers\n",
      "Step 110: Loss = -5.997264416278974 for 7 qubits with 2 layers\n",
      "Step 120: Loss = -5.998001261008832 for 7 qubits with 2 layers\n",
      "Step 130: Loss = -5.998509170397909 for 7 qubits with 2 layers\n",
      "Step 140: Loss = -5.9988260893206204 for 7 qubits with 2 layers\n",
      "Step 150: Loss = -5.999055012537413 for 7 qubits with 2 layers\n",
      "Step 160: Loss = -5.999230029867016 for 7 qubits with 2 layers\n",
      "Step 170: Loss = -5.999364921473983 for 7 qubits with 2 layers\n",
      "Step 180: Loss = -5.999472163704983 for 7 qubits with 2 layers\n",
      "Step 190: Loss = -5.999558447236224 for 7 qubits with 2 layers\n",
      "Step 200: Loss = -5.999628454114961 for 7 qubits with 2 layers\n",
      "Step 210: Loss = -5.999685757663048 for 7 qubits with 2 layers\n",
      "Step 220: Loss = -5.99973300077046 for 7 qubits with 2 layers\n",
      "Step 230: Loss = -5.999772186087278 for 7 qubits with 2 layers\n",
      "Step 240: Loss = -5.999804870705619 for 7 qubits with 2 layers\n",
      "Step 250: Loss = -5.999832273750103 for 7 qubits with 2 layers\n",
      "Step 260: Loss = -5.999855357611599 for 7 qubits with 2 layers\n",
      "Step 270: Loss = -5.999874888338117 for 7 qubits with 2 layers\n",
      "Step 280: Loss = -5.999891479886769 for 7 qubits with 2 layers\n",
      "Step 290: Loss = -5.999905627752123 for 7 qubits with 2 layers\n",
      "Step 300: Loss = -5.999917734164945 for 7 qubits with 2 layers\n",
      "Step 310: Loss = -5.999928127557896 for 7 qubits with 2 layers\n",
      "Step 320: Loss = -5.9999370775150505 for 7 qubits with 2 layers\n",
      "Step 330: Loss = -5.999944806440441 for 7 qubits with 2 layers\n",
      "Step 340: Loss = -5.999951498676878 for 7 qubits with 2 layers\n",
      "Step 350: Loss = -5.999957307703523 for 7 qubits with 2 layers\n",
      "Step 360: Loss = -5.999962361839179 for 7 qubits with 2 layers\n",
      "Step 370: Loss = -5.999966768793297 for 7 qubits with 2 layers\n",
      "Step 380: Loss = -5.999970619315253 for 7 qubits with 2 layers\n",
      "Step 390: Loss = -5.999973990136199 for 7 qubits with 2 layers\n",
      "Step 400: Loss = -5.999976946352407 for 7 qubits with 2 layers\n",
      "Step 410: Loss = -5.999979543365268 for 7 qubits with 2 layers\n",
      "Step 420: Loss = -5.999981828467193 for 7 qubits with 2 layers\n",
      "Step 430: Loss = -5.99998384214407 for 7 qubits with 2 layers\n",
      "Step 440: Loss = -5.999985619148792 for 7 qubits with 2 layers\n",
      "Step 450: Loss = -5.99998718938997 for 7 qubits with 2 layers\n",
      "Step 460: Loss = -5.999988578669931 for 7 qubits with 2 layers\n",
      "Step 470: Loss = -5.9999898093003665 for 7 qubits with 2 layers\n",
      "Step 480: Loss = -5.999990900617011 for 7 qubits with 2 layers\n",
      "Step 490: Loss = -5.999991869411759 for 7 qubits with 2 layers\n",
      "Running optimization for 7 qubits and 3 layers\n",
      "Step 0: Loss = -2.7186270185408086 for 7 qubits with 3 layers\n",
      "Step 10: Loss = -4.727775332474416 for 7 qubits with 3 layers\n",
      "Step 20: Loss = -5.259956934091596 for 7 qubits with 3 layers\n",
      "Step 30: Loss = -5.393784537157696 for 7 qubits with 3 layers\n",
      "Step 40: Loss = -5.4434405317301415 for 7 qubits with 3 layers\n",
      "Step 50: Loss = -5.450617730527501 for 7 qubits with 3 layers\n",
      "Step 60: Loss = -5.462618907818264 for 7 qubits with 3 layers\n",
      "Step 70: Loss = -5.46334706916734 for 7 qubits with 3 layers\n",
      "Step 80: Loss = -5.4650199255189245 for 7 qubits with 3 layers\n",
      "Step 90: Loss = -5.465280606247578 for 7 qubits with 3 layers\n",
      "Step 100: Loss = -5.465324430309025 for 7 qubits with 3 layers\n",
      "Step 110: Loss = -5.4653743900558025 for 7 qubits with 3 layers\n",
      "Step 120: Loss = -5.465388376753887 for 7 qubits with 3 layers\n",
      "Step 130: Loss = -5.465392963925831 for 7 qubits with 3 layers\n",
      "Step 140: Loss = -5.4653937952114955 for 7 qubits with 3 layers\n",
      "Step 150: Loss = -5.4653944072883105 for 7 qubits with 3 layers\n",
      "Step 160: Loss = -5.465394816133195 for 7 qubits with 3 layers\n",
      "Step 170: Loss = -5.465394905108471 for 7 qubits with 3 layers\n",
      "Step 180: Loss = -5.465394963648345 for 7 qubits with 3 layers\n",
      "Step 190: Loss = -5.465394965125966 for 7 qubits with 3 layers\n",
      "Step 200: Loss = -5.465394967088573 for 7 qubits with 3 layers\n",
      "Step 210: Loss = -5.4653949693112605 for 7 qubits with 3 layers\n",
      "Step 220: Loss = -5.465394969455995 for 7 qubits with 3 layers\n",
      "Step 230: Loss = -5.46539496959915 for 7 qubits with 3 layers\n",
      "Step 240: Loss = -5.465394969668781 for 7 qubits with 3 layers\n",
      "Step 250: Loss = -5.465394969708659 for 7 qubits with 3 layers\n",
      "Step 260: Loss = -5.4653949697101005 for 7 qubits with 3 layers\n",
      "Step 270: Loss = -5.465394969712598 for 7 qubits with 3 layers\n",
      "Step 280: Loss = -5.465394969714628 for 7 qubits with 3 layers\n",
      "Step 290: Loss = -5.465394969714972 for 7 qubits with 3 layers\n",
      "Step 300: Loss = -5.4653949697151 for 7 qubits with 3 layers\n",
      "Step 310: Loss = -5.465394969715187 for 7 qubits with 3 layers\n",
      "Step 320: Loss = -5.465394969715197 for 7 qubits with 3 layers\n",
      "Step 330: Loss = -5.465394969715211 for 7 qubits with 3 layers\n",
      "Step 340: Loss = -5.46539496971521 for 7 qubits with 3 layers\n",
      "Step 350: Loss = -5.46539496971521 for 7 qubits with 3 layers\n",
      "Step 360: Loss = -5.465394969715228 for 7 qubits with 3 layers\n",
      "Step 370: Loss = -5.465394969715213 for 7 qubits with 3 layers\n",
      "Step 380: Loss = -5.46539496971523 for 7 qubits with 3 layers\n",
      "Step 390: Loss = -5.4653949697152155 for 7 qubits with 3 layers\n",
      "Step 400: Loss = -5.465394969715205 for 7 qubits with 3 layers\n",
      "Step 410: Loss = -5.4653949697152076 for 7 qubits with 3 layers\n",
      "Step 420: Loss = -5.465394969715212 for 7 qubits with 3 layers\n",
      "Step 430: Loss = -5.465394969714545 for 7 qubits with 3 layers\n",
      "Step 440: Loss = -5.465393878571911 for 7 qubits with 3 layers\n",
      "Step 450: Loss = -5.465040066099725 for 7 qubits with 3 layers\n",
      "Step 460: Loss = -5.4653354228210524 for 7 qubits with 3 layers\n",
      "Step 470: Loss = -5.465370031526507 for 7 qubits with 3 layers\n",
      "Step 480: Loss = -5.465379311897075 for 7 qubits with 3 layers\n",
      "Step 490: Loss = -5.465393311780179 for 7 qubits with 3 layers\n",
      "Running optimization for 7 qubits and 4 layers\n",
      "Step 0: Loss = -2.8825552311840683 for 7 qubits with 4 layers\n",
      "Step 10: Loss = -5.219701097941766 for 7 qubits with 4 layers\n",
      "Step 20: Loss = -5.713075765207291 for 7 qubits with 4 layers\n",
      "Step 30: Loss = -5.877794900520875 for 7 qubits with 4 layers\n",
      "Step 40: Loss = -5.942067837171386 for 7 qubits with 4 layers\n",
      "Step 50: Loss = -5.973657488459682 for 7 qubits with 4 layers\n",
      "Step 60: Loss = -5.9885682162830385 for 7 qubits with 4 layers\n",
      "Step 70: Loss = -5.993965071186748 for 7 qubits with 4 layers\n",
      "Step 80: Loss = -5.9960606290380785 for 7 qubits with 4 layers\n",
      "Step 90: Loss = -5.997109984735237 for 7 qubits with 4 layers\n",
      "Step 100: Loss = -5.997752257662808 for 7 qubits with 4 layers\n",
      "Step 110: Loss = -5.9981808842745306 for 7 qubits with 4 layers\n",
      "Step 120: Loss = -5.998490981583408 for 7 qubits with 4 layers\n",
      "Step 130: Loss = -5.9987293635758565 for 7 qubits with 4 layers\n",
      "Step 140: Loss = -5.998916290981013 for 7 qubits with 4 layers\n",
      "Step 150: Loss = -5.999066465365196 for 7 qubits with 4 layers\n",
      "Step 160: Loss = -5.99918960180128 for 7 qubits with 4 layers\n",
      "Step 170: Loss = -5.999291660603252 for 7 qubits with 4 layers\n",
      "Step 180: Loss = -5.999377183123585 for 7 qubits with 4 layers\n",
      "Step 190: Loss = -5.999449464044619 for 7 qubits with 4 layers\n",
      "Step 200: Loss = -5.9995110268122325 for 7 qubits with 4 layers\n",
      "Step 210: Loss = -5.999563818747177 for 7 qubits with 4 layers\n",
      "Step 220: Loss = -5.999609370742883 for 7 qubits with 4 layers\n",
      "Step 230: Loss = -5.999648899042519 for 7 qubits with 4 layers\n",
      "Step 240: Loss = -5.999683377953467 for 7 qubits with 4 layers\n",
      "Step 250: Loss = -5.999713596498479 for 7 qubits with 4 layers\n",
      "Step 260: Loss = -5.999740197986604 for 7 qubits with 4 layers\n",
      "Step 270: Loss = -5.999763711201343 for 7 qubits with 4 layers\n",
      "Step 280: Loss = -5.999784573633129 for 7 qubits with 4 layers\n",
      "Step 290: Loss = -5.999803149587453 for 7 qubits with 4 layers\n",
      "Step 300: Loss = -5.999819744242436 for 7 qubits with 4 layers\n",
      "Step 310: Loss = -5.999834614650058 for 7 qubits with 4 layers\n",
      "Step 320: Loss = -5.999847978468006 for 7 qubits with 4 layers\n",
      "Step 330: Loss = -5.999860020894026 for 7 qubits with 4 layers\n",
      "Step 340: Loss = -5.999870900234196 for 7 qubits with 4 layers\n",
      "Step 350: Loss = -5.999880752396807 for 7 qubits with 4 layers\n",
      "Step 360: Loss = -5.999889694539315 for 7 qubits with 4 layers\n",
      "Step 370: Loss = -5.999897828047386 for 7 qubits with 4 layers\n",
      "Step 380: Loss = -5.999905240980704 for 7 qubits with 4 layers\n",
      "Step 390: Loss = -5.999912010091083 for 7 qubits with 4 layers\n",
      "Step 400: Loss = -5.9999182024953965 for 7 qubits with 4 layers\n",
      "Step 410: Loss = -5.999923877070025 for 7 qubits with 4 layers\n",
      "Step 420: Loss = -5.999929085616861 for 7 qubits with 4 layers\n",
      "Step 430: Loss = -5.999933873843082 for 7 qubits with 4 layers\n",
      "Step 440: Loss = -5.999938282188164 for 7 qubits with 4 layers\n",
      "Step 450: Loss = -5.999942346522951 for 7 qubits with 4 layers\n",
      "Step 460: Loss = -5.9999460987451645 for 7 qubits with 4 layers\n",
      "Step 470: Loss = -5.999949567286108 for 7 qubits with 4 layers\n",
      "Step 480: Loss = -5.999952777544711 for 7 qubits with 4 layers\n",
      "Step 490: Loss = -5.999955752260107 for 7 qubits with 4 layers\n",
      "Running optimization for 7 qubits and 5 layers\n",
      "Step 0: Loss = -1.8851988117820953 for 7 qubits with 5 layers\n",
      "Step 10: Loss = -5.388189231654187 for 7 qubits with 5 layers\n",
      "Step 20: Loss = -5.602705763319062 for 7 qubits with 5 layers\n",
      "Step 30: Loss = -5.717944881747615 for 7 qubits with 5 layers\n",
      "Step 40: Loss = -5.799001192079858 for 7 qubits with 5 layers\n",
      "Step 50: Loss = -5.818469648089663 for 7 qubits with 5 layers\n",
      "Step 60: Loss = -5.848479930140093 for 7 qubits with 5 layers\n",
      "Step 70: Loss = -5.856607198251375 for 7 qubits with 5 layers\n",
      "Step 80: Loss = -5.857242963788015 for 7 qubits with 5 layers\n",
      "Step 90: Loss = -5.858399722998243 for 7 qubits with 5 layers\n",
      "Step 100: Loss = -5.858901617168832 for 7 qubits with 5 layers\n",
      "Step 110: Loss = -5.859111853560267 for 7 qubits with 5 layers\n",
      "Step 120: Loss = -5.859383406781777 for 7 qubits with 5 layers\n",
      "Step 130: Loss = -5.859652556486248 for 7 qubits with 5 layers\n",
      "Step 140: Loss = -5.85994563263411 for 7 qubits with 5 layers\n",
      "Step 150: Loss = -5.8602773539961595 for 7 qubits with 5 layers\n",
      "Step 160: Loss = -5.860636859774416 for 7 qubits with 5 layers\n",
      "Step 170: Loss = -5.861017170645601 for 7 qubits with 5 layers\n",
      "Step 180: Loss = -5.861402951577477 for 7 qubits with 5 layers\n",
      "Step 190: Loss = -5.86177644049771 for 7 qubits with 5 layers\n",
      "Step 200: Loss = -5.862119631779968 for 7 qubits with 5 layers\n",
      "Step 210: Loss = -5.862417821096493 for 7 qubits with 5 layers\n",
      "Step 220: Loss = -5.862661812675643 for 7 qubits with 5 layers\n",
      "Step 230: Loss = -5.862849312239551 for 7 qubits with 5 layers\n",
      "Step 240: Loss = -5.862984264315009 for 7 qubits with 5 layers\n",
      "Step 250: Loss = -5.863074950939157 for 7 qubits with 5 layers\n",
      "Step 260: Loss = -5.8631316285025115 for 7 qubits with 5 layers\n",
      "Step 270: Loss = -5.863164401269348 for 7 qubits with 5 layers\n",
      "Step 280: Loss = -5.863181807558334 for 7 qubits with 5 layers\n",
      "Step 290: Loss = -5.863190211391043 for 7 qubits with 5 layers\n",
      "Step 300: Loss = -5.863193842390586 for 7 qubits with 5 layers\n",
      "Step 310: Loss = -5.863195211372435 for 7 qubits with 5 layers\n",
      "Step 320: Loss = -5.863195641790535 for 7 qubits with 5 layers\n",
      "Step 330: Loss = -5.863195744015807 for 7 qubits with 5 layers\n",
      "Step 340: Loss = -5.8631957571334405 for 7 qubits with 5 layers\n",
      "Step 350: Loss = -5.863195755761662 for 7 qubits with 5 layers\n",
      "Step 360: Loss = -5.863195755354245 for 7 qubits with 5 layers\n",
      "Step 370: Loss = -5.863195756112224 for 7 qubits with 5 layers\n",
      "Step 380: Loss = -5.863195756793537 for 7 qubits with 5 layers\n",
      "Step 390: Loss = -5.863195757085215 for 7 qubits with 5 layers\n",
      "Step 400: Loss = -5.863195757148885 for 7 qubits with 5 layers\n",
      "Step 410: Loss = -5.863195757149195 for 7 qubits with 5 layers\n",
      "Step 420: Loss = -5.863195757147244 for 7 qubits with 5 layers\n",
      "Step 430: Loss = -5.863195757149177 for 7 qubits with 5 layers\n",
      "Step 440: Loss = -5.863195757150726 for 7 qubits with 5 layers\n",
      "Step 450: Loss = -5.863195757151202 for 7 qubits with 5 layers\n",
      "Step 460: Loss = -5.863195757151163 for 7 qubits with 5 layers\n",
      "Step 470: Loss = -5.863195757151196 for 7 qubits with 5 layers\n",
      "Step 480: Loss = -5.863195757151189 for 7 qubits with 5 layers\n",
      "Step 490: Loss = -5.8631957571512086 for 7 qubits with 5 layers\n",
      "Running optimization for 7 qubits and 6 layers\n",
      "Step 0: Loss = -2.1871548504926293 for 7 qubits with 6 layers\n",
      "Step 10: Loss = -5.277689716795804 for 7 qubits with 6 layers\n",
      "Step 20: Loss = -5.704432439738574 for 7 qubits with 6 layers\n",
      "Step 30: Loss = -5.792672846988751 for 7 qubits with 6 layers\n",
      "Step 40: Loss = -5.9203443723641005 for 7 qubits with 6 layers\n",
      "Step 50: Loss = -5.944989505780763 for 7 qubits with 6 layers\n",
      "Step 60: Loss = -5.95273075784165 for 7 qubits with 6 layers\n",
      "Step 70: Loss = -5.955202261471094 for 7 qubits with 6 layers\n",
      "Step 80: Loss = -5.956290595485734 for 7 qubits with 6 layers\n",
      "Step 90: Loss = -5.956821377419723 for 7 qubits with 6 layers\n",
      "Step 100: Loss = -5.957161846798086 for 7 qubits with 6 layers\n",
      "Step 110: Loss = -5.957164622314379 for 7 qubits with 6 layers\n",
      "Step 120: Loss = -5.957195754349221 for 7 qubits with 6 layers\n",
      "Step 130: Loss = -5.9572058682564375 for 7 qubits with 6 layers\n",
      "Step 140: Loss = -5.95720904724165 for 7 qubits with 6 layers\n",
      "Step 150: Loss = -5.957209299327886 for 7 qubits with 6 layers\n",
      "Step 160: Loss = -5.957210153888312 for 7 qubits with 6 layers\n",
      "Step 170: Loss = -5.957210153771624 for 7 qubits with 6 layers\n",
      "Step 180: Loss = -5.957210219593843 for 7 qubits with 6 layers\n",
      "Step 190: Loss = -5.957210226850431 for 7 qubits with 6 layers\n",
      "Step 200: Loss = -5.957210233296259 for 7 qubits with 6 layers\n",
      "Step 210: Loss = -5.9572102381611955 for 7 qubits with 6 layers\n",
      "Step 220: Loss = -5.957210238142161 for 7 qubits with 6 layers\n",
      "Step 230: Loss = -5.95721023872462 for 7 qubits with 6 layers\n",
      "Step 240: Loss = -5.957210238744957 for 7 qubits with 6 layers\n",
      "Step 250: Loss = -5.957210238803556 for 7 qubits with 6 layers\n",
      "Step 260: Loss = -5.9572102388114905 for 7 qubits with 6 layers\n",
      "Step 270: Loss = -5.957210238819772 for 7 qubits with 6 layers\n",
      "Step 280: Loss = -5.957210238819723 for 7 qubits with 6 layers\n",
      "Step 290: Loss = -5.9572102388199815 for 7 qubits with 6 layers\n",
      "Step 300: Loss = -5.957210234160397 for 7 qubits with 6 layers\n",
      "Step 310: Loss = -5.955918554691742 for 7 qubits with 6 layers\n",
      "Step 320: Loss = -5.955480384221033 for 7 qubits with 6 layers\n",
      "Step 330: Loss = -5.9561772566436275 for 7 qubits with 6 layers\n",
      "Step 340: Loss = -5.9568172196892695 for 7 qubits with 6 layers\n",
      "Step 350: Loss = -5.9570721069843 for 7 qubits with 6 layers\n",
      "Step 360: Loss = -5.957153924247594 for 7 qubits with 6 layers\n",
      "Step 370: Loss = -5.957209618614668 for 7 qubits with 6 layers\n",
      "Step 380: Loss = -5.957202457542998 for 7 qubits with 6 layers\n",
      "Step 390: Loss = -5.957206912910845 for 7 qubits with 6 layers\n",
      "Step 400: Loss = -5.957209897522272 for 7 qubits with 6 layers\n",
      "Step 410: Loss = -5.9572096503138665 for 7 qubits with 6 layers\n",
      "Step 420: Loss = -5.957210019795697 for 7 qubits with 6 layers\n",
      "Step 430: Loss = -5.95721003343645 for 7 qubits with 6 layers\n",
      "Step 440: Loss = -5.957209183532064 for 7 qubits with 6 layers\n",
      "Step 450: Loss = -5.95344999275953 for 7 qubits with 6 layers\n",
      "Step 460: Loss = -5.9571646417241695 for 7 qubits with 6 layers\n",
      "Step 470: Loss = -5.956960389614085 for 7 qubits with 6 layers\n",
      "Step 480: Loss = -5.95705516413982 for 7 qubits with 6 layers\n",
      "Step 490: Loss = -5.957183647681553 for 7 qubits with 6 layers\n",
      "Running optimization for 7 qubits and 7 layers\n",
      "Step 0: Loss = -1.4496693598451904 for 7 qubits with 7 layers\n",
      "Step 10: Loss = -4.923344063791358 for 7 qubits with 7 layers\n",
      "Step 20: Loss = -5.423268858363408 for 7 qubits with 7 layers\n",
      "Step 30: Loss = -5.630086614715353 for 7 qubits with 7 layers\n",
      "Step 40: Loss = -5.718893840074204 for 7 qubits with 7 layers\n",
      "Step 50: Loss = -5.7547590176896195 for 7 qubits with 7 layers\n",
      "Step 60: Loss = -5.774336019925414 for 7 qubits with 7 layers\n",
      "Step 70: Loss = -5.797319011665924 for 7 qubits with 7 layers\n",
      "Step 80: Loss = -5.841080517601817 for 7 qubits with 7 layers\n",
      "Step 90: Loss = -5.912984504442056 for 7 qubits with 7 layers\n",
      "Step 100: Loss = -5.976178560219755 for 7 qubits with 7 layers\n",
      "Step 110: Loss = -5.983897070568068 for 7 qubits with 7 layers\n",
      "Step 120: Loss = -5.988336148845107 for 7 qubits with 7 layers\n",
      "Step 130: Loss = -5.992952849304255 for 7 qubits with 7 layers\n",
      "Step 140: Loss = -5.995214628215676 for 7 qubits with 7 layers\n",
      "Step 150: Loss = -5.996635775556255 for 7 qubits with 7 layers\n",
      "Step 160: Loss = -5.997360178313796 for 7 qubits with 7 layers\n",
      "Step 170: Loss = -5.997660647369682 for 7 qubits with 7 layers\n",
      "Step 180: Loss = -5.997862085651414 for 7 qubits with 7 layers\n",
      "Step 190: Loss = -5.997981258429752 for 7 qubits with 7 layers\n",
      "Step 200: Loss = -5.998051636204719 for 7 qubits with 7 layers\n",
      "Step 210: Loss = -5.998102078464859 for 7 qubits with 7 layers\n",
      "Step 220: Loss = -5.998147091200203 for 7 qubits with 7 layers\n",
      "Step 230: Loss = -5.9981938846624825 for 7 qubits with 7 layers\n",
      "Step 240: Loss = -5.99824755551791 for 7 qubits with 7 layers\n",
      "Step 250: Loss = -5.998311342332509 for 7 qubits with 7 layers\n",
      "Step 260: Loss = -5.998385583637921 for 7 qubits with 7 layers\n",
      "Step 270: Loss = -5.998467055199752 for 7 qubits with 7 layers\n",
      "Step 280: Loss = -5.998548477317517 for 7 qubits with 7 layers\n",
      "Step 290: Loss = -5.998620478529034 for 7 qubits with 7 layers\n",
      "Step 300: Loss = -5.998676315715771 for 7 qubits with 7 layers\n",
      "Step 310: Loss = -5.998715786057976 for 7 qubits with 7 layers\n",
      "Step 320: Loss = -5.998743824311155 for 7 qubits with 7 layers\n",
      "Step 330: Loss = -5.998765719784336 for 7 qubits with 7 layers\n",
      "Step 340: Loss = -5.998784528248396 for 7 qubits with 7 layers\n",
      "Step 350: Loss = -5.998801587608842 for 7 qubits with 7 layers\n",
      "Step 360: Loss = -5.9985673802544035 for 7 qubits with 7 layers\n",
      "Step 370: Loss = -5.9958933986626315 for 7 qubits with 7 layers\n",
      "Step 380: Loss = -5.99831117320446 for 7 qubits with 7 layers\n",
      "Step 390: Loss = -5.998806118430446 for 7 qubits with 7 layers\n",
      "Step 400: Loss = -5.998742893582026 for 7 qubits with 7 layers\n",
      "Step 410: Loss = -5.998836576443695 for 7 qubits with 7 layers\n",
      "Step 420: Loss = -5.998885911979263 for 7 qubits with 7 layers\n",
      "Step 430: Loss = -5.998925519104283 for 7 qubits with 7 layers\n",
      "Step 440: Loss = -5.998942538575447 for 7 qubits with 7 layers\n",
      "Step 450: Loss = -5.998965332118163 for 7 qubits with 7 layers\n",
      "Step 460: Loss = -5.99898553671272 for 7 qubits with 7 layers\n",
      "Step 470: Loss = -5.998961970549643 for 7 qubits with 7 layers\n",
      "Step 480: Loss = -5.997465326591753 for 7 qubits with 7 layers\n",
      "Step 490: Loss = -5.998965604063158 for 7 qubits with 7 layers\n",
      "Running optimization for 7 qubits and 8 layers\n",
      "Step 0: Loss = -0.9611813761625998 for 7 qubits with 8 layers\n",
      "Step 10: Loss = -4.701919425791774 for 7 qubits with 8 layers\n",
      "Step 20: Loss = -5.501275086145919 for 7 qubits with 8 layers\n",
      "Step 30: Loss = -5.761958947508971 for 7 qubits with 8 layers\n",
      "Step 40: Loss = -5.946851676146437 for 7 qubits with 8 layers\n",
      "Step 50: Loss = -5.964576870640498 for 7 qubits with 8 layers\n",
      "Step 60: Loss = -5.984275262171854 for 7 qubits with 8 layers\n",
      "Step 70: Loss = -5.988163143434687 for 7 qubits with 8 layers\n",
      "Step 80: Loss = -5.992076603858106 for 7 qubits with 8 layers\n",
      "Step 90: Loss = -5.994193056912977 for 7 qubits with 8 layers\n",
      "Step 100: Loss = -5.9959481445395255 for 7 qubits with 8 layers\n",
      "Step 110: Loss = -5.9970628054231305 for 7 qubits with 8 layers\n",
      "Step 120: Loss = -5.997975785672933 for 7 qubits with 8 layers\n",
      "Step 130: Loss = -5.998645243926911 for 7 qubits with 8 layers\n",
      "Step 140: Loss = -5.9991360724391285 for 7 qubits with 8 layers\n",
      "Step 150: Loss = -5.999476514969358 for 7 qubits with 8 layers\n",
      "Step 160: Loss = -5.999700070969614 for 7 qubits with 8 layers\n",
      "Step 170: Loss = -5.999838457904283 for 7 qubits with 8 layers\n",
      "Step 180: Loss = -5.999918909095378 for 7 qubits with 8 layers\n",
      "Step 190: Loss = -5.999962389449043 for 7 qubits with 8 layers\n",
      "Step 200: Loss = -5.9999841046637 for 7 qubits with 8 layers\n",
      "Step 210: Loss = -5.999994007033382 for 7 qubits with 8 layers\n",
      "Step 220: Loss = -5.999998051265305 for 7 qubits with 8 layers\n",
      "Step 230: Loss = -5.9999994863774155 for 7 qubits with 8 layers\n",
      "Step 240: Loss = -5.999999904784042 for 7 qubits with 8 layers\n",
      "Step 250: Loss = -5.9999999924632075 for 7 qubits with 8 layers\n",
      "Step 260: Loss = -5.9999999997306315 for 7 qubits with 8 layers\n",
      "Step 270: Loss = -5.999999997543078 for 7 qubits with 8 layers\n",
      "Step 280: Loss = -5.99999999766618 for 7 qubits with 8 layers\n",
      "Step 290: Loss = -5.999999998845807 for 7 qubits with 8 layers\n",
      "Step 300: Loss = -5.99999999965913 for 7 qubits with 8 layers\n",
      "Step 310: Loss = -5.99999999995197 for 7 qubits with 8 layers\n",
      "Step 320: Loss = -5.999999999999717 for 7 qubits with 8 layers\n",
      "Step 330: Loss = -5.999999999995323 for 7 qubits with 8 layers\n",
      "Step 340: Loss = -5.999999999994707 for 7 qubits with 8 layers\n",
      "Step 350: Loss = -5.999999999997793 for 7 qubits with 8 layers\n",
      "Step 360: Loss = -5.999999999999559 for 7 qubits with 8 layers\n",
      "Step 370: Loss = -5.999999999999904 for 7 qubits with 8 layers\n",
      "Step 380: Loss = -5.999999999999805 for 7 qubits with 8 layers\n",
      "Step 390: Loss = -5.999999999999838 for 7 qubits with 8 layers\n",
      "Step 400: Loss = -5.999999999999714 for 7 qubits with 8 layers\n",
      "Step 410: Loss = -5.999999955013676 for 7 qubits with 8 layers\n",
      "Step 420: Loss = -5.991727811042069 for 7 qubits with 8 layers\n",
      "Step 430: Loss = -5.997804584440631 for 7 qubits with 8 layers\n",
      "Step 440: Loss = -5.999667502285539 for 7 qubits with 8 layers\n",
      "Step 450: Loss = -5.999408639289569 for 7 qubits with 8 layers\n",
      "Step 460: Loss = -5.999879780565959 for 7 qubits with 8 layers\n",
      "Step 470: Loss = -5.999964738768546 for 7 qubits with 8 layers\n",
      "Step 480: Loss = -5.999971725531708 for 7 qubits with 8 layers\n",
      "Step 490: Loss = -5.999996805773508 for 7 qubits with 8 layers\n",
      "Running optimization for 7 qubits and 9 layers\n",
      "Step 0: Loss = -2.147894019049673 for 7 qubits with 9 layers\n",
      "Step 10: Loss = -5.389909958158551 for 7 qubits with 9 layers\n",
      "Step 20: Loss = -5.88953248809327 for 7 qubits with 9 layers\n",
      "Step 30: Loss = -5.935010994598674 for 7 qubits with 9 layers\n",
      "Step 40: Loss = -5.976095018687055 for 7 qubits with 9 layers\n",
      "Step 50: Loss = -5.9900888182868 for 7 qubits with 9 layers\n",
      "Step 60: Loss = -5.996343368258647 for 7 qubits with 9 layers\n",
      "Step 70: Loss = -5.9990290862679725 for 7 qubits with 9 layers\n",
      "Step 80: Loss = -5.99961286627745 for 7 qubits with 9 layers\n",
      "Step 90: Loss = -5.999776098241146 for 7 qubits with 9 layers\n",
      "Step 100: Loss = -5.9999515929482925 for 7 qubits with 9 layers\n",
      "Step 110: Loss = -5.999970670219821 for 7 qubits with 9 layers\n",
      "Step 120: Loss = -5.9999903070532055 for 7 qubits with 9 layers\n",
      "Step 130: Loss = -5.999998347671614 for 7 qubits with 9 layers\n",
      "Step 140: Loss = -5.999998754939742 for 7 qubits with 9 layers\n",
      "Step 150: Loss = -5.999999543553582 for 7 qubits with 9 layers\n",
      "Step 160: Loss = -5.99999986289149 for 7 qubits with 9 layers\n",
      "Step 170: Loss = -5.999999985723724 for 7 qubits with 9 layers\n",
      "Step 180: Loss = -5.999999976125463 for 7 qubits with 9 layers\n",
      "Step 190: Loss = -5.999999727051084 for 7 qubits with 9 layers\n",
      "Step 200: Loss = -5.990884320228408 for 7 qubits with 9 layers\n",
      "Step 210: Loss = -5.997001314770573 for 7 qubits with 9 layers\n",
      "Step 220: Loss = -5.999556030999972 for 7 qubits with 9 layers\n",
      "Step 230: Loss = -5.999924312913368 for 7 qubits with 9 layers\n",
      "Step 240: Loss = -5.999922364171874 for 7 qubits with 9 layers\n",
      "Step 250: Loss = -5.999999932222037 for 7 qubits with 9 layers\n",
      "Step 260: Loss = -5.999970599198294 for 7 qubits with 9 layers\n",
      "Step 270: Loss = -5.9999897671443385 for 7 qubits with 9 layers\n",
      "Step 280: Loss = -5.999988726147862 for 7 qubits with 9 layers\n",
      "Step 290: Loss = -5.999971233154729 for 7 qubits with 9 layers\n",
      "Step 300: Loss = -5.996310262435104 for 7 qubits with 9 layers\n",
      "Step 310: Loss = -5.997512868468202 for 7 qubits with 9 layers\n",
      "Step 320: Loss = -5.999532456909625 for 7 qubits with 9 layers\n",
      "Step 330: Loss = -5.999951153228433 for 7 qubits with 9 layers\n",
      "Step 340: Loss = -5.999930036433012 for 7 qubits with 9 layers\n",
      "Step 350: Loss = -5.9999880133740975 for 7 qubits with 9 layers\n",
      "Step 360: Loss = -5.999950825510691 for 7 qubits with 9 layers\n",
      "Step 370: Loss = -5.999923032964258 for 7 qubits with 9 layers\n",
      "Step 380: Loss = -5.996890374163787 for 7 qubits with 9 layers\n",
      "Step 390: Loss = -5.999621127315658 for 7 qubits with 9 layers\n",
      "Step 400: Loss = -5.999109563500798 for 7 qubits with 9 layers\n",
      "Step 410: Loss = -5.999881609416324 for 7 qubits with 9 layers\n",
      "Step 420: Loss = -5.999855759491396 for 7 qubits with 9 layers\n",
      "Step 430: Loss = -5.999993302077228 for 7 qubits with 9 layers\n",
      "Step 440: Loss = -5.999404860553772 for 7 qubits with 9 layers\n",
      "Step 450: Loss = -5.998785934111677 for 7 qubits with 9 layers\n",
      "Step 460: Loss = -5.999882355809997 for 7 qubits with 9 layers\n",
      "Step 470: Loss = -5.9998657101438075 for 7 qubits with 9 layers\n",
      "Step 480: Loss = -5.999894566233749 for 7 qubits with 9 layers\n",
      "Step 490: Loss = -5.9997603052585795 for 7 qubits with 9 layers\n",
      "Running optimization for 7 qubits and 10 layers\n",
      "Step 0: Loss = -3.3674666598377963 for 7 qubits with 10 layers\n",
      "Step 10: Loss = -5.382452712292481 for 7 qubits with 10 layers\n",
      "Step 20: Loss = -5.851481450391756 for 7 qubits with 10 layers\n",
      "Step 30: Loss = -5.9839978140412216 for 7 qubits with 10 layers\n",
      "Step 40: Loss = -5.979062860379736 for 7 qubits with 10 layers\n",
      "Step 50: Loss = -5.991747521146609 for 7 qubits with 10 layers\n",
      "Step 60: Loss = -5.996882322324913 for 7 qubits with 10 layers\n",
      "Step 70: Loss = -5.998774544090538 for 7 qubits with 10 layers\n",
      "Step 80: Loss = -5.999748398680787 for 7 qubits with 10 layers\n",
      "Step 90: Loss = -5.999881449959803 for 7 qubits with 10 layers\n",
      "Step 100: Loss = -5.999980144645992 for 7 qubits with 10 layers\n",
      "Step 110: Loss = -5.999988665146264 for 7 qubits with 10 layers\n",
      "Step 120: Loss = -5.999996443913553 for 7 qubits with 10 layers\n",
      "Step 130: Loss = -5.999998029454652 for 7 qubits with 10 layers\n",
      "Step 140: Loss = -5.9999992820240085 for 7 qubits with 10 layers\n",
      "Step 150: Loss = -5.999999658831368 for 7 qubits with 10 layers\n",
      "Step 160: Loss = -5.9999999163143745 for 7 qubits with 10 layers\n",
      "Step 170: Loss = -5.999999967170899 for 7 qubits with 10 layers\n",
      "Step 180: Loss = -5.99999999085664 for 7 qubits with 10 layers\n",
      "Step 190: Loss = -5.9999999937172435 for 7 qubits with 10 layers\n",
      "Step 200: Loss = -5.999999998346778 for 7 qubits with 10 layers\n",
      "Step 210: Loss = -5.999999999515973 for 7 qubits with 10 layers\n",
      "Step 220: Loss = -5.999999999738898 for 7 qubits with 10 layers\n",
      "Step 230: Loss = -5.999999999925558 for 7 qubits with 10 layers\n",
      "Step 240: Loss = -5.99999999997367 for 7 qubits with 10 layers\n",
      "Step 250: Loss = -5.999999999967772 for 7 qubits with 10 layers\n",
      "Step 260: Loss = -5.999999623654661 for 7 qubits with 10 layers\n",
      "Step 270: Loss = -5.984788511357982 for 7 qubits with 10 layers\n",
      "Step 280: Loss = -5.9966789485499525 for 7 qubits with 10 layers\n",
      "Step 290: Loss = -5.998977072092925 for 7 qubits with 10 layers\n",
      "Step 300: Loss = -5.999808014378694 for 7 qubits with 10 layers\n",
      "Step 310: Loss = -5.99997865536772 for 7 qubits with 10 layers\n",
      "Step 320: Loss = -5.999893916124685 for 7 qubits with 10 layers\n",
      "Step 330: Loss = -5.999969485658452 for 7 qubits with 10 layers\n",
      "Step 340: Loss = -5.999985356994027 for 7 qubits with 10 layers\n",
      "Step 350: Loss = -5.999997281230528 for 7 qubits with 10 layers\n",
      "Step 360: Loss = -5.999997297078836 for 7 qubits with 10 layers\n",
      "Step 370: Loss = -5.999999991953094 for 7 qubits with 10 layers\n",
      "Step 380: Loss = -5.999999701731952 for 7 qubits with 10 layers\n",
      "Step 390: Loss = -5.999992873963827 for 7 qubits with 10 layers\n",
      "Step 400: Loss = -5.99044496089438 for 7 qubits with 10 layers\n",
      "Step 410: Loss = -5.999151032055949 for 7 qubits with 10 layers\n",
      "Step 420: Loss = -5.9986857423995446 for 7 qubits with 10 layers\n",
      "Step 430: Loss = -5.999621234819129 for 7 qubits with 10 layers\n",
      "Step 440: Loss = -5.999983538661063 for 7 qubits with 10 layers\n",
      "Step 450: Loss = -5.99996408481957 for 7 qubits with 10 layers\n",
      "Step 460: Loss = -5.999718513705952 for 7 qubits with 10 layers\n",
      "Step 470: Loss = -5.999416650322071 for 7 qubits with 10 layers\n",
      "Step 480: Loss = -5.999826671419231 for 7 qubits with 10 layers\n",
      "Step 490: Loss = -5.999899146084797 for 7 qubits with 10 layers\n",
      "Running optimization for 7 qubits and 11 layers\n",
      "Step 0: Loss = -1.02097956442522 for 7 qubits with 11 layers\n",
      "Step 10: Loss = -5.530412665802581 for 7 qubits with 11 layers\n",
      "Step 20: Loss = -5.836814483310879 for 7 qubits with 11 layers\n",
      "Step 30: Loss = -5.965446224522399 for 7 qubits with 11 layers\n",
      "Step 40: Loss = -5.971371734011679 for 7 qubits with 11 layers\n",
      "Step 50: Loss = -5.994877803598166 for 7 qubits with 11 layers\n",
      "Step 60: Loss = -5.9967705997340515 for 7 qubits with 11 layers\n",
      "Step 70: Loss = -5.998946790665909 for 7 qubits with 11 layers\n",
      "Step 80: Loss = -5.99975301398884 for 7 qubits with 11 layers\n",
      "Step 90: Loss = -5.999828478772802 for 7 qubits with 11 layers\n",
      "Step 100: Loss = -5.99992801608427 for 7 qubits with 11 layers\n",
      "Step 110: Loss = -5.999990474306197 for 7 qubits with 11 layers\n",
      "Step 120: Loss = -5.999995979663543 for 7 qubits with 11 layers\n",
      "Step 130: Loss = -5.9999982993945515 for 7 qubits with 11 layers\n",
      "Step 140: Loss = -5.999999239983929 for 7 qubits with 11 layers\n",
      "Step 150: Loss = -5.999999571133879 for 7 qubits with 11 layers\n",
      "Step 160: Loss = -5.999999890488567 for 7 qubits with 11 layers\n",
      "Step 170: Loss = -5.9999999823959 for 7 qubits with 11 layers\n",
      "Step 180: Loss = -5.999999985391979 for 7 qubits with 11 layers\n",
      "Step 190: Loss = -5.999999996877172 for 7 qubits with 11 layers\n",
      "Step 200: Loss = -5.9999999983626715 for 7 qubits with 11 layers\n",
      "Step 210: Loss = -5.999999997658844 for 7 qubits with 11 layers\n",
      "Step 220: Loss = -5.999990957814319 for 7 qubits with 11 layers\n",
      "Step 230: Loss = -5.995488180610325 for 7 qubits with 11 layers\n",
      "Step 240: Loss = -5.996587299801891 for 7 qubits with 11 layers\n",
      "Step 250: Loss = -5.998872532396576 for 7 qubits with 11 layers\n",
      "Step 260: Loss = -5.999832449885336 for 7 qubits with 11 layers\n",
      "Step 270: Loss = -5.999967306586678 for 7 qubits with 11 layers\n",
      "Step 280: Loss = -5.999986105449032 for 7 qubits with 11 layers\n",
      "Step 290: Loss = -5.999984827070417 for 7 qubits with 11 layers\n",
      "Step 300: Loss = -5.9999997207931495 for 7 qubits with 11 layers\n",
      "Step 310: Loss = -5.999996507245762 for 7 qubits with 11 layers\n",
      "Step 320: Loss = -5.999996488637633 for 7 qubits with 11 layers\n",
      "Step 330: Loss = -5.999956458713984 for 7 qubits with 11 layers\n",
      "Step 340: Loss = -5.999940046324344 for 7 qubits with 11 layers\n",
      "Step 350: Loss = -5.999495953133611 for 7 qubits with 11 layers\n",
      "Step 360: Loss = -5.999977558819157 for 7 qubits with 11 layers\n",
      "Step 370: Loss = -5.999925510596892 for 7 qubits with 11 layers\n",
      "Step 380: Loss = -5.999745312897818 for 7 qubits with 11 layers\n",
      "Step 390: Loss = -5.999974723768039 for 7 qubits with 11 layers\n",
      "Step 400: Loss = -5.999961386065609 for 7 qubits with 11 layers\n",
      "Step 410: Loss = -5.9999925824505596 for 7 qubits with 11 layers\n",
      "Step 420: Loss = -5.999863754172666 for 7 qubits with 11 layers\n",
      "Step 430: Loss = -5.999944648688412 for 7 qubits with 11 layers\n",
      "Step 440: Loss = -5.996373276574982 for 7 qubits with 11 layers\n",
      "Step 450: Loss = -5.999039861740503 for 7 qubits with 11 layers\n",
      "Step 460: Loss = -5.999809078367559 for 7 qubits with 11 layers\n",
      "Step 470: Loss = -5.999998860128846 for 7 qubits with 11 layers\n",
      "Step 480: Loss = -5.999943212006993 for 7 qubits with 11 layers\n",
      "Step 490: Loss = -5.999979122959566 for 7 qubits with 11 layers\n",
      "Running optimization for 7 qubits and 12 layers\n",
      "Step 0: Loss = -3.1991613227806885 for 7 qubits with 12 layers\n",
      "Step 10: Loss = -5.733328911792574 for 7 qubits with 12 layers\n",
      "Step 20: Loss = -5.933369714143401 for 7 qubits with 12 layers\n",
      "Step 30: Loss = -5.964919321086692 for 7 qubits with 12 layers\n",
      "Step 40: Loss = -5.994722584179789 for 7 qubits with 12 layers\n",
      "Step 50: Loss = -5.995802391836067 for 7 qubits with 12 layers\n",
      "Step 60: Loss = -5.998216309924819 for 7 qubits with 12 layers\n",
      "Step 70: Loss = -5.99931795597804 for 7 qubits with 12 layers\n",
      "Step 80: Loss = -5.999834396670052 for 7 qubits with 12 layers\n",
      "Step 90: Loss = -5.999959381246622 for 7 qubits with 12 layers\n",
      "Step 100: Loss = -5.999979424912723 for 7 qubits with 12 layers\n",
      "Step 110: Loss = -5.999988779252078 for 7 qubits with 12 layers\n",
      "Step 120: Loss = -5.999998691406427 for 7 qubits with 12 layers\n",
      "Step 130: Loss = -5.999998277654136 for 7 qubits with 12 layers\n",
      "Step 140: Loss = -5.999999445426063 for 7 qubits with 12 layers\n",
      "Step 150: Loss = -5.99999986468424 for 7 qubits with 12 layers\n",
      "Step 160: Loss = -5.99999995271985 for 7 qubits with 12 layers\n",
      "Step 170: Loss = -5.999999986303517 for 7 qubits with 12 layers\n",
      "Step 180: Loss = -5.999999992600093 for 7 qubits with 12 layers\n",
      "Step 190: Loss = -5.999999996584636 for 7 qubits with 12 layers\n",
      "Step 200: Loss = -5.999999999344324 for 7 qubits with 12 layers\n",
      "Step 210: Loss = -5.999999999497976 for 7 qubits with 12 layers\n",
      "Step 220: Loss = -5.9999999998437765 for 7 qubits with 12 layers\n",
      "Step 230: Loss = -5.999999999960024 for 7 qubits with 12 layers\n",
      "Step 240: Loss = -5.999999999962593 for 7 qubits with 12 layers\n",
      "Step 250: Loss = -5.999999965698919 for 7 qubits with 12 layers\n",
      "Step 260: Loss = -5.994494084807179 for 7 qubits with 12 layers\n",
      "Step 270: Loss = -5.994297846431845 for 7 qubits with 12 layers\n",
      "Step 280: Loss = -5.995365393521935 for 7 qubits with 12 layers\n",
      "Step 290: Loss = -5.999356389600386 for 7 qubits with 12 layers\n",
      "Step 300: Loss = -5.999951252316115 for 7 qubits with 12 layers\n",
      "Step 310: Loss = -5.999992957941577 for 7 qubits with 12 layers\n",
      "Step 320: Loss = -5.999969195569694 for 7 qubits with 12 layers\n",
      "Step 330: Loss = -5.999973392080712 for 7 qubits with 12 layers\n",
      "Step 340: Loss = -5.999989113835095 for 7 qubits with 12 layers\n",
      "Step 350: Loss = -5.999994342619385 for 7 qubits with 12 layers\n",
      "Step 360: Loss = -5.998726696328746 for 7 qubits with 12 layers\n",
      "Step 370: Loss = -5.999295138145989 for 7 qubits with 12 layers\n",
      "Step 380: Loss = -5.998621484153747 for 7 qubits with 12 layers\n",
      "Step 390: Loss = -5.99947466927386 for 7 qubits with 12 layers\n",
      "Step 400: Loss = -5.9999044011502685 for 7 qubits with 12 layers\n",
      "Step 410: Loss = -5.999949564271364 for 7 qubits with 12 layers\n",
      "Step 420: Loss = -5.999942908424437 for 7 qubits with 12 layers\n",
      "Step 430: Loss = -5.999912348325365 for 7 qubits with 12 layers\n",
      "Step 440: Loss = -5.993116889510222 for 7 qubits with 12 layers\n",
      "Step 450: Loss = -5.994225040860923 for 7 qubits with 12 layers\n",
      "Step 460: Loss = -5.997856557519746 for 7 qubits with 12 layers\n",
      "Step 470: Loss = -5.99913101499637 for 7 qubits with 12 layers\n",
      "Step 480: Loss = -5.999867906813976 for 7 qubits with 12 layers\n",
      "Step 490: Loss = -5.999855862070661 for 7 qubits with 12 layers\n",
      "Running optimization for 7 qubits and 13 layers\n",
      "Step 0: Loss = -3.4229789105398223 for 7 qubits with 13 layers\n",
      "Step 10: Loss = -5.705770177709459 for 7 qubits with 13 layers\n",
      "Step 20: Loss = -5.9320476122994155 for 7 qubits with 13 layers\n",
      "Step 30: Loss = -5.9719431377634615 for 7 qubits with 13 layers\n",
      "Step 40: Loss = -5.989627744164097 for 7 qubits with 13 layers\n",
      "Step 50: Loss = -5.996305260730897 for 7 qubits with 13 layers\n",
      "Step 60: Loss = -5.99938675400409 for 7 qubits with 13 layers\n",
      "Step 70: Loss = -5.999433585094109 for 7 qubits with 13 layers\n",
      "Step 80: Loss = -5.999808771192608 for 7 qubits with 13 layers\n",
      "Step 90: Loss = -5.999970121401299 for 7 qubits with 13 layers\n",
      "Step 100: Loss = -5.999980089891051 for 7 qubits with 13 layers\n",
      "Step 110: Loss = -5.999989520385376 for 7 qubits with 13 layers\n",
      "Step 120: Loss = -5.999996196250716 for 7 qubits with 13 layers\n",
      "Step 130: Loss = -5.999998935819456 for 7 qubits with 13 layers\n",
      "Step 140: Loss = -5.999999535990986 for 7 qubits with 13 layers\n",
      "Step 150: Loss = -5.999999874994471 for 7 qubits with 13 layers\n",
      "Step 160: Loss = -5.99999997863428 for 7 qubits with 13 layers\n",
      "Step 170: Loss = -5.999999987734751 for 7 qubits with 13 layers\n",
      "Step 180: Loss = -5.999999996056866 for 7 qubits with 13 layers\n",
      "Step 190: Loss = -5.999999999251132 for 7 qubits with 13 layers\n",
      "Step 200: Loss = -5.9999999990198445 for 7 qubits with 13 layers\n",
      "Step 210: Loss = -5.9999999997092495 for 7 qubits with 13 layers\n",
      "Step 220: Loss = -5.999999999797709 for 7 qubits with 13 layers\n",
      "Step 230: Loss = -5.999999999875537 for 7 qubits with 13 layers\n",
      "Step 240: Loss = -5.999999856777338 for 7 qubits with 13 layers\n",
      "Step 250: Loss = -5.980054131318892 for 7 qubits with 13 layers\n",
      "Step 260: Loss = -5.983986815330992 for 7 qubits with 13 layers\n",
      "Step 270: Loss = -5.994558297346942 for 7 qubits with 13 layers\n",
      "Step 280: Loss = -5.997854219524492 for 7 qubits with 13 layers\n",
      "Step 290: Loss = -5.999369434622956 for 7 qubits with 13 layers\n",
      "Step 300: Loss = -5.999988984371472 for 7 qubits with 13 layers\n",
      "Step 310: Loss = -5.999884418701021 for 7 qubits with 13 layers\n",
      "Step 320: Loss = -5.999958602556054 for 7 qubits with 13 layers\n",
      "Step 330: Loss = -5.999982236560421 for 7 qubits with 13 layers\n",
      "Step 340: Loss = -5.9999985027431375 for 7 qubits with 13 layers\n",
      "Step 350: Loss = -5.999910868741425 for 7 qubits with 13 layers\n",
      "Step 360: Loss = -5.99765890783138 for 7 qubits with 13 layers\n",
      "Step 370: Loss = -5.999965233613437 for 7 qubits with 13 layers\n",
      "Step 380: Loss = -5.999867221648225 for 7 qubits with 13 layers\n",
      "Step 390: Loss = -5.999714693596202 for 7 qubits with 13 layers\n",
      "Step 400: Loss = -5.999750803878113 for 7 qubits with 13 layers\n",
      "Step 410: Loss = -5.999207573837734 for 7 qubits with 13 layers\n",
      "Step 420: Loss = -5.996438071978821 for 7 qubits with 13 layers\n",
      "Step 430: Loss = -5.999722829532904 for 7 qubits with 13 layers\n",
      "Step 440: Loss = -5.999687980875969 for 7 qubits with 13 layers\n",
      "Step 450: Loss = -5.9998227027763456 for 7 qubits with 13 layers\n",
      "Step 460: Loss = -5.999983348461653 for 7 qubits with 13 layers\n",
      "Step 470: Loss = -5.999361323441014 for 7 qubits with 13 layers\n",
      "Step 480: Loss = -5.995189718957891 for 7 qubits with 13 layers\n",
      "Step 490: Loss = -5.997479293053494 for 7 qubits with 13 layers\n",
      "Running optimization for 7 qubits and 14 layers\n",
      "Step 0: Loss = -1.727078805391351 for 7 qubits with 14 layers\n",
      "Step 10: Loss = -5.609721529693833 for 7 qubits with 14 layers\n",
      "Step 20: Loss = -5.865706848611037 for 7 qubits with 14 layers\n",
      "Step 30: Loss = -5.960372946515966 for 7 qubits with 14 layers\n",
      "Step 40: Loss = -5.982924135725454 for 7 qubits with 14 layers\n",
      "Step 50: Loss = -5.995465627660363 for 7 qubits with 14 layers\n",
      "Step 60: Loss = -5.998207628565111 for 7 qubits with 14 layers\n",
      "Step 70: Loss = -5.999067563953562 for 7 qubits with 14 layers\n",
      "Step 80: Loss = -5.999797409127124 for 7 qubits with 14 layers\n",
      "Step 90: Loss = -5.9999137774723765 for 7 qubits with 14 layers\n",
      "Step 100: Loss = -5.999948099645257 for 7 qubits with 14 layers\n",
      "Step 110: Loss = -5.999985470477257 for 7 qubits with 14 layers\n",
      "Step 120: Loss = -5.9999964299128346 for 7 qubits with 14 layers\n",
      "Step 130: Loss = -5.999998710085429 for 7 qubits with 14 layers\n",
      "Step 140: Loss = -5.999999395281419 for 7 qubits with 14 layers\n",
      "Step 150: Loss = -5.999999757565458 for 7 qubits with 14 layers\n",
      "Step 160: Loss = -5.999999912759177 for 7 qubits with 14 layers\n",
      "Step 170: Loss = -5.999999961284253 for 7 qubits with 14 layers\n",
      "Step 180: Loss = -5.999999983548751 for 7 qubits with 14 layers\n",
      "Step 190: Loss = -5.999999995627197 for 7 qubits with 14 layers\n",
      "Step 200: Loss = -5.999999997974917 for 7 qubits with 14 layers\n",
      "Step 210: Loss = -5.999999996177777 for 7 qubits with 14 layers\n",
      "Step 220: Loss = -5.9999743763929265 for 7 qubits with 14 layers\n",
      "Step 230: Loss = -5.980508859483842 for 7 qubits with 14 layers\n",
      "Step 240: Loss = -5.998324812091941 for 7 qubits with 14 layers\n",
      "Step 250: Loss = -5.996993260890453 for 7 qubits with 14 layers\n",
      "Step 260: Loss = -5.999834255765128 for 7 qubits with 14 layers\n",
      "Step 270: Loss = -5.999570077832646 for 7 qubits with 14 layers\n",
      "Step 280: Loss = -5.999850148669791 for 7 qubits with 14 layers\n",
      "Step 290: Loss = -5.999941535084474 for 7 qubits with 14 layers\n",
      "Step 300: Loss = -5.999987371118903 for 7 qubits with 14 layers\n",
      "Step 310: Loss = -5.9999905179883495 for 7 qubits with 14 layers\n",
      "Step 320: Loss = -5.999999801690529 for 7 qubits with 14 layers\n",
      "Step 330: Loss = -5.999923664446443 for 7 qubits with 14 layers\n",
      "Step 340: Loss = -5.987239066854709 for 7 qubits with 14 layers\n",
      "Step 350: Loss = -5.9945679228242055 for 7 qubits with 14 layers\n",
      "Step 360: Loss = -5.999027179229966 for 7 qubits with 14 layers\n",
      "Step 370: Loss = -5.999969206890076 for 7 qubits with 14 layers\n",
      "Step 380: Loss = -5.999632802377983 for 7 qubits with 14 layers\n",
      "Step 390: Loss = -5.999821550509399 for 7 qubits with 14 layers\n",
      "Step 400: Loss = -5.999999688715739 for 7 qubits with 14 layers\n",
      "Step 410: Loss = -5.999927231077087 for 7 qubits with 14 layers\n",
      "Step 420: Loss = -5.990002014487364 for 7 qubits with 14 layers\n",
      "Step 430: Loss = -5.995849181674226 for 7 qubits with 14 layers\n",
      "Step 440: Loss = -5.99926462259592 for 7 qubits with 14 layers\n",
      "Step 450: Loss = -5.999082603834027 for 7 qubits with 14 layers\n",
      "Step 460: Loss = -5.999557719917195 for 7 qubits with 14 layers\n",
      "Step 470: Loss = -5.999792994846074 for 7 qubits with 14 layers\n",
      "Step 480: Loss = -5.999901606375877 for 7 qubits with 14 layers\n",
      "Step 490: Loss = -5.999787085168452 for 7 qubits with 14 layers\n",
      "Running optimization for 7 qubits and 15 layers\n",
      "Step 0: Loss = -1.3307244383013468 for 7 qubits with 15 layers\n",
      "Step 10: Loss = -5.660320469927769 for 7 qubits with 15 layers\n",
      "Step 20: Loss = -5.853952913530618 for 7 qubits with 15 layers\n",
      "Step 30: Loss = -5.944228287547069 for 7 qubits with 15 layers\n",
      "Step 40: Loss = -5.982938109252326 for 7 qubits with 15 layers\n",
      "Step 50: Loss = -5.993214699863194 for 7 qubits with 15 layers\n",
      "Step 60: Loss = -5.998322807925435 for 7 qubits with 15 layers\n",
      "Step 70: Loss = -5.998829347833944 for 7 qubits with 15 layers\n",
      "Step 80: Loss = -5.999604059657282 for 7 qubits with 15 layers\n",
      "Step 90: Loss = -5.999835808686482 for 7 qubits with 15 layers\n",
      "Step 100: Loss = -5.999969776618033 for 7 qubits with 15 layers\n",
      "Step 110: Loss = -5.999983660512675 for 7 qubits with 15 layers\n",
      "Step 120: Loss = -5.9999935478519095 for 7 qubits with 15 layers\n",
      "Step 130: Loss = -5.999998023686722 for 7 qubits with 15 layers\n",
      "Step 140: Loss = -5.999999191480635 for 7 qubits with 15 layers\n",
      "Step 150: Loss = -5.999999690297744 for 7 qubits with 15 layers\n",
      "Step 160: Loss = -5.999999872310286 for 7 qubits with 15 layers\n",
      "Step 170: Loss = -5.99999995454322 for 7 qubits with 15 layers\n",
      "Step 180: Loss = -5.999999990515905 for 7 qubits with 15 layers\n",
      "Step 190: Loss = -5.999999995017527 for 7 qubits with 15 layers\n",
      "Step 200: Loss = -5.999999998399438 for 7 qubits with 15 layers\n",
      "Step 210: Loss = -5.999999999590614 for 7 qubits with 15 layers\n",
      "Step 220: Loss = -5.999999984881896 for 7 qubits with 15 layers\n",
      "Step 230: Loss = -5.997226960090456 for 7 qubits with 15 layers\n",
      "Step 240: Loss = -5.994787016946833 for 7 qubits with 15 layers\n",
      "Step 250: Loss = -5.994878904899411 for 7 qubits with 15 layers\n",
      "Step 260: Loss = -5.998605795197702 for 7 qubits with 15 layers\n",
      "Step 270: Loss = -5.999976032905977 for 7 qubits with 15 layers\n",
      "Step 280: Loss = -5.999786530736733 for 7 qubits with 15 layers\n",
      "Step 290: Loss = -5.999891503293359 for 7 qubits with 15 layers\n",
      "Step 300: Loss = -5.999980304427493 for 7 qubits with 15 layers\n",
      "Step 310: Loss = -5.9999838798701495 for 7 qubits with 15 layers\n",
      "Step 320: Loss = -5.999991344324423 for 7 qubits with 15 layers\n",
      "Step 330: Loss = -5.999998738575172 for 7 qubits with 15 layers\n",
      "Step 340: Loss = -5.9999741812741885 for 7 qubits with 15 layers\n",
      "Step 350: Loss = -5.97419058928411 for 7 qubits with 15 layers\n",
      "Step 360: Loss = -5.9989864725098805 for 7 qubits with 15 layers\n",
      "Step 370: Loss = -5.999655548128991 for 7 qubits with 15 layers\n",
      "Step 380: Loss = -5.999873614099435 for 7 qubits with 15 layers\n",
      "Step 390: Loss = -5.999772421556365 for 7 qubits with 15 layers\n",
      "Step 400: Loss = -5.9999754592116314 for 7 qubits with 15 layers\n",
      "Step 410: Loss = -5.99999519351975 for 7 qubits with 15 layers\n",
      "Step 420: Loss = -5.99986053172959 for 7 qubits with 15 layers\n",
      "Step 430: Loss = -5.99193249187224 for 7 qubits with 15 layers\n",
      "Step 440: Loss = -5.997637407464367 for 7 qubits with 15 layers\n",
      "Step 450: Loss = -5.999962932499773 for 7 qubits with 15 layers\n",
      "Step 460: Loss = -5.99938507385308 for 7 qubits with 15 layers\n",
      "Step 470: Loss = -5.999988659395618 for 7 qubits with 15 layers\n",
      "Step 480: Loss = -5.989185032008843 for 7 qubits with 15 layers\n",
      "Step 490: Loss = -5.98923309852144 for 7 qubits with 15 layers\n",
      "Running optimization for 8 qubits and 1 layers\n",
      "Step 0: Loss = -0.7543957357158612 for 8 qubits with 1 layers\n",
      "Step 10: Loss = -1.8273605931311354 for 8 qubits with 1 layers\n",
      "Step 20: Loss = -3.213984395393648 for 8 qubits with 1 layers\n",
      "Step 30: Loss = -4.673794344912309 for 8 qubits with 1 layers\n",
      "Step 40: Loss = -5.88905033496927 for 8 qubits with 1 layers\n",
      "Step 50: Loss = -6.637748847966229 for 8 qubits with 1 layers\n",
      "Step 60: Loss = -6.940925401571375 for 8 qubits with 1 layers\n",
      "Step 70: Loss = -6.999574561937489 for 8 qubits with 1 layers\n",
      "Step 80: Loss = -6.994807879657504 for 8 qubits with 1 layers\n",
      "Step 90: Loss = -6.994045103973392 for 8 qubits with 1 layers\n",
      "Step 100: Loss = -6.998022825862161 for 8 qubits with 1 layers\n",
      "Step 110: Loss = -6.999883844626136 for 8 qubits with 1 layers\n",
      "Step 120: Loss = -6.999946316310201 for 8 qubits with 1 layers\n",
      "Step 130: Loss = -6.999900443330754 for 8 qubits with 1 layers\n",
      "Step 140: Loss = -6.9999742227947515 for 8 qubits with 1 layers\n",
      "Step 150: Loss = -6.999999990916738 for 8 qubits with 1 layers\n",
      "Step 160: Loss = -6.999996771041656 for 8 qubits with 1 layers\n",
      "Step 170: Loss = -6.999998490196431 for 8 qubits with 1 layers\n",
      "Step 180: Loss = -6.999999987855076 for 8 qubits with 1 layers\n",
      "Step 190: Loss = -6.999999852550413 for 8 qubits with 1 layers\n",
      "Step 200: Loss = -6.999999938539418 for 8 qubits with 1 layers\n",
      "Step 210: Loss = -6.999999999740244 for 8 qubits with 1 layers\n",
      "Step 220: Loss = -6.999999990537125 for 8 qubits with 1 layers\n",
      "Step 230: Loss = -6.999999998850432 for 8 qubits with 1 layers\n",
      "Step 240: Loss = -6.999999999475374 for 8 qubits with 1 layers\n",
      "Step 250: Loss = -6.999999999621755 for 8 qubits with 1 layers\n",
      "Step 260: Loss = -6.999999999990452 for 8 qubits with 1 layers\n",
      "Step 270: Loss = -6.9999999999411795 for 8 qubits with 1 layers\n",
      "Step 280: Loss = -6.999999999999931 for 8 qubits with 1 layers\n",
      "Step 290: Loss = -6.999999999992526 for 8 qubits with 1 layers\n",
      "Step 300: Loss = -6.999999999999931 for 8 qubits with 1 layers\n",
      "Step 310: Loss = -6.999999999999029 for 8 qubits with 1 layers\n",
      "Step 320: Loss = -6.999999999999967 for 8 qubits with 1 layers\n",
      "Step 330: Loss = -6.9999999999998765 for 8 qubits with 1 layers\n",
      "Step 340: Loss = -7.000000000000011 for 8 qubits with 1 layers\n",
      "Step 350: Loss = -6.999999999999986 for 8 qubits with 1 layers\n",
      "Step 360: Loss = -6.999999999999988 for 8 qubits with 1 layers\n",
      "Step 370: Loss = -7.000000000000007 for 8 qubits with 1 layers\n",
      "Step 380: Loss = -6.999999999999961 for 8 qubits with 1 layers\n",
      "Step 390: Loss = -6.999999999999983 for 8 qubits with 1 layers\n",
      "Step 400: Loss = -6.999999999999995 for 8 qubits with 1 layers\n",
      "Step 410: Loss = -6.999999999999989 for 8 qubits with 1 layers\n",
      "Step 420: Loss = -6.999999999999977 for 8 qubits with 1 layers\n",
      "Step 430: Loss = -6.999999999999983 for 8 qubits with 1 layers\n",
      "Step 440: Loss = -6.999999999999998 for 8 qubits with 1 layers\n",
      "Step 450: Loss = -6.999999999999981 for 8 qubits with 1 layers\n",
      "Step 460: Loss = -6.999999999999975 for 8 qubits with 1 layers\n",
      "Step 470: Loss = -6.999999999999976 for 8 qubits with 1 layers\n",
      "Step 480: Loss = -6.999999999999988 for 8 qubits with 1 layers\n",
      "Step 490: Loss = -6.999999999999987 for 8 qubits with 1 layers\n",
      "Running optimization for 8 qubits and 2 layers\n",
      "Step 0: Loss = -0.3065065021528991 for 8 qubits with 2 layers\n",
      "Step 10: Loss = -2.1610516299156286 for 8 qubits with 2 layers\n",
      "Step 20: Loss = -4.0594551766833 for 8 qubits with 2 layers\n",
      "Step 30: Loss = -4.91950529168849 for 8 qubits with 2 layers\n",
      "Step 40: Loss = -5.777233605231057 for 8 qubits with 2 layers\n",
      "Step 50: Loss = -6.476615403940476 for 8 qubits with 2 layers\n",
      "Step 60: Loss = -6.806611296069736 for 8 qubits with 2 layers\n",
      "Step 70: Loss = -6.938507452952238 for 8 qubits with 2 layers\n",
      "Step 80: Loss = -6.974691883072278 for 8 qubits with 2 layers\n",
      "Step 90: Loss = -6.9892212594984375 for 8 qubits with 2 layers\n",
      "Step 100: Loss = -6.993764780505183 for 8 qubits with 2 layers\n",
      "Step 110: Loss = -6.9960906870819795 for 8 qubits with 2 layers\n",
      "Step 120: Loss = -6.9971763734843195 for 8 qubits with 2 layers\n",
      "Step 130: Loss = -6.9978884226758495 for 8 qubits with 2 layers\n",
      "Step 140: Loss = -6.998347668587108 for 8 qubits with 2 layers\n",
      "Step 150: Loss = -6.99868124496628 for 8 qubits with 2 layers\n",
      "Step 160: Loss = -6.9989318256414865 for 8 qubits with 2 layers\n",
      "Step 170: Loss = -6.999124604426191 for 8 qubits with 2 layers\n",
      "Step 180: Loss = -6.999276549930534 for 8 qubits with 2 layers\n",
      "Step 190: Loss = -6.999397717958627 for 8 qubits with 2 layers\n",
      "Step 200: Loss = -6.99949538775773 for 8 qubits with 2 layers\n",
      "Step 210: Loss = -6.999574846257081 for 8 qubits with 2 layers\n",
      "Step 220: Loss = -6.999639984702089 for 8 qubits with 2 layers\n",
      "Step 230: Loss = -6.99969374564318 for 8 qubits with 2 layers\n",
      "Step 240: Loss = -6.999738390791937 for 8 qubits with 2 layers\n",
      "Step 250: Loss = -6.9997756745681095 for 8 qubits with 2 layers\n",
      "Step 260: Loss = -6.999806970673684 for 8 qubits with 2 layers\n",
      "Step 270: Loss = -6.99983336508953 for 8 qubits with 2 layers\n",
      "Step 280: Loss = -6.999855722695602 for 8 qubits with 2 layers\n",
      "Step 290: Loss = -6.999874737406293 for 8 qubits with 2 layers\n",
      "Step 300: Loss = -6.999890969599653 for 8 qubits with 2 layers\n",
      "Step 310: Loss = -6.999904874635598 for 8 qubits with 2 layers\n",
      "Step 320: Loss = -6.999916824668787 for 8 qubits with 2 layers\n",
      "Step 330: Loss = -6.999927125495749 for 8 qubits with 2 layers\n",
      "Step 340: Loss = -6.9999360296588335 for 8 qubits with 2 layers\n",
      "Step 350: Loss = -6.999943746711995 for 8 qubits with 2 layers\n",
      "Step 360: Loss = -6.999950451314891 for 8 qubits with 2 layers\n",
      "Step 370: Loss = -6.999956289656406 for 8 qubits with 2 layers\n",
      "Step 380: Loss = -6.999961384580396 for 8 qubits with 2 layers\n",
      "Step 390: Loss = -6.999965839700295 for 8 qubits with 2 layers\n",
      "Step 400: Loss = -6.999969742718111 for 8 qubits with 2 layers\n",
      "Step 410: Loss = -6.999973168116006 for 8 qubits with 2 layers\n",
      "Step 420: Loss = -6.9999761793486766 for 8 qubits with 2 layers\n",
      "Step 430: Loss = -6.99997883063805 for 8 qubits with 2 layers\n",
      "Step 440: Loss = -6.999981168447612 for 8 qubits with 2 layers\n",
      "Step 450: Loss = -6.999983232699647 for 8 qubits with 2 layers\n",
      "Step 460: Loss = -6.9999850577835065 for 8 qubits with 2 layers\n",
      "Step 470: Loss = -6.999986673393826 for 8 qubits with 2 layers\n",
      "Step 480: Loss = -6.999988105230477 for 8 qubits with 2 layers\n",
      "Step 490: Loss = -6.999989375584052 for 8 qubits with 2 layers\n",
      "Running optimization for 8 qubits and 3 layers\n",
      "Step 0: Loss = -2.2265126014685244 for 8 qubits with 3 layers\n",
      "Step 10: Loss = -4.535785595302044 for 8 qubits with 3 layers\n",
      "Step 20: Loss = -6.15968998701677 for 8 qubits with 3 layers\n",
      "Step 30: Loss = -6.851654106343605 for 8 qubits with 3 layers\n",
      "Step 40: Loss = -6.9729860623528115 for 8 qubits with 3 layers\n",
      "Step 50: Loss = -6.998149887583935 for 8 qubits with 3 layers\n",
      "Step 60: Loss = -6.9979028948618005 for 8 qubits with 3 layers\n",
      "Step 70: Loss = -6.9988964804365645 for 8 qubits with 3 layers\n",
      "Step 80: Loss = -6.999710136301208 for 8 qubits with 3 layers\n",
      "Step 90: Loss = -6.999973803798511 for 8 qubits with 3 layers\n",
      "Step 100: Loss = -6.999982453977982 for 8 qubits with 3 layers\n",
      "Step 110: Loss = -6.9999880863983375 for 8 qubits with 3 layers\n",
      "Step 120: Loss = -6.9999899510553325 for 8 qubits with 3 layers\n",
      "Step 130: Loss = -6.999990978319351 for 8 qubits with 3 layers\n",
      "Step 140: Loss = -6.999991996720354 for 8 qubits with 3 layers\n",
      "Step 150: Loss = -6.99999260734879 for 8 qubits with 3 layers\n",
      "Step 160: Loss = -6.999993006125271 for 8 qubits with 3 layers\n",
      "Step 170: Loss = -6.999993346298689 for 8 qubits with 3 layers\n",
      "Step 180: Loss = -6.999993678751932 for 8 qubits with 3 layers\n",
      "Step 190: Loss = -6.999994009518524 for 8 qubits with 3 layers\n",
      "Step 200: Loss = -6.999994331432382 for 8 qubits with 3 layers\n",
      "Step 210: Loss = -6.999994640267054 for 8 qubits with 3 layers\n",
      "Step 220: Loss = -6.99999493736328 for 8 qubits with 3 layers\n",
      "Step 230: Loss = -6.999995224000651 for 8 qubits with 3 layers\n",
      "Step 240: Loss = -6.999995499510883 for 8 qubits with 3 layers\n",
      "Step 250: Loss = -6.999995763606487 for 8 qubits with 3 layers\n",
      "Step 260: Loss = -6.999996016275347 for 8 qubits with 3 layers\n",
      "Step 270: Loss = -6.999996257452148 for 8 qubits with 3 layers\n",
      "Step 280: Loss = -6.99999648721187 for 8 qubits with 3 layers\n",
      "Step 290: Loss = -6.999996705683293 for 8 qubits with 3 layers\n",
      "Step 300: Loss = -6.999996913064242 for 8 qubits with 3 layers\n",
      "Step 310: Loss = -6.999997109603344 for 8 qubits with 3 layers\n",
      "Step 320: Loss = -6.9999972955930465 for 8 qubits with 3 layers\n",
      "Step 330: Loss = -6.999997471360402 for 8 qubits with 3 layers\n",
      "Step 340: Loss = -6.999997637259426 for 8 qubits with 3 layers\n",
      "Step 350: Loss = -6.99999779366354 for 8 qubits with 3 layers\n",
      "Step 360: Loss = -6.999997940959657 for 8 qubits with 3 layers\n",
      "Step 370: Loss = -6.999998079542542 for 8 qubits with 3 layers\n",
      "Step 380: Loss = -6.999998209809955 for 8 qubits with 3 layers\n",
      "Step 390: Loss = -6.999998332158817 for 8 qubits with 3 layers\n",
      "Step 400: Loss = -6.999998446981751 for 8 qubits with 3 layers\n",
      "Step 410: Loss = -6.999998554664167 for 8 qubits with 3 layers\n",
      "Step 420: Loss = -6.999998655582302 for 8 qubits with 3 layers\n",
      "Step 430: Loss = -6.999998750101159 for 8 qubits with 3 layers\n",
      "Step 440: Loss = -6.99999883857337 for 8 qubits with 3 layers\n",
      "Step 450: Loss = -6.999998921338257 for 8 qubits with 3 layers\n",
      "Step 460: Loss = -6.9999989987211 for 8 qubits with 3 layers\n",
      "Step 470: Loss = -6.999999071033098 for 8 qubits with 3 layers\n",
      "Step 480: Loss = -6.999999138570646 for 8 qubits with 3 layers\n",
      "Step 490: Loss = -6.99999920161616 for 8 qubits with 3 layers\n",
      "Running optimization for 8 qubits and 4 layers\n",
      "Step 0: Loss = -1.1954093198818572 for 8 qubits with 4 layers\n",
      "Step 10: Loss = -3.7591827173161954 for 8 qubits with 4 layers\n",
      "Step 20: Loss = -4.403542856815826 for 8 qubits with 4 layers\n",
      "Step 30: Loss = -5.942121751838148 for 8 qubits with 4 layers\n",
      "Step 40: Loss = -6.7299597885113895 for 8 qubits with 4 layers\n",
      "Step 50: Loss = -6.921434926433346 for 8 qubits with 4 layers\n",
      "Step 60: Loss = -6.980846077410973 for 8 qubits with 4 layers\n",
      "Step 70: Loss = -6.994104454933991 for 8 qubits with 4 layers\n",
      "Step 80: Loss = -6.996251527588845 for 8 qubits with 4 layers\n",
      "Step 90: Loss = -6.9970825922068665 for 8 qubits with 4 layers\n",
      "Step 100: Loss = -6.9976520485010685 for 8 qubits with 4 layers\n",
      "Step 110: Loss = -6.998058755946463 for 8 qubits with 4 layers\n",
      "Step 120: Loss = -6.998348020426541 for 8 qubits with 4 layers\n",
      "Step 130: Loss = -6.998561939959395 for 8 qubits with 4 layers\n",
      "Step 140: Loss = -6.9987337868738475 for 8 qubits with 4 layers\n",
      "Step 150: Loss = -6.998878035236392 for 8 qubits with 4 layers\n",
      "Step 160: Loss = -6.99900194231038 for 8 qubits with 4 layers\n",
      "Step 170: Loss = -6.999109652684828 for 8 qubits with 4 layers\n",
      "Step 180: Loss = -6.999203801926707 for 8 qubits with 4 layers\n",
      "Step 190: Loss = -6.99928642832943 for 8 qubits with 4 layers\n",
      "Step 200: Loss = -6.99935915145431 for 8 qubits with 4 layers\n",
      "Step 210: Loss = -6.9994233246785775 for 8 qubits with 4 layers\n",
      "Step 220: Loss = -6.9994800858049295 for 8 qubits with 4 layers\n",
      "Step 230: Loss = -6.999530406844481 for 8 qubits with 4 layers\n",
      "Step 240: Loss = -6.999575122132173 for 8 qubits with 4 layers\n",
      "Step 250: Loss = -6.999614946012533 for 8 qubits with 4 layers\n",
      "Step 260: Loss = -6.999650490910328 for 8 qubits with 4 layers\n",
      "Step 270: Loss = -6.999682283660201 for 8 qubits with 4 layers\n",
      "Step 280: Loss = -6.999710778269162 for 8 qubits with 4 layers\n",
      "Step 290: Loss = -6.99973636686747 for 8 qubits with 4 layers\n",
      "Step 300: Loss = -6.999759388973873 for 8 qubits with 4 layers\n",
      "Step 310: Loss = -6.999780139230011 for 8 qubits with 4 layers\n",
      "Step 320: Loss = -6.999798873954185 for 8 qubits with 4 layers\n",
      "Step 330: Loss = -6.999815816661506 for 8 qubits with 4 layers\n",
      "Step 340: Loss = -6.99983116272795 for 8 qubits with 4 layers\n",
      "Step 350: Loss = -6.999845083334949 for 8 qubits with 4 layers\n",
      "Step 360: Loss = -6.999857728813291 for 8 qubits with 4 layers\n",
      "Step 370: Loss = -6.999869231482723 for 8 qubits with 4 layers\n",
      "Step 380: Loss = -6.999879708069164 for 8 qubits with 4 layers\n",
      "Step 390: Loss = -6.999889261765022 for 8 qubits with 4 layers\n",
      "Step 400: Loss = -6.999897983990907 for 8 qubits with 4 layers\n",
      "Step 410: Loss = -6.999905955904176 for 8 qubits with 4 layers\n",
      "Step 420: Loss = -6.999913249694655 for 8 qubits with 4 layers\n",
      "Step 430: Loss = -6.99991992969911 for 8 qubits with 4 layers\n",
      "Step 440: Loss = -6.999926053362017 for 8 qubits with 4 layers\n",
      "Step 450: Loss = -6.999931672067289 for 8 qubits with 4 layers\n",
      "Step 460: Loss = -6.999936831857062 for 8 qubits with 4 layers\n",
      "Step 470: Loss = -6.999941574056811 for 8 qubits with 4 layers\n",
      "Step 480: Loss = -6.999945935818911 for 8 qubits with 4 layers\n",
      "Step 490: Loss = -6.9999499505962195 for 8 qubits with 4 layers\n",
      "Running optimization for 8 qubits and 5 layers\n",
      "Step 0: Loss = -3.3172396143354552 for 8 qubits with 5 layers\n",
      "Step 10: Loss = -5.831856063671504 for 8 qubits with 5 layers\n",
      "Step 20: Loss = -6.374034916782645 for 8 qubits with 5 layers\n",
      "Step 30: Loss = -6.802194478178289 for 8 qubits with 5 layers\n",
      "Step 40: Loss = -6.924238564313498 for 8 qubits with 5 layers\n",
      "Step 50: Loss = -6.976305784076914 for 8 qubits with 5 layers\n",
      "Step 60: Loss = -6.9908712494892375 for 8 qubits with 5 layers\n",
      "Step 70: Loss = -6.995919773019583 for 8 qubits with 5 layers\n",
      "Step 80: Loss = -6.997264692091769 for 8 qubits with 5 layers\n",
      "Step 90: Loss = -6.9979842561151475 for 8 qubits with 5 layers\n",
      "Step 100: Loss = -6.998458446489893 for 8 qubits with 5 layers\n",
      "Step 110: Loss = -6.998743433670645 for 8 qubits with 5 layers\n",
      "Step 120: Loss = -6.99894626370263 for 8 qubits with 5 layers\n",
      "Step 130: Loss = -6.999104326756296 for 8 qubits with 5 layers\n",
      "Step 140: Loss = -6.999236048468335 for 8 qubits with 5 layers\n",
      "Step 150: Loss = -6.999351649109767 for 8 qubits with 5 layers\n",
      "Step 160: Loss = -6.9994554095294035 for 8 qubits with 5 layers\n",
      "Step 170: Loss = -6.999550700054791 for 8 qubits with 5 layers\n",
      "Step 180: Loss = -6.99963968481489 for 8 qubits with 5 layers\n",
      "Step 190: Loss = -6.999723315309508 for 8 qubits with 5 layers\n",
      "Step 200: Loss = -6.999800974143371 for 8 qubits with 5 layers\n",
      "Step 210: Loss = -6.999870128911221 for 8 qubits with 5 layers\n",
      "Step 220: Loss = -6.999926638360267 for 8 qubits with 5 layers\n",
      "Step 230: Loss = -6.999966546781817 for 8 qubits with 5 layers\n",
      "Step 240: Loss = -6.999989025208266 for 8 qubits with 5 layers\n",
      "Step 250: Loss = -6.999997989814712 for 8 qubits with 5 layers\n",
      "Step 260: Loss = -6.99999994313936 for 8 qubits with 5 layers\n",
      "Step 270: Loss = -6.999999917856532 for 8 qubits with 5 layers\n",
      "Step 280: Loss = -6.999999863443777 for 8 qubits with 5 layers\n",
      "Step 290: Loss = -6.999999937493178 for 8 qubits with 5 layers\n",
      "Step 300: Loss = -6.9999999898968195 for 8 qubits with 5 layers\n",
      "Step 310: Loss = -6.99999980508283 for 8 qubits with 5 layers\n",
      "Step 320: Loss = -6.999821455166752 for 8 qubits with 5 layers\n",
      "Step 330: Loss = -6.999478757386885 for 8 qubits with 5 layers\n",
      "Step 340: Loss = -6.999897626529986 for 8 qubits with 5 layers\n",
      "Step 350: Loss = -6.99999596661107 for 8 qubits with 5 layers\n",
      "Step 360: Loss = -6.999991942787984 for 8 qubits with 5 layers\n",
      "Step 370: Loss = -6.999987875218011 for 8 qubits with 5 layers\n",
      "Step 380: Loss = -6.999994459629036 for 8 qubits with 5 layers\n",
      "Step 390: Loss = -6.999998902900062 for 8 qubits with 5 layers\n",
      "Step 400: Loss = -6.999999790186811 for 8 qubits with 5 layers\n",
      "Step 410: Loss = -6.999834001184009 for 8 qubits with 5 layers\n",
      "Step 420: Loss = -6.998627635298535 for 8 qubits with 5 layers\n",
      "Step 430: Loss = -6.999516492013684 for 8 qubits with 5 layers\n",
      "Step 440: Loss = -6.999998669716115 for 8 qubits with 5 layers\n",
      "Step 450: Loss = -6.999901970059652 for 8 qubits with 5 layers\n",
      "Step 460: Loss = -6.999959267115318 for 8 qubits with 5 layers\n",
      "Step 470: Loss = -6.999993481369499 for 8 qubits with 5 layers\n",
      "Step 480: Loss = -6.999999430477963 for 8 qubits with 5 layers\n",
      "Step 490: Loss = -6.999312258181479 for 8 qubits with 5 layers\n",
      "Running optimization for 8 qubits and 6 layers\n",
      "Step 0: Loss = -2.3990573619409337 for 8 qubits with 6 layers\n",
      "Step 10: Loss = -5.891856127624919 for 8 qubits with 6 layers\n",
      "Step 20: Loss = -6.430989314803419 for 8 qubits with 6 layers\n",
      "Step 30: Loss = -6.6230935063583996 for 8 qubits with 6 layers\n",
      "Step 40: Loss = -6.748614281457587 for 8 qubits with 6 layers\n",
      "Step 50: Loss = -6.790227013513559 for 8 qubits with 6 layers\n",
      "Step 60: Loss = -6.820482046073674 for 8 qubits with 6 layers\n",
      "Step 70: Loss = -6.838490656698215 for 8 qubits with 6 layers\n",
      "Step 80: Loss = -6.845332074029681 for 8 qubits with 6 layers\n",
      "Step 90: Loss = -6.849541196874794 for 8 qubits with 6 layers\n",
      "Step 100: Loss = -6.854330020739486 for 8 qubits with 6 layers\n",
      "Step 110: Loss = -6.8591656762752615 for 8 qubits with 6 layers\n",
      "Step 120: Loss = -6.863967176859106 for 8 qubits with 6 layers\n",
      "Step 130: Loss = -6.868569033186983 for 8 qubits with 6 layers\n",
      "Step 140: Loss = -6.8727394164360405 for 8 qubits with 6 layers\n",
      "Step 150: Loss = -6.876695115934387 for 8 qubits with 6 layers\n",
      "Step 160: Loss = -6.880908094172453 for 8 qubits with 6 layers\n",
      "Step 170: Loss = -6.885917275061548 for 8 qubits with 6 layers\n",
      "Step 180: Loss = -6.892757594050723 for 8 qubits with 6 layers\n",
      "Step 190: Loss = -6.903565987103674 for 8 qubits with 6 layers\n",
      "Step 200: Loss = -6.920737087830078 for 8 qubits with 6 layers\n",
      "Step 210: Loss = -6.93636301587795 for 8 qubits with 6 layers\n",
      "Step 220: Loss = -6.947195018535149 for 8 qubits with 6 layers\n",
      "Step 230: Loss = -6.955392814724837 for 8 qubits with 6 layers\n",
      "Step 240: Loss = -6.960779494395884 for 8 qubits with 6 layers\n",
      "Step 250: Loss = -6.964731306055042 for 8 qubits with 6 layers\n",
      "Step 260: Loss = -6.96623710517989 for 8 qubits with 6 layers\n",
      "Step 270: Loss = -6.968433243220614 for 8 qubits with 6 layers\n",
      "Step 280: Loss = -6.968704836927487 for 8 qubits with 6 layers\n",
      "Step 290: Loss = -6.969992621435713 for 8 qubits with 6 layers\n",
      "Step 300: Loss = -6.971635643716121 for 8 qubits with 6 layers\n",
      "Step 310: Loss = -6.972826714523396 for 8 qubits with 6 layers\n",
      "Step 320: Loss = -6.973978877314739 for 8 qubits with 6 layers\n",
      "Step 330: Loss = -6.975022635986091 for 8 qubits with 6 layers\n",
      "Step 340: Loss = -6.976141008946333 for 8 qubits with 6 layers\n",
      "Step 350: Loss = -6.97731992841115 for 8 qubits with 6 layers\n",
      "Step 360: Loss = -6.978576701953232 for 8 qubits with 6 layers\n",
      "Step 370: Loss = -6.9799425855184785 for 8 qubits with 6 layers\n",
      "Step 380: Loss = -6.9814411499708005 for 8 qubits with 6 layers\n",
      "Step 390: Loss = -6.983042849435199 for 8 qubits with 6 layers\n",
      "Step 400: Loss = -6.979154649964307 for 8 qubits with 6 layers\n",
      "Step 410: Loss = -6.982852983638512 for 8 qubits with 6 layers\n",
      "Step 420: Loss = -6.9864327008933 for 8 qubits with 6 layers\n",
      "Step 430: Loss = -6.988670837828394 for 8 qubits with 6 layers\n",
      "Step 440: Loss = -6.9902153878675435 for 8 qubits with 6 layers\n",
      "Step 450: Loss = -6.991957563048457 for 8 qubits with 6 layers\n",
      "Step 460: Loss = -6.99357902628841 for 8 qubits with 6 layers\n",
      "Step 470: Loss = -6.995129504390573 for 8 qubits with 6 layers\n",
      "Step 480: Loss = -6.9964622134106165 for 8 qubits with 6 layers\n",
      "Step 490: Loss = -6.988475006359687 for 8 qubits with 6 layers\n",
      "Running optimization for 8 qubits and 7 layers\n",
      "Step 0: Loss = -4.8125621734598045 for 8 qubits with 7 layers\n",
      "Step 10: Loss = -6.222245520833153 for 8 qubits with 7 layers\n",
      "Step 20: Loss = -6.454690773570174 for 8 qubits with 7 layers\n",
      "Step 30: Loss = -6.569037916841935 for 8 qubits with 7 layers\n",
      "Step 40: Loss = -6.632417234056733 for 8 qubits with 7 layers\n",
      "Step 50: Loss = -6.660064620089272 for 8 qubits with 7 layers\n",
      "Step 60: Loss = -6.681148166575756 for 8 qubits with 7 layers\n",
      "Step 70: Loss = -6.699528354719208 for 8 qubits with 7 layers\n",
      "Step 80: Loss = -6.7320724796347875 for 8 qubits with 7 layers\n",
      "Step 90: Loss = -6.800835536167067 for 8 qubits with 7 layers\n",
      "Step 100: Loss = -6.897064063581454 for 8 qubits with 7 layers\n",
      "Step 110: Loss = -6.93703095203608 for 8 qubits with 7 layers\n",
      "Step 120: Loss = -6.940997541389151 for 8 qubits with 7 layers\n",
      "Step 130: Loss = -6.94030846265365 for 8 qubits with 7 layers\n",
      "Step 140: Loss = -6.940405216812796 for 8 qubits with 7 layers\n",
      "Step 150: Loss = -6.9396353431822035 for 8 qubits with 7 layers\n",
      "Step 160: Loss = -6.939905699698702 for 8 qubits with 7 layers\n",
      "Step 170: Loss = -6.940914034873618 for 8 qubits with 7 layers\n",
      "Step 180: Loss = -6.941280674753628 for 8 qubits with 7 layers\n",
      "Step 190: Loss = -6.941292054861371 for 8 qubits with 7 layers\n",
      "Step 200: Loss = -6.941294635057619 for 8 qubits with 7 layers\n",
      "Step 210: Loss = -6.941346866528367 for 8 qubits with 7 layers\n",
      "Step 220: Loss = -6.9413938559726915 for 8 qubits with 7 layers\n",
      "Step 230: Loss = -6.94150935658751 for 8 qubits with 7 layers\n",
      "Step 240: Loss = -6.935824619494967 for 8 qubits with 7 layers\n",
      "Step 250: Loss = -6.939971780279033 for 8 qubits with 7 layers\n",
      "Step 260: Loss = -6.9443867989394406 for 8 qubits with 7 layers\n",
      "Step 270: Loss = -6.950459314422474 for 8 qubits with 7 layers\n",
      "Step 280: Loss = -6.972227781024014 for 8 qubits with 7 layers\n",
      "Step 290: Loss = -6.9938529094469475 for 8 qubits with 7 layers\n",
      "Step 300: Loss = -6.999023271001142 for 8 qubits with 7 layers\n",
      "Step 310: Loss = -6.99835585251012 for 8 qubits with 7 layers\n",
      "Step 320: Loss = -6.994321375976529 for 8 qubits with 7 layers\n",
      "Step 330: Loss = -6.997794155544466 for 8 qubits with 7 layers\n",
      "Step 340: Loss = -6.998884039824282 for 8 qubits with 7 layers\n",
      "Step 350: Loss = -6.998886635133719 for 8 qubits with 7 layers\n",
      "Step 360: Loss = -6.999157627094576 for 8 qubits with 7 layers\n",
      "Step 370: Loss = -6.998491398313265 for 8 qubits with 7 layers\n",
      "Step 380: Loss = -6.99868535776931 for 8 qubits with 7 layers\n",
      "Step 390: Loss = -6.998497223531303 for 8 qubits with 7 layers\n",
      "Step 400: Loss = -6.998933586066285 for 8 qubits with 7 layers\n",
      "Step 410: Loss = -6.998338605532387 for 8 qubits with 7 layers\n",
      "Step 420: Loss = -6.998335647223961 for 8 qubits with 7 layers\n",
      "Step 430: Loss = -6.998615647707155 for 8 qubits with 7 layers\n",
      "Step 440: Loss = -6.9990792114575076 for 8 qubits with 7 layers\n",
      "Step 450: Loss = -6.997928939624794 for 8 qubits with 7 layers\n",
      "Step 460: Loss = -6.998686189120969 for 8 qubits with 7 layers\n",
      "Step 470: Loss = -6.998858762689083 for 8 qubits with 7 layers\n",
      "Step 480: Loss = -6.999096959844081 for 8 qubits with 7 layers\n",
      "Step 490: Loss = -6.998654297115596 for 8 qubits with 7 layers\n",
      "Running optimization for 8 qubits and 8 layers\n",
      "Step 0: Loss = -2.06486491581532 for 8 qubits with 8 layers\n",
      "Step 10: Loss = -5.928435690593797 for 8 qubits with 8 layers\n",
      "Step 20: Loss = -6.583076673776951 for 8 qubits with 8 layers\n",
      "Step 30: Loss = -6.72935856541436 for 8 qubits with 8 layers\n",
      "Step 40: Loss = -6.802144430875648 for 8 qubits with 8 layers\n",
      "Step 50: Loss = -6.861262014363314 for 8 qubits with 8 layers\n",
      "Step 60: Loss = -6.903685855080423 for 8 qubits with 8 layers\n",
      "Step 70: Loss = -6.926677330108457 for 8 qubits with 8 layers\n",
      "Step 80: Loss = -6.931264674577989 for 8 qubits with 8 layers\n",
      "Step 90: Loss = -6.9375566344771045 for 8 qubits with 8 layers\n",
      "Step 100: Loss = -6.942745434557541 for 8 qubits with 8 layers\n",
      "Step 110: Loss = -6.949910938256917 for 8 qubits with 8 layers\n",
      "Step 120: Loss = -6.958255099850436 for 8 qubits with 8 layers\n",
      "Step 130: Loss = -6.965192906044436 for 8 qubits with 8 layers\n",
      "Step 140: Loss = -6.968601316275093 for 8 qubits with 8 layers\n",
      "Step 150: Loss = -6.96957783498282 for 8 qubits with 8 layers\n",
      "Step 160: Loss = -6.969871306168825 for 8 qubits with 8 layers\n",
      "Step 170: Loss = -6.970142003860052 for 8 qubits with 8 layers\n",
      "Step 180: Loss = -6.970483217589798 for 8 qubits with 8 layers\n",
      "Step 190: Loss = -6.970796912608305 for 8 qubits with 8 layers\n",
      "Step 200: Loss = -6.971097536874411 for 8 qubits with 8 layers\n",
      "Step 210: Loss = -6.971399485213778 for 8 qubits with 8 layers\n",
      "Step 220: Loss = -6.971704158588637 for 8 qubits with 8 layers\n",
      "Step 230: Loss = -6.972009307479492 for 8 qubits with 8 layers\n",
      "Step 240: Loss = -6.972315284924467 for 8 qubits with 8 layers\n",
      "Step 250: Loss = -6.972622060211734 for 8 qubits with 8 layers\n",
      "Step 260: Loss = -6.972929346976617 for 8 qubits with 8 layers\n",
      "Step 270: Loss = -6.9732369535822185 for 8 qubits with 8 layers\n",
      "Step 280: Loss = -6.973544696594794 for 8 qubits with 8 layers\n",
      "Step 290: Loss = -6.973852383109614 for 8 qubits with 8 layers\n",
      "Step 300: Loss = -6.974159805486384 for 8 qubits with 8 layers\n",
      "Step 310: Loss = -6.968603527575197 for 8 qubits with 8 layers\n",
      "Step 320: Loss = -6.973669238071544 for 8 qubits with 8 layers\n",
      "Step 330: Loss = -6.974000243845412 for 8 qubits with 8 layers\n",
      "Step 340: Loss = -6.974267461253607 for 8 qubits with 8 layers\n",
      "Step 350: Loss = -6.975492197543379 for 8 qubits with 8 layers\n",
      "Step 360: Loss = -6.975795813360443 for 8 qubits with 8 layers\n",
      "Step 370: Loss = -6.976076300067787 for 8 qubits with 8 layers\n",
      "Step 380: Loss = -6.9763185595014745 for 8 qubits with 8 layers\n",
      "Step 390: Loss = -6.9766005458527545 for 8 qubits with 8 layers\n",
      "Step 400: Loss = -6.976878641804812 for 8 qubits with 8 layers\n",
      "Step 410: Loss = -6.977129360496943 for 8 qubits with 8 layers\n",
      "Step 420: Loss = -6.967840869705717 for 8 qubits with 8 layers\n",
      "Step 430: Loss = -6.973947655768788 for 8 qubits with 8 layers\n",
      "Step 440: Loss = -6.976367380494973 for 8 qubits with 8 layers\n",
      "Step 450: Loss = -6.978115762768564 for 8 qubits with 8 layers\n",
      "Step 460: Loss = -6.978165811597874 for 8 qubits with 8 layers\n",
      "Step 470: Loss = -6.978549074672408 for 8 qubits with 8 layers\n",
      "Step 480: Loss = -6.978750955925295 for 8 qubits with 8 layers\n",
      "Step 490: Loss = -6.979063153115911 for 8 qubits with 8 layers\n",
      "Running optimization for 8 qubits and 9 layers\n",
      "Step 0: Loss = -2.5795100779699816 for 8 qubits with 9 layers\n",
      "Step 10: Loss = -6.175816335936779 for 8 qubits with 9 layers\n",
      "Step 20: Loss = -6.757048395115909 for 8 qubits with 9 layers\n",
      "Step 30: Loss = -6.952404485336431 for 8 qubits with 9 layers\n",
      "Step 40: Loss = -6.9654672196706 for 8 qubits with 9 layers\n",
      "Step 50: Loss = -6.982404986609154 for 8 qubits with 9 layers\n",
      "Step 60: Loss = -6.995211045974626 for 8 qubits with 9 layers\n",
      "Step 70: Loss = -6.997894611935266 for 8 qubits with 9 layers\n",
      "Step 80: Loss = -6.998900759919413 for 8 qubits with 9 layers\n",
      "Step 90: Loss = -6.99964804397095 for 8 qubits with 9 layers\n",
      "Step 100: Loss = -6.999829365679358 for 8 qubits with 9 layers\n",
      "Step 110: Loss = -6.9999103779313305 for 8 qubits with 9 layers\n",
      "Step 120: Loss = -6.999955859585955 for 8 qubits with 9 layers\n",
      "Step 130: Loss = -6.999974209911055 for 8 qubits with 9 layers\n",
      "Step 140: Loss = -6.999987656973531 for 8 qubits with 9 layers\n",
      "Step 150: Loss = -6.999993876178715 for 8 qubits with 9 layers\n",
      "Step 160: Loss = -6.999996998017547 for 8 qubits with 9 layers\n",
      "Step 170: Loss = -6.9999986930460745 for 8 qubits with 9 layers\n",
      "Step 180: Loss = -6.999999470020041 for 8 qubits with 9 layers\n",
      "Step 190: Loss = -6.999999805061471 for 8 qubits with 9 layers\n",
      "Step 200: Loss = -6.999999939647161 for 8 qubits with 9 layers\n",
      "Step 210: Loss = -6.999999985413753 for 8 qubits with 9 layers\n",
      "Step 220: Loss = -6.99999999761746 for 8 qubits with 9 layers\n",
      "Step 230: Loss = -6.9999999997858 for 8 qubits with 9 layers\n",
      "Step 240: Loss = -6.999999999899075 for 8 qubits with 9 layers\n",
      "Step 250: Loss = -6.9999999998669304 for 8 qubits with 9 layers\n",
      "Step 260: Loss = -6.999999999903992 for 8 qubits with 9 layers\n",
      "Step 270: Loss = -6.99999999996175 for 8 qubits with 9 layers\n",
      "Step 280: Loss = -6.999999999990319 for 8 qubits with 9 layers\n",
      "Step 290: Loss = -6.999999999998731 for 8 qubits with 9 layers\n",
      "Step 300: Loss = -6.999999999999417 for 8 qubits with 9 layers\n",
      "Step 310: Loss = -6.9999999994248405 for 8 qubits with 9 layers\n",
      "Step 320: Loss = -6.999922008713215 for 8 qubits with 9 layers\n",
      "Step 330: Loss = -6.999909521541976 for 8 qubits with 9 layers\n",
      "Step 340: Loss = -6.999542618020273 for 8 qubits with 9 layers\n",
      "Step 350: Loss = -6.998386947696488 for 8 qubits with 9 layers\n",
      "Step 360: Loss = -6.999101669865984 for 8 qubits with 9 layers\n",
      "Step 370: Loss = -6.999766271001545 for 8 qubits with 9 layers\n",
      "Step 380: Loss = -6.999996843317562 for 8 qubits with 9 layers\n",
      "Step 390: Loss = -6.999944960717994 for 8 qubits with 9 layers\n",
      "Step 400: Loss = -6.999976542899443 for 8 qubits with 9 layers\n",
      "Step 410: Loss = -6.999991526453597 for 8 qubits with 9 layers\n",
      "Step 420: Loss = -6.999999246889626 for 8 qubits with 9 layers\n",
      "Step 430: Loss = -6.9999997853788205 for 8 qubits with 9 layers\n",
      "Step 440: Loss = -6.999999178530484 for 8 qubits with 9 layers\n",
      "Step 450: Loss = -6.999999890861528 for 8 qubits with 9 layers\n",
      "Step 460: Loss = -6.99995304900162 for 8 qubits with 9 layers\n",
      "Step 470: Loss = -6.991564134047541 for 8 qubits with 9 layers\n",
      "Step 480: Loss = -6.993203514478432 for 8 qubits with 9 layers\n",
      "Step 490: Loss = -6.999732767332888 for 8 qubits with 9 layers\n",
      "Running optimization for 8 qubits and 10 layers\n",
      "Step 0: Loss = -1.1844206525344791 for 8 qubits with 10 layers\n",
      "Step 10: Loss = -6.2836784373964045 for 8 qubits with 10 layers\n",
      "Step 20: Loss = -6.5603719863873176 for 8 qubits with 10 layers\n",
      "Step 30: Loss = -6.864645458063776 for 8 qubits with 10 layers\n",
      "Step 40: Loss = -6.942106132713204 for 8 qubits with 10 layers\n",
      "Step 50: Loss = -6.966667109770291 for 8 qubits with 10 layers\n",
      "Step 60: Loss = -6.979089405187144 for 8 qubits with 10 layers\n",
      "Step 70: Loss = -6.989892084368828 for 8 qubits with 10 layers\n",
      "Step 80: Loss = -6.995871377597914 for 8 qubits with 10 layers\n",
      "Step 90: Loss = -6.9989512844861625 for 8 qubits with 10 layers\n",
      "Step 100: Loss = -6.999764105839164 for 8 qubits with 10 layers\n",
      "Step 110: Loss = -6.999949580101532 for 8 qubits with 10 layers\n",
      "Step 120: Loss = -6.999954682715081 for 8 qubits with 10 layers\n",
      "Step 130: Loss = -6.9999769643713705 for 8 qubits with 10 layers\n",
      "Step 140: Loss = -6.999994445088512 for 8 qubits with 10 layers\n",
      "Step 150: Loss = -6.999999367647225 for 8 qubits with 10 layers\n",
      "Step 160: Loss = -6.9999991502294 for 8 qubits with 10 layers\n",
      "Step 170: Loss = -6.999999640216617 for 8 qubits with 10 layers\n",
      "Step 180: Loss = -6.99999996321046 for 8 qubits with 10 layers\n",
      "Step 190: Loss = -6.9999999703828575 for 8 qubits with 10 layers\n",
      "Step 200: Loss = -6.999999982892723 for 8 qubits with 10 layers\n",
      "Step 210: Loss = -6.9999999973269755 for 8 qubits with 10 layers\n",
      "Step 220: Loss = -6.999999998969337 for 8 qubits with 10 layers\n",
      "Step 230: Loss = -6.999999999092616 for 8 qubits with 10 layers\n",
      "Step 240: Loss = -6.999999999919433 for 8 qubits with 10 layers\n",
      "Step 250: Loss = -6.99999999993387 for 8 qubits with 10 layers\n",
      "Step 260: Loss = -6.999999999605118 for 8 qubits with 10 layers\n",
      "Step 270: Loss = -6.999996989850497 for 8 qubits with 10 layers\n",
      "Step 280: Loss = -6.9998823740403635 for 8 qubits with 10 layers\n",
      "Step 290: Loss = -6.994466703079656 for 8 qubits with 10 layers\n",
      "Step 300: Loss = -6.999038436397092 for 8 qubits with 10 layers\n",
      "Step 310: Loss = -6.99984594413271 for 8 qubits with 10 layers\n",
      "Step 320: Loss = -6.999849556968425 for 8 qubits with 10 layers\n",
      "Step 330: Loss = -6.999898922025612 for 8 qubits with 10 layers\n",
      "Step 340: Loss = -6.999984935110145 for 8 qubits with 10 layers\n",
      "Step 350: Loss = -6.9999835982534355 for 8 qubits with 10 layers\n",
      "Step 360: Loss = -6.999992184067678 for 8 qubits with 10 layers\n",
      "Step 370: Loss = -6.999999998315957 for 8 qubits with 10 layers\n",
      "Step 380: Loss = -6.999972211170926 for 8 qubits with 10 layers\n",
      "Step 390: Loss = -6.984620871588921 for 8 qubits with 10 layers\n",
      "Step 400: Loss = -6.996955140386637 for 8 qubits with 10 layers\n",
      "Step 410: Loss = -6.999024017610653 for 8 qubits with 10 layers\n",
      "Step 420: Loss = -6.999163398850897 for 8 qubits with 10 layers\n",
      "Step 430: Loss = -6.9996137487291685 for 8 qubits with 10 layers\n",
      "Step 440: Loss = -6.999995509386347 for 8 qubits with 10 layers\n",
      "Step 450: Loss = -6.999518506724749 for 8 qubits with 10 layers\n",
      "Step 460: Loss = -6.994475997397333 for 8 qubits with 10 layers\n",
      "Step 470: Loss = -6.9973016768611815 for 8 qubits with 10 layers\n",
      "Step 480: Loss = -6.999897249798604 for 8 qubits with 10 layers\n",
      "Step 490: Loss = -6.999575424420429 for 8 qubits with 10 layers\n",
      "Running optimization for 8 qubits and 11 layers\n",
      "Step 0: Loss = -1.7157008479672007 for 8 qubits with 11 layers\n",
      "Step 10: Loss = -6.2972614765584245 for 8 qubits with 11 layers\n",
      "Step 20: Loss = -6.925787658356859 for 8 qubits with 11 layers\n",
      "Step 30: Loss = -6.938556872915378 for 8 qubits with 11 layers\n",
      "Step 40: Loss = -6.9664145450272414 for 8 qubits with 11 layers\n",
      "Step 50: Loss = -6.989324169235967 for 8 qubits with 11 layers\n",
      "Step 60: Loss = -6.995880048934741 for 8 qubits with 11 layers\n",
      "Step 70: Loss = -6.998777946142913 for 8 qubits with 11 layers\n",
      "Step 80: Loss = -6.999518556066581 for 8 qubits with 11 layers\n",
      "Step 90: Loss = -6.999775764094273 for 8 qubits with 11 layers\n",
      "Step 100: Loss = -6.99991605981503 for 8 qubits with 11 layers\n",
      "Step 110: Loss = -6.99997769219352 for 8 qubits with 11 layers\n",
      "Step 120: Loss = -6.999990010745021 for 8 qubits with 11 layers\n",
      "Step 130: Loss = -6.999996234493542 for 8 qubits with 11 layers\n",
      "Step 140: Loss = -6.999999027216182 for 8 qubits with 11 layers\n",
      "Step 150: Loss = -6.999999443175287 for 8 qubits with 11 layers\n",
      "Step 160: Loss = -6.999999851712474 for 8 qubits with 11 layers\n",
      "Step 170: Loss = -6.999999958291405 for 8 qubits with 11 layers\n",
      "Step 180: Loss = -6.999999976068006 for 8 qubits with 11 layers\n",
      "Step 190: Loss = -6.999999814919495 for 8 qubits with 11 layers\n",
      "Step 200: Loss = -6.9948258279335365 for 8 qubits with 11 layers\n",
      "Step 210: Loss = -6.998386510540913 for 8 qubits with 11 layers\n",
      "Step 220: Loss = -6.999348307051869 for 8 qubits with 11 layers\n",
      "Step 230: Loss = -6.999816841128275 for 8 qubits with 11 layers\n",
      "Step 240: Loss = -6.999969038765361 for 8 qubits with 11 layers\n",
      "Step 250: Loss = -6.9999938141749265 for 8 qubits with 11 layers\n",
      "Step 260: Loss = -6.999976283632686 for 8 qubits with 11 layers\n",
      "Step 270: Loss = -6.999998501063217 for 8 qubits with 11 layers\n",
      "Step 280: Loss = -6.999472940870174 for 8 qubits with 11 layers\n",
      "Step 290: Loss = -6.995528132447684 for 8 qubits with 11 layers\n",
      "Step 300: Loss = -6.996810791586798 for 8 qubits with 11 layers\n",
      "Step 310: Loss = -6.998717626216472 for 8 qubits with 11 layers\n",
      "Step 320: Loss = -6.999989676858966 for 8 qubits with 11 layers\n",
      "Step 330: Loss = -6.9998331135549146 for 8 qubits with 11 layers\n",
      "Step 340: Loss = -6.999975098729293 for 8 qubits with 11 layers\n",
      "Step 350: Loss = -6.999935139786508 for 8 qubits with 11 layers\n",
      "Step 360: Loss = -6.999035246237398 for 8 qubits with 11 layers\n",
      "Step 370: Loss = -6.994947457836648 for 8 qubits with 11 layers\n",
      "Step 380: Loss = -6.9973247256432805 for 8 qubits with 11 layers\n",
      "Step 390: Loss = -6.999103995013705 for 8 qubits with 11 layers\n",
      "Step 400: Loss = -6.9995069276959905 for 8 qubits with 11 layers\n",
      "Step 410: Loss = -6.999891298304777 for 8 qubits with 11 layers\n",
      "Step 420: Loss = -6.999208000109667 for 8 qubits with 11 layers\n",
      "Step 430: Loss = -6.999940417655777 for 8 qubits with 11 layers\n",
      "Step 440: Loss = -6.999359716895799 for 8 qubits with 11 layers\n",
      "Step 450: Loss = -6.998993091898845 for 8 qubits with 11 layers\n",
      "Step 460: Loss = -6.999880375832447 for 8 qubits with 11 layers\n",
      "Step 470: Loss = -6.998774628448342 for 8 qubits with 11 layers\n",
      "Step 480: Loss = -6.999501650680808 for 8 qubits with 11 layers\n",
      "Step 490: Loss = -6.999723912175458 for 8 qubits with 11 layers\n",
      "Running optimization for 8 qubits and 12 layers\n",
      "Step 0: Loss = -2.704347888845204 for 8 qubits with 12 layers\n",
      "Step 10: Loss = -6.348133239878512 for 8 qubits with 12 layers\n",
      "Step 20: Loss = -6.8014759224096775 for 8 qubits with 12 layers\n",
      "Step 30: Loss = -6.922087460385846 for 8 qubits with 12 layers\n",
      "Step 40: Loss = -6.97585006642744 for 8 qubits with 12 layers\n",
      "Step 50: Loss = -6.989560007533747 for 8 qubits with 12 layers\n",
      "Step 60: Loss = -6.9962434196403205 for 8 qubits with 12 layers\n",
      "Step 70: Loss = -6.998733501533451 for 8 qubits with 12 layers\n",
      "Step 80: Loss = -6.999482809924206 for 8 qubits with 12 layers\n",
      "Step 90: Loss = -6.999884027546807 for 8 qubits with 12 layers\n",
      "Step 100: Loss = -6.999964246889629 for 8 qubits with 12 layers\n",
      "Step 110: Loss = -6.999983762785345 for 8 qubits with 12 layers\n",
      "Step 120: Loss = -6.999994600704323 for 8 qubits with 12 layers\n",
      "Step 130: Loss = -6.999997933194177 for 8 qubits with 12 layers\n",
      "Step 140: Loss = -6.999999017347594 for 8 qubits with 12 layers\n",
      "Step 150: Loss = -6.999999733035833 for 8 qubits with 12 layers\n",
      "Step 160: Loss = -6.999999888214612 for 8 qubits with 12 layers\n",
      "Step 170: Loss = -6.999999946954556 for 8 qubits with 12 layers\n",
      "Step 180: Loss = -6.999999984575235 for 8 qubits with 12 layers\n",
      "Step 190: Loss = -6.999999994120274 for 8 qubits with 12 layers\n",
      "Step 200: Loss = -6.999999997824286 for 8 qubits with 12 layers\n",
      "Step 210: Loss = -6.999999999438668 for 8 qubits with 12 layers\n",
      "Step 220: Loss = -6.999999999725925 for 8 qubits with 12 layers\n",
      "Step 230: Loss = -6.999999999880436 for 8 qubits with 12 layers\n",
      "Step 240: Loss = -6.999999999958448 for 8 qubits with 12 layers\n",
      "Step 250: Loss = -6.999999999983156 for 8 qubits with 12 layers\n",
      "Step 260: Loss = -6.999999999993804 for 8 qubits with 12 layers\n",
      "Step 270: Loss = -6.9999999999857 for 8 qubits with 12 layers\n",
      "Step 280: Loss = -6.999999945081654 for 8 qubits with 12 layers\n",
      "Step 290: Loss = -6.9885307036812385 for 8 qubits with 12 layers\n",
      "Step 300: Loss = -6.991592408346232 for 8 qubits with 12 layers\n",
      "Step 310: Loss = -6.999081531736946 for 8 qubits with 12 layers\n",
      "Step 320: Loss = -6.9996989621159855 for 8 qubits with 12 layers\n",
      "Step 330: Loss = -6.9996291048366475 for 8 qubits with 12 layers\n",
      "Step 340: Loss = -6.999873353002376 for 8 qubits with 12 layers\n",
      "Step 350: Loss = -6.999988846114226 for 8 qubits with 12 layers\n",
      "Step 360: Loss = -6.9999891824420795 for 8 qubits with 12 layers\n",
      "Step 370: Loss = -6.999989837286147 for 8 qubits with 12 layers\n",
      "Step 380: Loss = -6.9999983720404355 for 8 qubits with 12 layers\n",
      "Step 390: Loss = -6.999996488675267 for 8 qubits with 12 layers\n",
      "Step 400: Loss = -6.999999750687399 for 8 qubits with 12 layers\n",
      "Step 410: Loss = -6.999989875392613 for 8 qubits with 12 layers\n",
      "Step 420: Loss = -6.992808949819913 for 8 qubits with 12 layers\n",
      "Step 430: Loss = -6.990833470229399 for 8 qubits with 12 layers\n",
      "Step 440: Loss = -6.997445346793193 for 8 qubits with 12 layers\n",
      "Step 450: Loss = -6.999287467244981 for 8 qubits with 12 layers\n",
      "Step 460: Loss = -6.999559727073213 for 8 qubits with 12 layers\n",
      "Step 470: Loss = -6.999785542286717 for 8 qubits with 12 layers\n",
      "Step 480: Loss = -6.993093091514435 for 8 qubits with 12 layers\n",
      "Step 490: Loss = -6.998989295262154 for 8 qubits with 12 layers\n",
      "Running optimization for 8 qubits and 13 layers\n",
      "Step 0: Loss = -3.4276216457184954 for 8 qubits with 13 layers\n",
      "Step 10: Loss = -6.666271428965356 for 8 qubits with 13 layers\n",
      "Step 20: Loss = -6.862132511081086 for 8 qubits with 13 layers\n",
      "Step 30: Loss = -6.953464866389428 for 8 qubits with 13 layers\n",
      "Step 40: Loss = -6.982728040676327 for 8 qubits with 13 layers\n",
      "Step 50: Loss = -6.995051934864302 for 8 qubits with 13 layers\n",
      "Step 60: Loss = -6.9977007776961475 for 8 qubits with 13 layers\n",
      "Step 70: Loss = -6.999278410841544 for 8 qubits with 13 layers\n",
      "Step 80: Loss = -6.99956570221081 for 8 qubits with 13 layers\n",
      "Step 90: Loss = -6.999903547492317 for 8 qubits with 13 layers\n",
      "Step 100: Loss = -6.99993976569082 for 8 qubits with 13 layers\n",
      "Step 110: Loss = -6.999983947871128 for 8 qubits with 13 layers\n",
      "Step 120: Loss = -6.999997573789184 for 8 qubits with 13 layers\n",
      "Step 130: Loss = -6.999997890413417 for 8 qubits with 13 layers\n",
      "Step 140: Loss = -6.999999349848445 for 8 qubits with 13 layers\n",
      "Step 150: Loss = -6.999999710650046 for 8 qubits with 13 layers\n",
      "Step 160: Loss = -6.999999884747077 for 8 qubits with 13 layers\n",
      "Step 170: Loss = -6.999999952485931 for 8 qubits with 13 layers\n",
      "Step 180: Loss = -6.999999988721288 for 8 qubits with 13 layers\n",
      "Step 190: Loss = -6.999999997371225 for 8 qubits with 13 layers\n",
      "Step 200: Loss = -6.999999998510343 for 8 qubits with 13 layers\n",
      "Step 210: Loss = -6.999999999833471 for 8 qubits with 13 layers\n",
      "Step 220: Loss = -6.999999999752996 for 8 qubits with 13 layers\n",
      "Step 230: Loss = -6.99999999991022 for 8 qubits with 13 layers\n",
      "Step 240: Loss = -6.999999999964929 for 8 qubits with 13 layers\n",
      "Step 250: Loss = -6.999999999992822 for 8 qubits with 13 layers\n",
      "Step 260: Loss = -6.999999999996229 for 8 qubits with 13 layers\n",
      "Step 270: Loss = -6.999999999998699 for 8 qubits with 13 layers\n",
      "Step 280: Loss = -6.999999999999302 for 8 qubits with 13 layers\n",
      "Step 290: Loss = -6.999999999999025 for 8 qubits with 13 layers\n",
      "Step 300: Loss = -6.999999978545221 for 8 qubits with 13 layers\n",
      "Step 310: Loss = -6.9795052646059785 for 8 qubits with 13 layers\n",
      "Step 320: Loss = -6.974629417693126 for 8 qubits with 13 layers\n",
      "Step 330: Loss = -6.995045734086398 for 8 qubits with 13 layers\n",
      "Step 340: Loss = -6.9995000161915675 for 8 qubits with 13 layers\n",
      "Step 350: Loss = -6.999764221300706 for 8 qubits with 13 layers\n",
      "Step 360: Loss = -6.999969101443946 for 8 qubits with 13 layers\n",
      "Step 370: Loss = -6.9998728255102165 for 8 qubits with 13 layers\n",
      "Step 380: Loss = -6.999988337591455 for 8 qubits with 13 layers\n",
      "Step 390: Loss = -6.999989026263954 for 8 qubits with 13 layers\n",
      "Step 400: Loss = -6.999998819212517 for 8 qubits with 13 layers\n",
      "Step 410: Loss = -6.9999965048111035 for 8 qubits with 13 layers\n",
      "Step 420: Loss = -6.999998471031188 for 8 qubits with 13 layers\n",
      "Step 430: Loss = -6.999999945542308 for 8 qubits with 13 layers\n",
      "Step 440: Loss = -6.999998251877419 for 8 qubits with 13 layers\n",
      "Step 450: Loss = -6.998577155311427 for 8 qubits with 13 layers\n",
      "Step 460: Loss = -6.998668852981368 for 8 qubits with 13 layers\n",
      "Step 470: Loss = -6.999426111945397 for 8 qubits with 13 layers\n",
      "Step 480: Loss = -6.998153235473048 for 8 qubits with 13 layers\n",
      "Step 490: Loss = -6.999166980302698 for 8 qubits with 13 layers\n",
      "Running optimization for 8 qubits and 14 layers\n",
      "Step 0: Loss = -2.7804468547308088 for 8 qubits with 14 layers\n",
      "Step 10: Loss = -6.644939076835644 for 8 qubits with 14 layers\n",
      "Step 20: Loss = -6.871038802706867 for 8 qubits with 14 layers\n",
      "Step 30: Loss = -6.956113426239023 for 8 qubits with 14 layers\n",
      "Step 40: Loss = -6.979961526105027 for 8 qubits with 14 layers\n",
      "Step 50: Loss = -6.990612587669151 for 8 qubits with 14 layers\n",
      "Step 60: Loss = -6.999040247383564 for 8 qubits with 14 layers\n",
      "Step 70: Loss = -6.998904838904017 for 8 qubits with 14 layers\n",
      "Step 80: Loss = -6.999742574740917 for 8 qubits with 14 layers\n",
      "Step 90: Loss = -6.999916759996921 for 8 qubits with 14 layers\n",
      "Step 100: Loss = -6.999949485944536 for 8 qubits with 14 layers\n",
      "Step 110: Loss = -6.999980606771983 for 8 qubits with 14 layers\n",
      "Step 120: Loss = -6.999993756418014 for 8 qubits with 14 layers\n",
      "Step 130: Loss = -6.999997982645817 for 8 qubits with 14 layers\n",
      "Step 140: Loss = -6.99999873982167 for 8 qubits with 14 layers\n",
      "Step 150: Loss = -6.999999863263707 for 8 qubits with 14 layers\n",
      "Step 160: Loss = -6.999999874150919 for 8 qubits with 14 layers\n",
      "Step 170: Loss = -6.99999996480266 for 8 qubits with 14 layers\n",
      "Step 180: Loss = -6.999999989377791 for 8 qubits with 14 layers\n",
      "Step 190: Loss = -6.999999996106984 for 8 qubits with 14 layers\n",
      "Step 200: Loss = -6.999999998077662 for 8 qubits with 14 layers\n",
      "Step 210: Loss = -6.9999999993769375 for 8 qubits with 14 layers\n",
      "Step 220: Loss = -6.999999868299921 for 8 qubits with 14 layers\n",
      "Step 230: Loss = -6.941278575155498 for 8 qubits with 14 layers\n",
      "Step 240: Loss = -6.9931366914733575 for 8 qubits with 14 layers\n",
      "Step 250: Loss = -6.999951308780705 for 8 qubits with 14 layers\n",
      "Step 260: Loss = -6.999500836552968 for 8 qubits with 14 layers\n",
      "Step 270: Loss = -6.9998090675989015 for 8 qubits with 14 layers\n",
      "Step 280: Loss = -6.999970995654307 for 8 qubits with 14 layers\n",
      "Step 290: Loss = -6.999814441437321 for 8 qubits with 14 layers\n",
      "Step 300: Loss = -6.999964360163492 for 8 qubits with 14 layers\n",
      "Step 310: Loss = -6.999982737175502 for 8 qubits with 14 layers\n",
      "Step 320: Loss = -6.999991305650729 for 8 qubits with 14 layers\n",
      "Step 330: Loss = -6.999994813353678 for 8 qubits with 14 layers\n",
      "Step 340: Loss = -6.999999268407626 for 8 qubits with 14 layers\n",
      "Step 350: Loss = -6.999999906206565 for 8 qubits with 14 layers\n",
      "Step 360: Loss = -6.999876785184665 for 8 qubits with 14 layers\n",
      "Step 370: Loss = -6.996200442984324 for 8 qubits with 14 layers\n",
      "Step 380: Loss = -6.996452594940385 for 8 qubits with 14 layers\n",
      "Step 390: Loss = -6.995855258054693 for 8 qubits with 14 layers\n",
      "Step 400: Loss = -6.998838416430292 for 8 qubits with 14 layers\n",
      "Step 410: Loss = -6.999579388631102 for 8 qubits with 14 layers\n",
      "Step 420: Loss = -6.986171286291517 for 8 qubits with 14 layers\n",
      "Step 430: Loss = -6.999385763052823 for 8 qubits with 14 layers\n",
      "Step 440: Loss = -6.999931262385001 for 8 qubits with 14 layers\n",
      "Step 450: Loss = -6.999646785660454 for 8 qubits with 14 layers\n",
      "Step 460: Loss = -6.999741482578243 for 8 qubits with 14 layers\n",
      "Step 470: Loss = -6.996917316780846 for 8 qubits with 14 layers\n",
      "Step 480: Loss = -6.997622097374016 for 8 qubits with 14 layers\n",
      "Step 490: Loss = -6.995947324688918 for 8 qubits with 14 layers\n",
      "Running optimization for 8 qubits and 15 layers\n",
      "Step 0: Loss = -1.151985331446053 for 8 qubits with 15 layers\n",
      "Step 10: Loss = -5.824299680359261 for 8 qubits with 15 layers\n",
      "Step 20: Loss = -6.7687713437437145 for 8 qubits with 15 layers\n",
      "Step 30: Loss = -6.918038753882452 for 8 qubits with 15 layers\n",
      "Step 40: Loss = -6.960765541884267 for 8 qubits with 15 layers\n",
      "Step 50: Loss = -6.98905987348004 for 8 qubits with 15 layers\n",
      "Step 60: Loss = -6.996857494432547 for 8 qubits with 15 layers\n",
      "Step 70: Loss = -6.999009681275901 for 8 qubits with 15 layers\n",
      "Step 80: Loss = -6.999522090180905 for 8 qubits with 15 layers\n",
      "Step 90: Loss = -6.99990093657578 for 8 qubits with 15 layers\n",
      "Step 100: Loss = -6.999917347359977 for 8 qubits with 15 layers\n",
      "Step 110: Loss = -6.999967293845954 for 8 qubits with 15 layers\n",
      "Step 120: Loss = -6.999985828036123 for 8 qubits with 15 layers\n",
      "Step 130: Loss = -6.99999481418506 for 8 qubits with 15 layers\n",
      "Step 140: Loss = -6.999998898366693 for 8 qubits with 15 layers\n",
      "Step 150: Loss = -6.99999942128766 for 8 qubits with 15 layers\n",
      "Step 160: Loss = -6.999999770288031 for 8 qubits with 15 layers\n",
      "Step 170: Loss = -6.999999964455696 for 8 qubits with 15 layers\n",
      "Step 180: Loss = -6.99999997910736 for 8 qubits with 15 layers\n",
      "Step 190: Loss = -6.999999991692461 for 8 qubits with 15 layers\n",
      "Step 200: Loss = -6.999999996845547 for 8 qubits with 15 layers\n",
      "Step 210: Loss = -6.999999998871136 for 8 qubits with 15 layers\n",
      "Step 220: Loss = -6.999999999732017 for 8 qubits with 15 layers\n",
      "Step 230: Loss = -6.999999999677516 for 8 qubits with 15 layers\n",
      "Step 240: Loss = -6.999999983819848 for 8 qubits with 15 layers\n",
      "Step 250: Loss = -6.999279037970798 for 8 qubits with 15 layers\n",
      "Step 260: Loss = -6.988566738365554 for 8 qubits with 15 layers\n",
      "Step 270: Loss = -6.9978217779203575 for 8 qubits with 15 layers\n",
      "Step 280: Loss = -6.999172610964906 for 8 qubits with 15 layers\n",
      "Step 290: Loss = -6.999842699653561 for 8 qubits with 15 layers\n",
      "Step 300: Loss = -6.999817336966249 for 8 qubits with 15 layers\n",
      "Step 310: Loss = -6.999914662949029 for 8 qubits with 15 layers\n",
      "Step 320: Loss = -6.999941011129667 for 8 qubits with 15 layers\n",
      "Step 330: Loss = -6.9981738145269645 for 8 qubits with 15 layers\n",
      "Step 340: Loss = -6.99008292750275 for 8 qubits with 15 layers\n",
      "Step 350: Loss = -6.99808279482187 for 8 qubits with 15 layers\n",
      "Step 360: Loss = -6.999340943382745 for 8 qubits with 15 layers\n",
      "Step 370: Loss = -6.999461126348919 for 8 qubits with 15 layers\n",
      "Step 380: Loss = -6.999704364292826 for 8 qubits with 15 layers\n",
      "Step 390: Loss = -6.999412798470053 for 8 qubits with 15 layers\n",
      "Step 400: Loss = -6.997614050185126 for 8 qubits with 15 layers\n",
      "Step 410: Loss = -6.998167811119896 for 8 qubits with 15 layers\n",
      "Step 420: Loss = -6.993960808469865 for 8 qubits with 15 layers\n",
      "Step 430: Loss = -6.989323925432851 for 8 qubits with 15 layers\n",
      "Step 440: Loss = -6.999366772993046 for 8 qubits with 15 layers\n",
      "Step 450: Loss = -6.998066232362997 for 8 qubits with 15 layers\n",
      "Step 460: Loss = -6.999919375920735 for 8 qubits with 15 layers\n",
      "Step 470: Loss = -6.999966148991324 for 8 qubits with 15 layers\n",
      "Step 480: Loss = -6.999886995471967 for 8 qubits with 15 layers\n",
      "Step 490: Loss = -6.985842964220763 for 8 qubits with 15 layers\n",
      "Running optimization for 9 qubits and 1 layers\n",
      "Step 0: Loss = -0.10556103665598449 for 9 qubits with 1 layers\n",
      "Step 10: Loss = -0.7809472908850046 for 9 qubits with 1 layers\n",
      "Step 20: Loss = -2.063236528921004 for 9 qubits with 1 layers\n",
      "Step 30: Loss = -3.797172701925139 for 9 qubits with 1 layers\n",
      "Step 40: Loss = -5.595133869216063 for 9 qubits with 1 layers\n",
      "Step 50: Loss = -6.990301213099225 for 9 qubits with 1 layers\n",
      "Step 60: Loss = -7.741282338242833 for 9 qubits with 1 layers\n",
      "Step 70: Loss = -7.977995800737872 for 9 qubits with 1 layers\n",
      "Step 80: Loss = -7.99803164244763 for 9 qubits with 1 layers\n",
      "Step 90: Loss = -7.9901517299360085 for 9 qubits with 1 layers\n",
      "Step 100: Loss = -7.994162722141204 for 9 qubits with 1 layers\n",
      "Step 110: Loss = -7.998983035573417 for 9 qubits with 1 layers\n",
      "Step 120: Loss = -7.999999408340068 for 9 qubits with 1 layers\n",
      "Step 130: Loss = -7.999844935166286 for 9 qubits with 1 layers\n",
      "Step 140: Loss = -7.999907695155768 for 9 qubits with 1 layers\n",
      "Step 150: Loss = -7.999992755814155 for 9 qubits with 1 layers\n",
      "Step 160: Loss = -7.999997467058428 for 9 qubits with 1 layers\n",
      "Step 170: Loss = -7.999995713097719 for 9 qubits with 1 layers\n",
      "Step 180: Loss = -7.999999441842271 for 9 qubits with 1 layers\n",
      "Step 190: Loss = -7.999999906263088 for 9 qubits with 1 layers\n",
      "Step 200: Loss = -7.9999998030791755 for 9 qubits with 1 layers\n",
      "Step 210: Loss = -7.999999985937586 for 9 qubits with 1 layers\n",
      "Step 220: Loss = -7.999999989014267 for 9 qubits with 1 layers\n",
      "Step 230: Loss = -7.999999992240597 for 9 qubits with 1 layers\n",
      "Step 240: Loss = -7.999999999983659 for 9 qubits with 1 layers\n",
      "Step 250: Loss = -7.999999998898191 for 9 qubits with 1 layers\n",
      "Step 260: Loss = -7.999999999933202 for 9 qubits with 1 layers\n",
      "Step 270: Loss = -7.999999999900298 for 9 qubits with 1 layers\n",
      "Step 280: Loss = -7.999999999976525 for 9 qubits with 1 layers\n",
      "Step 290: Loss = -7.999999999991559 for 9 qubits with 1 layers\n",
      "Step 300: Loss = -7.99999999999605 for 9 qubits with 1 layers\n",
      "Step 310: Loss = -7.999999999999048 for 9 qubits with 1 layers\n",
      "Step 320: Loss = -7.999999999999503 for 9 qubits with 1 layers\n",
      "Step 330: Loss = -7.999999999999809 for 9 qubits with 1 layers\n",
      "Step 340: Loss = -7.999999999999954 for 9 qubits with 1 layers\n",
      "Step 350: Loss = -7.999999999999922 for 9 qubits with 1 layers\n",
      "Step 360: Loss = -7.999999999999957 for 9 qubits with 1 layers\n",
      "Step 370: Loss = -7.99999999999997 for 9 qubits with 1 layers\n",
      "Step 380: Loss = -7.999999999999963 for 9 qubits with 1 layers\n",
      "Step 390: Loss = -7.999999999999971 for 9 qubits with 1 layers\n",
      "Step 400: Loss = -7.999999999999956 for 9 qubits with 1 layers\n",
      "Step 410: Loss = -7.999999999999931 for 9 qubits with 1 layers\n",
      "Step 420: Loss = -7.999999999999998 for 9 qubits with 1 layers\n",
      "Step 430: Loss = -7.999999999999936 for 9 qubits with 1 layers\n",
      "Step 440: Loss = -7.999999999999978 for 9 qubits with 1 layers\n",
      "Step 450: Loss = -7.9999999999999725 for 9 qubits with 1 layers\n",
      "Step 460: Loss = -7.999999999999956 for 9 qubits with 1 layers\n",
      "Step 470: Loss = -7.999999999999985 for 9 qubits with 1 layers\n",
      "Step 480: Loss = -7.999999999999993 for 9 qubits with 1 layers\n",
      "Step 490: Loss = -7.99999999999995 for 9 qubits with 1 layers\n",
      "Running optimization for 9 qubits and 2 layers\n",
      "Step 0: Loss = -2.2832148926862987 for 9 qubits with 2 layers\n",
      "Step 10: Loss = -3.2946668238497097 for 9 qubits with 2 layers\n",
      "Step 20: Loss = -3.67702664017156 for 9 qubits with 2 layers\n",
      "Step 30: Loss = -3.8357488782953855 for 9 qubits with 2 layers\n",
      "Step 40: Loss = -3.839681907955762 for 9 qubits with 2 layers\n",
      "Step 50: Loss = -3.8377145424884387 for 9 qubits with 2 layers\n",
      "Step 60: Loss = -3.846082539636381 for 9 qubits with 2 layers\n",
      "Step 70: Loss = -3.846439311517739 for 9 qubits with 2 layers\n",
      "Step 80: Loss = -3.8474541220734584 for 9 qubits with 2 layers\n",
      "Step 90: Loss = -3.847853444412264 for 9 qubits with 2 layers\n",
      "Step 100: Loss = -3.8478873279623103 for 9 qubits with 2 layers\n",
      "Step 110: Loss = -3.847918014052784 for 9 qubits with 2 layers\n",
      "Step 120: Loss = -3.8479173700432967 for 9 qubits with 2 layers\n",
      "Step 130: Loss = -3.8479265364746342 for 9 qubits with 2 layers\n",
      "Step 140: Loss = -3.8479268389991965 for 9 qubits with 2 layers\n",
      "Step 150: Loss = -3.84792759967992 for 9 qubits with 2 layers\n",
      "Step 160: Loss = -3.847927624987764 for 9 qubits with 2 layers\n",
      "Step 170: Loss = -3.8479277166633143 for 9 qubits with 2 layers\n",
      "Step 180: Loss = -3.847927752311589 for 9 qubits with 2 layers\n",
      "Step 190: Loss = -3.8479277503254665 for 9 qubits with 2 layers\n",
      "Step 200: Loss = -3.8479277555644176 for 9 qubits with 2 layers\n",
      "Step 210: Loss = -3.8479277569920747 for 9 qubits with 2 layers\n",
      "Step 220: Loss = -3.847927756818712 for 9 qubits with 2 layers\n",
      "Step 230: Loss = -3.847927757064755 for 9 qubits with 2 layers\n",
      "Step 240: Loss = -3.847927757163831 for 9 qubits with 2 layers\n",
      "Step 250: Loss = -3.847927757163733 for 9 qubits with 2 layers\n",
      "Step 260: Loss = -3.8479277571689976 for 9 qubits with 2 layers\n",
      "Step 270: Loss = -3.8479277571710715 for 9 qubits with 2 layers\n",
      "Step 280: Loss = -3.8479277571724846 for 9 qubits with 2 layers\n",
      "Step 290: Loss = -3.847927757172979 for 9 qubits with 2 layers\n",
      "Step 300: Loss = -3.847927757173013 for 9 qubits with 2 layers\n",
      "Step 310: Loss = -3.847927757173032 for 9 qubits with 2 layers\n",
      "Step 320: Loss = -3.847927757173049 for 9 qubits with 2 layers\n",
      "Step 330: Loss = -3.847927757173046 for 9 qubits with 2 layers\n",
      "Step 340: Loss = -3.8479277571730384 for 9 qubits with 2 layers\n",
      "Step 350: Loss = -3.8479277571730615 for 9 qubits with 2 layers\n",
      "Step 360: Loss = -3.8479277571730828 for 9 qubits with 2 layers\n",
      "Step 370: Loss = -3.847927757173055 for 9 qubits with 2 layers\n",
      "Step 380: Loss = -3.8479277571730517 for 9 qubits with 2 layers\n",
      "Step 390: Loss = -3.847927757173041 for 9 qubits with 2 layers\n",
      "Step 400: Loss = -3.8479277571730397 for 9 qubits with 2 layers\n",
      "Step 410: Loss = -3.8479277571730783 for 9 qubits with 2 layers\n",
      "Step 420: Loss = -3.847927757173042 for 9 qubits with 2 layers\n",
      "Step 430: Loss = -3.847927757173046 for 9 qubits with 2 layers\n",
      "Step 440: Loss = -3.847927757173073 for 9 qubits with 2 layers\n",
      "Step 450: Loss = -3.8479277571730623 for 9 qubits with 2 layers\n",
      "Step 460: Loss = -3.847927757173058 for 9 qubits with 2 layers\n",
      "Step 470: Loss = -3.847927757173024 for 9 qubits with 2 layers\n",
      "Step 480: Loss = -3.8479277571730393 for 9 qubits with 2 layers\n",
      "Step 490: Loss = -3.8479277571730455 for 9 qubits with 2 layers\n",
      "Running optimization for 9 qubits and 3 layers\n",
      "Step 0: Loss = -3.1864196893161556 for 9 qubits with 3 layers\n",
      "Step 10: Loss = -6.5661411079813155 for 9 qubits with 3 layers\n",
      "Step 20: Loss = -7.867655776435137 for 9 qubits with 3 layers\n",
      "Step 30: Loss = -7.8697946853461 for 9 qubits with 3 layers\n",
      "Step 40: Loss = -7.958479109541542 for 9 qubits with 3 layers\n",
      "Step 50: Loss = -7.99485747000935 for 9 qubits with 3 layers\n",
      "Step 60: Loss = -7.990255950904817 for 9 qubits with 3 layers\n",
      "Step 70: Loss = -7.999101823218331 for 9 qubits with 3 layers\n",
      "Step 80: Loss = -7.998601312056302 for 9 qubits with 3 layers\n",
      "Step 90: Loss = -7.99985747498299 for 9 qubits with 3 layers\n",
      "Step 100: Loss = -7.999847370415997 for 9 qubits with 3 layers\n",
      "Step 110: Loss = -7.9999634844199425 for 9 qubits with 3 layers\n",
      "Step 120: Loss = -7.999992693407924 for 9 qubits with 3 layers\n",
      "Step 130: Loss = -7.9999926459376685 for 9 qubits with 3 layers\n",
      "Step 140: Loss = -7.999999545378551 for 9 qubits with 3 layers\n",
      "Step 150: Loss = -7.999999487555797 for 9 qubits with 3 layers\n",
      "Step 160: Loss = -7.999999596057021 for 9 qubits with 3 layers\n",
      "Step 170: Loss = -7.999999949782448 for 9 qubits with 3 layers\n",
      "Step 180: Loss = -7.999999990210121 for 9 qubits with 3 layers\n",
      "Step 190: Loss = -7.999999988273948 for 9 qubits with 3 layers\n",
      "Step 200: Loss = -7.999999993967135 for 9 qubits with 3 layers\n",
      "Step 210: Loss = -7.999999998184941 for 9 qubits with 3 layers\n",
      "Step 220: Loss = -7.999999999716593 for 9 qubits with 3 layers\n",
      "Step 230: Loss = -7.999999999976387 for 9 qubits with 3 layers\n",
      "Step 240: Loss = -7.999999999970731 for 9 qubits with 3 layers\n",
      "Step 250: Loss = -7.99999999999042 for 9 qubits with 3 layers\n",
      "Step 260: Loss = -7.999999999994886 for 9 qubits with 3 layers\n",
      "Step 270: Loss = -7.999999999997102 for 9 qubits with 3 layers\n",
      "Step 280: Loss = -7.999999999998794 for 9 qubits with 3 layers\n",
      "Step 290: Loss = -7.999999999999657 for 9 qubits with 3 layers\n",
      "Step 300: Loss = -7.999999999999878 for 9 qubits with 3 layers\n",
      "Step 310: Loss = -7.999999999999928 for 9 qubits with 3 layers\n",
      "Step 320: Loss = -7.9999999999998845 for 9 qubits with 3 layers\n",
      "Step 330: Loss = -7.999999999999992 for 9 qubits with 3 layers\n",
      "Step 340: Loss = -7.999999999999943 for 9 qubits with 3 layers\n",
      "Step 350: Loss = -7.9999999999999325 for 9 qubits with 3 layers\n",
      "Step 360: Loss = -7.999999999999888 for 9 qubits with 3 layers\n",
      "Step 370: Loss = -7.999999999999916 for 9 qubits with 3 layers\n",
      "Step 380: Loss = -7.99999999999995 for 9 qubits with 3 layers\n",
      "Step 390: Loss = -7.999999999999958 for 9 qubits with 3 layers\n",
      "Step 400: Loss = -7.999999999985229 for 9 qubits with 3 layers\n",
      "Step 410: Loss = -7.99999262598005 for 9 qubits with 3 layers\n",
      "Step 420: Loss = -7.999532889309273 for 9 qubits with 3 layers\n",
      "Step 430: Loss = -7.998230042427187 for 9 qubits with 3 layers\n",
      "Step 440: Loss = -7.999853654295856 for 9 qubits with 3 layers\n",
      "Step 450: Loss = -7.99985700350647 for 9 qubits with 3 layers\n",
      "Step 460: Loss = -7.999986011174179 for 9 qubits with 3 layers\n",
      "Step 470: Loss = -7.999972964851191 for 9 qubits with 3 layers\n",
      "Step 480: Loss = -7.99998907386803 for 9 qubits with 3 layers\n",
      "Step 490: Loss = -7.99999835238002 for 9 qubits with 3 layers\n",
      "Running optimization for 9 qubits and 4 layers\n",
      "Step 0: Loss = -2.1840607406609607 for 9 qubits with 4 layers\n",
      "Step 10: Loss = -4.504039007836196 for 9 qubits with 4 layers\n",
      "Step 20: Loss = -5.105572278925251 for 9 qubits with 4 layers\n",
      "Step 30: Loss = -6.416627575621645 for 9 qubits with 4 layers\n",
      "Step 40: Loss = -7.631698243556601 for 9 qubits with 4 layers\n",
      "Step 50: Loss = -7.970047836653691 for 9 qubits with 4 layers\n",
      "Step 60: Loss = -7.99773146589363 for 9 qubits with 4 layers\n",
      "Step 70: Loss = -7.998660806238512 for 9 qubits with 4 layers\n",
      "Step 80: Loss = -7.999169600003114 for 9 qubits with 4 layers\n",
      "Step 90: Loss = -7.9996687385379275 for 9 qubits with 4 layers\n",
      "Step 100: Loss = -7.99968901414303 for 9 qubits with 4 layers\n",
      "Step 110: Loss = -7.999810817200392 for 9 qubits with 4 layers\n",
      "Step 120: Loss = -7.999854188798815 for 9 qubits with 4 layers\n",
      "Step 130: Loss = -7.99989960006421 for 9 qubits with 4 layers\n",
      "Step 140: Loss = -7.9999285828384235 for 9 qubits with 4 layers\n",
      "Step 150: Loss = -7.999947265910992 for 9 qubits with 4 layers\n",
      "Step 160: Loss = -7.999959071007098 for 9 qubits with 4 layers\n",
      "Step 170: Loss = -7.999965848077627 for 9 qubits with 4 layers\n",
      "Step 180: Loss = -7.999969497015656 for 9 qubits with 4 layers\n",
      "Step 190: Loss = -7.999971460817152 for 9 qubits with 4 layers\n",
      "Step 200: Loss = -7.999972635038063 for 9 qubits with 4 layers\n",
      "Step 210: Loss = -7.999973501739653 for 9 qubits with 4 layers\n",
      "Step 220: Loss = -7.999974276367481 for 9 qubits with 4 layers\n",
      "Step 230: Loss = -7.999975034934935 for 9 qubits with 4 layers\n",
      "Step 240: Loss = -7.999975795824284 for 9 qubits with 4 layers\n",
      "Step 250: Loss = -7.999976559621722 for 9 qubits with 4 layers\n",
      "Step 260: Loss = -7.99997732365946 for 9 qubits with 4 layers\n",
      "Step 270: Loss = -7.99997808585323 for 9 qubits with 4 layers\n",
      "Step 280: Loss = -7.999978844607676 for 9 qubits with 4 layers\n",
      "Step 290: Loss = -7.999979598446367 for 9 qubits with 4 layers\n",
      "Step 300: Loss = -7.999980345883735 for 9 qubits with 4 layers\n",
      "Step 310: Loss = -7.999981085457504 for 9 qubits with 4 layers\n",
      "Step 320: Loss = -7.999981815771286 for 9 qubits with 4 layers\n",
      "Step 330: Loss = -7.999982535515074 for 9 qubits with 4 layers\n",
      "Step 340: Loss = -7.999983243471184 for 9 qubits with 4 layers\n",
      "Step 350: Loss = -7.99998393851608 for 9 qubits with 4 layers\n",
      "Step 360: Loss = -7.999984619623233 for 9 qubits with 4 layers\n",
      "Step 370: Loss = -7.999985285864837 for 9 qubits with 4 layers\n",
      "Step 380: Loss = -7.999985936413157 for 9 qubits with 4 layers\n",
      "Step 390: Loss = -7.999986570540278 for 9 qubits with 4 layers\n",
      "Step 400: Loss = -7.999987187618033 for 9 qubits with 4 layers\n",
      "Step 410: Loss = -7.999987787115927 for 9 qubits with 4 layers\n",
      "Step 420: Loss = -7.99998836859922 for 9 qubits with 4 layers\n",
      "Step 430: Loss = -7.9999889317268265 for 9 qubits with 4 layers\n",
      "Step 440: Loss = -7.999989476246448 for 9 qubits with 4 layers\n",
      "Step 450: Loss = -7.999990001992075 for 9 qubits with 4 layers\n",
      "Step 460: Loss = -7.9999905088787235 for 9 qubits with 4 layers\n",
      "Step 470: Loss = -7.9999909968982355 for 9 qubits with 4 layers\n",
      "Step 480: Loss = -7.999991466113713 for 9 qubits with 4 layers\n",
      "Step 490: Loss = -7.999991916654624 for 9 qubits with 4 layers\n",
      "Running optimization for 9 qubits and 5 layers\n",
      "Step 0: Loss = -2.5773023749788857 for 9 qubits with 5 layers\n",
      "Step 10: Loss = -4.887468336194842 for 9 qubits with 5 layers\n",
      "Step 20: Loss = -6.168057343767823 for 9 qubits with 5 layers\n",
      "Step 30: Loss = -6.908743080122617 for 9 qubits with 5 layers\n",
      "Step 40: Loss = -7.2003791784608016 for 9 qubits with 5 layers\n",
      "Step 50: Loss = -7.324850910983035 for 9 qubits with 5 layers\n",
      "Step 60: Loss = -7.369341256018507 for 9 qubits with 5 layers\n",
      "Step 70: Loss = -7.401859312246548 for 9 qubits with 5 layers\n",
      "Step 80: Loss = -7.406810799021557 for 9 qubits with 5 layers\n",
      "Step 90: Loss = -7.408631369183843 for 9 qubits with 5 layers\n",
      "Step 100: Loss = -7.4109941466047955 for 9 qubits with 5 layers\n",
      "Step 110: Loss = -7.410970425927268 for 9 qubits with 5 layers\n",
      "Step 120: Loss = -7.411180609141071 for 9 qubits with 5 layers\n",
      "Step 130: Loss = -7.4112486668201445 for 9 qubits with 5 layers\n",
      "Step 140: Loss = -7.411247012768243 for 9 qubits with 5 layers\n",
      "Step 150: Loss = -7.411258860798813 for 9 qubits with 5 layers\n",
      "Step 160: Loss = -7.411257953181952 for 9 qubits with 5 layers\n",
      "Step 170: Loss = -7.411259917265894 for 9 qubits with 5 layers\n",
      "Step 180: Loss = -7.411259901109166 for 9 qubits with 5 layers\n",
      "Step 190: Loss = -7.4112600961018 for 9 qubits with 5 layers\n",
      "Step 200: Loss = -7.41126009948044 for 9 qubits with 5 layers\n",
      "Step 210: Loss = -7.411260121357923 for 9 qubits with 5 layers\n",
      "Step 220: Loss = -7.411260122898694 for 9 qubits with 5 layers\n",
      "Step 230: Loss = -7.411260125097957 for 9 qubits with 5 layers\n",
      "Step 240: Loss = -7.411260126056498 for 9 qubits with 5 layers\n",
      "Step 250: Loss = -7.4112601260596 for 9 qubits with 5 layers\n",
      "Step 260: Loss = -7.411260126211911 for 9 qubits with 5 layers\n",
      "Step 270: Loss = -7.411260126236991 for 9 qubits with 5 layers\n",
      "Step 280: Loss = -7.4112601262398154 for 9 qubits with 5 layers\n",
      "Step 290: Loss = -7.411260126245417 for 9 qubits with 5 layers\n",
      "Step 300: Loss = -7.411260126246017 for 9 qubits with 5 layers\n",
      "Step 310: Loss = -7.411260125240759 for 9 qubits with 5 layers\n",
      "Step 320: Loss = -7.411052177934027 for 9 qubits with 5 layers\n",
      "Step 330: Loss = -7.408664316685216 for 9 qubits with 5 layers\n",
      "Step 340: Loss = -7.410923782827316 for 9 qubits with 5 layers\n",
      "Step 350: Loss = -7.411251303779116 for 9 qubits with 5 layers\n",
      "Step 360: Loss = -7.411230257700559 for 9 qubits with 5 layers\n",
      "Step 370: Loss = -7.4112596005722855 for 9 qubits with 5 layers\n",
      "Step 380: Loss = -7.411241793280158 for 9 qubits with 5 layers\n",
      "Step 390: Loss = -7.4112585573222205 for 9 qubits with 5 layers\n",
      "Step 400: Loss = -7.411257863143916 for 9 qubits with 5 layers\n",
      "Step 410: Loss = -7.411260027952599 for 9 qubits with 5 layers\n",
      "Step 420: Loss = -7.411259416990716 for 9 qubits with 5 layers\n",
      "Step 430: Loss = -7.411259802746603 for 9 qubits with 5 layers\n",
      "Step 440: Loss = -7.411260117382749 for 9 qubits with 5 layers\n",
      "Step 450: Loss = -7.411259348885701 for 9 qubits with 5 layers\n",
      "Step 460: Loss = -7.410221532340628 for 9 qubits with 5 layers\n",
      "Step 470: Loss = -7.409547930706862 for 9 qubits with 5 layers\n",
      "Step 480: Loss = -7.410617027501397 for 9 qubits with 5 layers\n",
      "Step 490: Loss = -7.41103062723334 for 9 qubits with 5 layers\n",
      "Running optimization for 9 qubits and 6 layers\n",
      "Step 0: Loss = -3.489515050841299 for 9 qubits with 6 layers\n",
      "Step 10: Loss = -6.243365088536043 for 9 qubits with 6 layers\n",
      "Step 20: Loss = -7.183472843163996 for 9 qubits with 6 layers\n",
      "Step 30: Loss = -7.54935077324701 for 9 qubits with 6 layers\n",
      "Step 40: Loss = -7.755712561249659 for 9 qubits with 6 layers\n",
      "Step 50: Loss = -7.852190886815001 for 9 qubits with 6 layers\n",
      "Step 60: Loss = -7.854522354547414 for 9 qubits with 6 layers\n",
      "Step 70: Loss = -7.868274868626473 for 9 qubits with 6 layers\n",
      "Step 80: Loss = -7.869642174278734 for 9 qubits with 6 layers\n",
      "Step 90: Loss = -7.871763959669541 for 9 qubits with 6 layers\n",
      "Step 100: Loss = -7.872076994124486 for 9 qubits with 6 layers\n",
      "Step 110: Loss = -7.8722393974715885 for 9 qubits with 6 layers\n",
      "Step 120: Loss = -7.8722841973892255 for 9 qubits with 6 layers\n",
      "Step 130: Loss = -7.8722993951939415 for 9 qubits with 6 layers\n",
      "Step 140: Loss = -7.872307838433528 for 9 qubits with 6 layers\n",
      "Step 150: Loss = -7.87231100322532 for 9 qubits with 6 layers\n",
      "Step 160: Loss = -7.872311262923993 for 9 qubits with 6 layers\n",
      "Step 170: Loss = -7.8723117144810795 for 9 qubits with 6 layers\n",
      "Step 180: Loss = -7.872311810956604 for 9 qubits with 6 layers\n",
      "Step 190: Loss = -7.872311854228087 for 9 qubits with 6 layers\n",
      "Step 200: Loss = -7.872311859675413 for 9 qubits with 6 layers\n",
      "Step 210: Loss = -7.872311862203336 for 9 qubits with 6 layers\n",
      "Step 220: Loss = -7.87231186615125 for 9 qubits with 6 layers\n",
      "Step 230: Loss = -7.872311866797913 for 9 qubits with 6 layers\n",
      "Step 240: Loss = -7.872311867122647 for 9 qubits with 6 layers\n",
      "Step 250: Loss = -7.8723118672583094 for 9 qubits with 6 layers\n",
      "Step 260: Loss = -7.872311867281114 for 9 qubits with 6 layers\n",
      "Step 270: Loss = -7.872311867290365 for 9 qubits with 6 layers\n",
      "Step 280: Loss = -7.872311867297519 for 9 qubits with 6 layers\n",
      "Step 290: Loss = -7.872311867298561 for 9 qubits with 6 layers\n",
      "Step 300: Loss = -7.872311867299287 for 9 qubits with 6 layers\n",
      "Step 310: Loss = -7.872311867299494 for 9 qubits with 6 layers\n",
      "Step 320: Loss = -7.872311867299655 for 9 qubits with 6 layers\n",
      "Step 330: Loss = -7.8723118672996595 for 9 qubits with 6 layers\n",
      "Step 340: Loss = -7.872311867299648 for 9 qubits with 6 layers\n",
      "Step 350: Loss = -7.872311867299627 for 9 qubits with 6 layers\n",
      "Step 360: Loss = -7.872311867299645 for 9 qubits with 6 layers\n",
      "Step 370: Loss = -7.872311867299622 for 9 qubits with 6 layers\n",
      "Step 380: Loss = -7.872311867299651 for 9 qubits with 6 layers\n",
      "Step 390: Loss = -7.8723118672996275 for 9 qubits with 6 layers\n",
      "Step 400: Loss = -7.872311862900846 for 9 qubits with 6 layers\n",
      "Step 410: Loss = -7.863821569034493 for 9 qubits with 6 layers\n",
      "Step 420: Loss = -7.8650657514147975 for 9 qubits with 6 layers\n",
      "Step 430: Loss = -7.8711904368760415 for 9 qubits with 6 layers\n",
      "Step 440: Loss = -7.871064828855841 for 9 qubits with 6 layers\n",
      "Step 450: Loss = -7.872228563897008 for 9 qubits with 6 layers\n",
      "Step 460: Loss = -7.872276537673408 for 9 qubits with 6 layers\n",
      "Step 470: Loss = -7.872296981064678 for 9 qubits with 6 layers\n",
      "Step 480: Loss = -7.872302862052613 for 9 qubits with 6 layers\n",
      "Step 490: Loss = -7.872303282300975 for 9 qubits with 6 layers\n",
      "Running optimization for 9 qubits and 7 layers\n",
      "Step 0: Loss = -0.4943052894949771 for 9 qubits with 7 layers\n",
      "Step 10: Loss = -5.42794541170643 for 9 qubits with 7 layers\n",
      "Step 20: Loss = -7.061134359607778 for 9 qubits with 7 layers\n",
      "Step 30: Loss = -7.268608004586966 for 9 qubits with 7 layers\n",
      "Step 40: Loss = -7.412154836105787 for 9 qubits with 7 layers\n",
      "Step 50: Loss = -7.498449854984751 for 9 qubits with 7 layers\n",
      "Step 60: Loss = -7.530628918863384 for 9 qubits with 7 layers\n",
      "Step 70: Loss = -7.566302230790443 for 9 qubits with 7 layers\n",
      "Step 80: Loss = -7.630282965635074 for 9 qubits with 7 layers\n",
      "Step 90: Loss = -7.709095970368714 for 9 qubits with 7 layers\n",
      "Step 100: Loss = -7.768896326343201 for 9 qubits with 7 layers\n",
      "Step 110: Loss = -7.816759745819479 for 9 qubits with 7 layers\n",
      "Step 120: Loss = -7.862632651167995 for 9 qubits with 7 layers\n",
      "Step 130: Loss = -7.899167391002537 for 9 qubits with 7 layers\n",
      "Step 140: Loss = -7.919184297545189 for 9 qubits with 7 layers\n",
      "Step 150: Loss = -7.933242457402305 for 9 qubits with 7 layers\n",
      "Step 160: Loss = -7.93766532291917 for 9 qubits with 7 layers\n",
      "Step 170: Loss = -7.938889744070809 for 9 qubits with 7 layers\n",
      "Step 180: Loss = -7.939197645092269 for 9 qubits with 7 layers\n",
      "Step 190: Loss = -7.939819267314374 for 9 qubits with 7 layers\n",
      "Step 200: Loss = -7.940277708495272 for 9 qubits with 7 layers\n",
      "Step 210: Loss = -7.940619393951447 for 9 qubits with 7 layers\n",
      "Step 220: Loss = -7.940948567622492 for 9 qubits with 7 layers\n",
      "Step 230: Loss = -7.941258925482458 for 9 qubits with 7 layers\n",
      "Step 240: Loss = -7.941547626637237 for 9 qubits with 7 layers\n",
      "Step 250: Loss = -7.941822120637874 for 9 qubits with 7 layers\n",
      "Step 260: Loss = -7.942084727294229 for 9 qubits with 7 layers\n",
      "Step 270: Loss = -7.942337480100294 for 9 qubits with 7 layers\n",
      "Step 280: Loss = -7.942583031945154 for 9 qubits with 7 layers\n",
      "Step 290: Loss = -7.942824060272124 for 9 qubits with 7 layers\n",
      "Step 300: Loss = -7.94306367562397 for 9 qubits with 7 layers\n",
      "Step 310: Loss = -7.943305713153348 for 9 qubits with 7 layers\n",
      "Step 320: Loss = -7.94355511436054 for 9 qubits with 7 layers\n",
      "Step 330: Loss = -7.943818656708907 for 9 qubits with 7 layers\n",
      "Step 340: Loss = -7.944106164932239 for 9 qubits with 7 layers\n",
      "Step 350: Loss = -7.944432684896084 for 9 qubits with 7 layers\n",
      "Step 360: Loss = -7.9448224264419425 for 9 qubits with 7 layers\n",
      "Step 370: Loss = -7.945315813712687 for 9 qubits with 7 layers\n",
      "Step 380: Loss = -7.94598050598658 for 9 qubits with 7 layers\n",
      "Step 390: Loss = -7.946919597281657 for 9 qubits with 7 layers\n",
      "Step 400: Loss = -7.948237721809928 for 9 qubits with 7 layers\n",
      "Step 410: Loss = -7.949891464254697 for 9 qubits with 7 layers\n",
      "Step 420: Loss = -7.951565930881838 for 9 qubits with 7 layers\n",
      "Step 430: Loss = -7.95298839862998 for 9 qubits with 7 layers\n",
      "Step 440: Loss = -7.954214738862102 for 9 qubits with 7 layers\n",
      "Step 450: Loss = -7.955389566124987 for 9 qubits with 7 layers\n",
      "Step 460: Loss = -7.940536177203207 for 9 qubits with 7 layers\n",
      "Step 470: Loss = -7.954176890880667 for 9 qubits with 7 layers\n",
      "Step 480: Loss = -7.954354467426649 for 9 qubits with 7 layers\n",
      "Step 490: Loss = -7.958194945974928 for 9 qubits with 7 layers\n",
      "Running optimization for 9 qubits and 8 layers\n",
      "Step 0: Loss = -1.92252747429035 for 9 qubits with 8 layers\n",
      "Step 10: Loss = -5.684798194184401 for 9 qubits with 8 layers\n",
      "Step 20: Loss = -7.256455629494781 for 9 qubits with 8 layers\n",
      "Step 30: Loss = -7.536314764924228 for 9 qubits with 8 layers\n",
      "Step 40: Loss = -7.752792105595088 for 9 qubits with 8 layers\n",
      "Step 50: Loss = -7.805354496030368 for 9 qubits with 8 layers\n",
      "Step 60: Loss = -7.8311288498166896 for 9 qubits with 8 layers\n",
      "Step 70: Loss = -7.866509773900895 for 9 qubits with 8 layers\n",
      "Step 80: Loss = -7.887964804702926 for 9 qubits with 8 layers\n",
      "Step 90: Loss = -7.894144262727992 for 9 qubits with 8 layers\n",
      "Step 100: Loss = -7.895461808565813 for 9 qubits with 8 layers\n",
      "Step 110: Loss = -7.895898011439379 for 9 qubits with 8 layers\n",
      "Step 120: Loss = -7.896245713666697 for 9 qubits with 8 layers\n",
      "Step 130: Loss = -7.89648566845465 for 9 qubits with 8 layers\n",
      "Step 140: Loss = -7.896653814639208 for 9 qubits with 8 layers\n",
      "Step 150: Loss = -7.896846546596557 for 9 qubits with 8 layers\n",
      "Step 160: Loss = -7.897092067063305 for 9 qubits with 8 layers\n",
      "Step 170: Loss = -7.89748087141229 for 9 qubits with 8 layers\n",
      "Step 180: Loss = -7.898294934755034 for 9 qubits with 8 layers\n",
      "Step 190: Loss = -7.900780137964617 for 9 qubits with 8 layers\n",
      "Step 200: Loss = -7.911353552359707 for 9 qubits with 8 layers\n",
      "Step 210: Loss = -7.8812483064813765 for 9 qubits with 8 layers\n",
      "Step 220: Loss = -7.9342972415346935 for 9 qubits with 8 layers\n",
      "Step 230: Loss = -7.939770600618873 for 9 qubits with 8 layers\n",
      "Step 240: Loss = -7.946625160206272 for 9 qubits with 8 layers\n",
      "Step 250: Loss = -7.946860295975141 for 9 qubits with 8 layers\n",
      "Step 260: Loss = -7.947117973678183 for 9 qubits with 8 layers\n",
      "Step 270: Loss = -7.947270287360233 for 9 qubits with 8 layers\n",
      "Step 280: Loss = -7.947311818150082 for 9 qubits with 8 layers\n",
      "Step 290: Loss = -7.947412807804019 for 9 qubits with 8 layers\n",
      "Step 300: Loss = -7.947482420047705 for 9 qubits with 8 layers\n",
      "Step 310: Loss = -7.947550193629511 for 9 qubits with 8 layers\n",
      "Step 320: Loss = -7.947610962927092 for 9 qubits with 8 layers\n",
      "Step 330: Loss = -7.947671427386423 for 9 qubits with 8 layers\n",
      "Step 340: Loss = -7.947728805816134 for 9 qubits with 8 layers\n",
      "Step 350: Loss = -7.947669372142059 for 9 qubits with 8 layers\n",
      "Step 360: Loss = -7.937751300342762 for 9 qubits with 8 layers\n",
      "Step 370: Loss = -7.94226146033091 for 9 qubits with 8 layers\n",
      "Step 380: Loss = -7.945597925553557 for 9 qubits with 8 layers\n",
      "Step 390: Loss = -7.947210301184902 for 9 qubits with 8 layers\n",
      "Step 400: Loss = -7.947597950682819 for 9 qubits with 8 layers\n",
      "Step 410: Loss = -7.947991392751755 for 9 qubits with 8 layers\n",
      "Step 420: Loss = -7.948015756777282 for 9 qubits with 8 layers\n",
      "Step 430: Loss = -7.94801442234425 for 9 qubits with 8 layers\n",
      "Step 440: Loss = -7.9480118090407075 for 9 qubits with 8 layers\n",
      "Step 450: Loss = -7.936526272502404 for 9 qubits with 8 layers\n",
      "Step 460: Loss = -7.943393680989757 for 9 qubits with 8 layers\n",
      "Step 470: Loss = -7.94674904524104 for 9 qubits with 8 layers\n",
      "Step 480: Loss = -7.948129256834694 for 9 qubits with 8 layers\n",
      "Step 490: Loss = -7.948088111377476 for 9 qubits with 8 layers\n",
      "Running optimization for 9 qubits and 9 layers\n",
      "Step 0: Loss = -1.8665752190388472 for 9 qubits with 9 layers\n",
      "Step 10: Loss = -5.904420119878765 for 9 qubits with 9 layers\n",
      "Step 20: Loss = -7.55022174310755 for 9 qubits with 9 layers\n",
      "Step 30: Loss = -7.761937319681211 for 9 qubits with 9 layers\n",
      "Step 40: Loss = -7.918802797297487 for 9 qubits with 9 layers\n",
      "Step 50: Loss = -7.9479560596199175 for 9 qubits with 9 layers\n",
      "Step 60: Loss = -7.976791647049096 for 9 qubits with 9 layers\n",
      "Step 70: Loss = -7.989257257074669 for 9 qubits with 9 layers\n",
      "Step 80: Loss = -7.994454330432148 for 9 qubits with 9 layers\n",
      "Step 90: Loss = -7.997035610693814 for 9 qubits with 9 layers\n",
      "Step 100: Loss = -7.998234409382819 for 9 qubits with 9 layers\n",
      "Step 110: Loss = -7.998962300278085 for 9 qubits with 9 layers\n",
      "Step 120: Loss = -7.999401824169352 for 9 qubits with 9 layers\n",
      "Step 130: Loss = -7.9996789940028314 for 9 qubits with 9 layers\n",
      "Step 140: Loss = -7.999848430892226 for 9 qubits with 9 layers\n",
      "Step 150: Loss = -7.999932273541852 for 9 qubits with 9 layers\n",
      "Step 160: Loss = -7.9999709163363235 for 9 qubits with 9 layers\n",
      "Step 170: Loss = -7.999987528469075 for 9 qubits with 9 layers\n",
      "Step 180: Loss = -7.999994406762839 for 9 qubits with 9 layers\n",
      "Step 190: Loss = -7.9999974893848025 for 9 qubits with 9 layers\n",
      "Step 200: Loss = -7.999998984557627 for 9 qubits with 9 layers\n",
      "Step 210: Loss = -7.999999653551111 for 9 qubits with 9 layers\n",
      "Step 220: Loss = -7.99999990668145 for 9 qubits with 9 layers\n",
      "Step 230: Loss = -7.9999999801746355 for 9 qubits with 9 layers\n",
      "Step 240: Loss = -7.9999999965271815 for 9 qubits with 9 layers\n",
      "Step 250: Loss = -7.999999988945433 for 9 qubits with 9 layers\n",
      "Step 260: Loss = -7.998991979591339 for 9 qubits with 9 layers\n",
      "Step 270: Loss = -7.97493560261081 for 9 qubits with 9 layers\n",
      "Step 280: Loss = -7.993543467414778 for 9 qubits with 9 layers\n",
      "Step 290: Loss = -7.998727801448889 for 9 qubits with 9 layers\n",
      "Step 300: Loss = -7.9999680366314125 for 9 qubits with 9 layers\n",
      "Step 310: Loss = -7.999535536036886 for 9 qubits with 9 layers\n",
      "Step 320: Loss = -7.9998867806737355 for 9 qubits with 9 layers\n",
      "Step 330: Loss = -7.999943053031002 for 9 qubits with 9 layers\n",
      "Step 340: Loss = -7.9999857949557995 for 9 qubits with 9 layers\n",
      "Step 350: Loss = -7.99998789780913 for 9 qubits with 9 layers\n",
      "Step 360: Loss = -7.9999998848173846 for 9 qubits with 9 layers\n",
      "Step 370: Loss = -7.999998805433783 for 9 qubits with 9 layers\n",
      "Step 380: Loss = -7.99996784223218 for 9 qubits with 9 layers\n",
      "Step 390: Loss = -7.992841890728593 for 9 qubits with 9 layers\n",
      "Step 400: Loss = -7.9998445923120185 for 9 qubits with 9 layers\n",
      "Step 410: Loss = -7.996307397497737 for 9 qubits with 9 layers\n",
      "Step 420: Loss = -7.998528773806861 for 9 qubits with 9 layers\n",
      "Step 430: Loss = -7.999506185751651 for 9 qubits with 9 layers\n",
      "Step 440: Loss = -7.999735467251135 for 9 qubits with 9 layers\n",
      "Step 450: Loss = -7.9999648647691615 for 9 qubits with 9 layers\n",
      "Step 460: Loss = -7.99997394632687 for 9 qubits with 9 layers\n",
      "Step 470: Loss = -7.999997438293043 for 9 qubits with 9 layers\n",
      "Step 480: Loss = -7.999779255817598 for 9 qubits with 9 layers\n",
      "Step 490: Loss = -7.992492173586379 for 9 qubits with 9 layers\n",
      "Running optimization for 9 qubits and 10 layers\n",
      "Step 0: Loss = -3.8606944855451886 for 9 qubits with 10 layers\n",
      "Step 10: Loss = -7.509438885207507 for 9 qubits with 10 layers\n",
      "Step 20: Loss = -7.665061458051337 for 9 qubits with 10 layers\n",
      "Step 30: Loss = -7.836268495885833 for 9 qubits with 10 layers\n",
      "Step 40: Loss = -7.938135478604396 for 9 qubits with 10 layers\n",
      "Step 50: Loss = -7.957774957464834 for 9 qubits with 10 layers\n",
      "Step 60: Loss = -7.989305499345816 for 9 qubits with 10 layers\n",
      "Step 70: Loss = -7.996964999209893 for 9 qubits with 10 layers\n",
      "Step 80: Loss = -7.999301116049908 for 9 qubits with 10 layers\n",
      "Step 90: Loss = -7.99924139883575 for 9 qubits with 10 layers\n",
      "Step 100: Loss = -7.999774955473962 for 9 qubits with 10 layers\n",
      "Step 110: Loss = -7.999887362052042 for 9 qubits with 10 layers\n",
      "Step 120: Loss = -7.999924251879524 for 9 qubits with 10 layers\n",
      "Step 130: Loss = -7.999955489012282 for 9 qubits with 10 layers\n",
      "Step 140: Loss = -7.999965415849238 for 9 qubits with 10 layers\n",
      "Step 150: Loss = -7.999974201100881 for 9 qubits with 10 layers\n",
      "Step 160: Loss = -7.999961749180931 for 9 qubits with 10 layers\n",
      "Step 170: Loss = -7.999338912655929 for 9 qubits with 10 layers\n",
      "Step 180: Loss = -7.999411935465036 for 9 qubits with 10 layers\n",
      "Step 190: Loss = -7.999179969141949 for 9 qubits with 10 layers\n",
      "Step 200: Loss = -7.999869197116857 for 9 qubits with 10 layers\n",
      "Step 210: Loss = -7.999793938740874 for 9 qubits with 10 layers\n",
      "Step 220: Loss = -7.999962211186298 for 9 qubits with 10 layers\n",
      "Step 230: Loss = -7.99992786175999 for 9 qubits with 10 layers\n",
      "Step 240: Loss = -7.999968121146885 for 9 qubits with 10 layers\n",
      "Step 250: Loss = -7.982935808813748 for 9 qubits with 10 layers\n",
      "Step 260: Loss = -7.99218860697794 for 9 qubits with 10 layers\n",
      "Step 270: Loss = -7.999807452448008 for 9 qubits with 10 layers\n",
      "Step 280: Loss = -7.998818236596295 for 9 qubits with 10 layers\n",
      "Step 290: Loss = -7.999465802922271 for 9 qubits with 10 layers\n",
      "Step 300: Loss = -7.999974642615352 for 9 qubits with 10 layers\n",
      "Step 310: Loss = -7.999283776698426 for 9 qubits with 10 layers\n",
      "Step 320: Loss = -7.995404393059236 for 9 qubits with 10 layers\n",
      "Step 330: Loss = -7.996575510472258 for 9 qubits with 10 layers\n",
      "Step 340: Loss = -7.999989260333702 for 9 qubits with 10 layers\n",
      "Step 350: Loss = -7.999931329886895 for 9 qubits with 10 layers\n",
      "Step 360: Loss = -7.9946151586611425 for 9 qubits with 10 layers\n",
      "Step 370: Loss = -7.997147277999791 for 9 qubits with 10 layers\n",
      "Step 380: Loss = -7.999056088145024 for 9 qubits with 10 layers\n",
      "Step 390: Loss = -7.998777960865324 for 9 qubits with 10 layers\n",
      "Step 400: Loss = -7.997606770101778 for 9 qubits with 10 layers\n",
      "Step 410: Loss = -7.997242663716307 for 9 qubits with 10 layers\n",
      "Step 420: Loss = -7.998494141729618 for 9 qubits with 10 layers\n",
      "Step 430: Loss = -7.997462933123794 for 9 qubits with 10 layers\n",
      "Step 440: Loss = -7.998295859701353 for 9 qubits with 10 layers\n",
      "Step 450: Loss = -7.9993215987461745 for 9 qubits with 10 layers\n",
      "Step 460: Loss = -7.993975344306355 for 9 qubits with 10 layers\n",
      "Step 470: Loss = -7.995292334840676 for 9 qubits with 10 layers\n",
      "Step 480: Loss = -7.999817895586619 for 9 qubits with 10 layers\n",
      "Step 490: Loss = -7.99983711430604 for 9 qubits with 10 layers\n",
      "Running optimization for 9 qubits and 11 layers\n",
      "Step 0: Loss = -3.496677836731257 for 9 qubits with 11 layers\n",
      "Step 10: Loss = -7.47431893777725 for 9 qubits with 11 layers\n",
      "Step 20: Loss = -7.539270080446494 for 9 qubits with 11 layers\n",
      "Step 30: Loss = -7.746900302459123 for 9 qubits with 11 layers\n",
      "Step 40: Loss = -7.840340013810142 for 9 qubits with 11 layers\n",
      "Step 50: Loss = -7.940898614439612 for 9 qubits with 11 layers\n",
      "Step 60: Loss = -7.9683482236530345 for 9 qubits with 11 layers\n",
      "Step 70: Loss = -7.987041657449016 for 9 qubits with 11 layers\n",
      "Step 80: Loss = -7.994648835918175 for 9 qubits with 11 layers\n",
      "Step 90: Loss = -7.997283877459353 for 9 qubits with 11 layers\n",
      "Step 100: Loss = -7.9989824577117545 for 9 qubits with 11 layers\n",
      "Step 110: Loss = -7.999580575354381 for 9 qubits with 11 layers\n",
      "Step 120: Loss = -7.999746154755679 for 9 qubits with 11 layers\n",
      "Step 130: Loss = -7.999852156522287 for 9 qubits with 11 layers\n",
      "Step 140: Loss = -7.999900848396967 for 9 qubits with 11 layers\n",
      "Step 150: Loss = -7.999936282044013 for 9 qubits with 11 layers\n",
      "Step 160: Loss = -7.999962788691915 for 9 qubits with 11 layers\n",
      "Step 170: Loss = -7.999979163496716 for 9 qubits with 11 layers\n",
      "Step 180: Loss = -7.999988840761029 for 9 qubits with 11 layers\n",
      "Step 190: Loss = -7.999850407416956 for 9 qubits with 11 layers\n",
      "Step 200: Loss = -7.9493303093186665 for 9 qubits with 11 layers\n",
      "Step 210: Loss = -7.998701341371619 for 9 qubits with 11 layers\n",
      "Step 220: Loss = -7.997277107718418 for 9 qubits with 11 layers\n",
      "Step 230: Loss = -7.998046906842987 for 9 qubits with 11 layers\n",
      "Step 240: Loss = -7.999237255224216 for 9 qubits with 11 layers\n",
      "Step 250: Loss = -7.999719100140232 for 9 qubits with 11 layers\n",
      "Step 260: Loss = -7.999866248610246 for 9 qubits with 11 layers\n",
      "Step 270: Loss = -7.999933317499946 for 9 qubits with 11 layers\n",
      "Step 280: Loss = -7.999973500705705 for 9 qubits with 11 layers\n",
      "Step 290: Loss = -7.999977527917498 for 9 qubits with 11 layers\n",
      "Step 300: Loss = -7.9953802373244685 for 9 qubits with 11 layers\n",
      "Step 310: Loss = -7.996211372698981 for 9 qubits with 11 layers\n",
      "Step 320: Loss = -7.992804968556769 for 9 qubits with 11 layers\n",
      "Step 330: Loss = -7.9999310950447216 for 9 qubits with 11 layers\n",
      "Step 340: Loss = -7.9991423059203886 for 9 qubits with 11 layers\n",
      "Step 350: Loss = -7.99986402506097 for 9 qubits with 11 layers\n",
      "Step 360: Loss = -7.999636785041603 for 9 qubits with 11 layers\n",
      "Step 370: Loss = -7.998152102730188 for 9 qubits with 11 layers\n",
      "Step 380: Loss = -7.996368683454788 for 9 qubits with 11 layers\n",
      "Step 390: Loss = -7.998972043314426 for 9 qubits with 11 layers\n",
      "Step 400: Loss = -7.999988031843291 for 9 qubits with 11 layers\n",
      "Step 410: Loss = -7.998600367382757 for 9 qubits with 11 layers\n",
      "Step 420: Loss = -7.999240717556418 for 9 qubits with 11 layers\n",
      "Step 430: Loss = -7.989642330424884 for 9 qubits with 11 layers\n",
      "Step 440: Loss = -7.996569405801956 for 9 qubits with 11 layers\n",
      "Step 450: Loss = -7.998339881913566 for 9 qubits with 11 layers\n",
      "Step 460: Loss = -7.998715827034992 for 9 qubits with 11 layers\n",
      "Step 470: Loss = -7.9993491775224275 for 9 qubits with 11 layers\n",
      "Step 480: Loss = -7.998453175999115 for 9 qubits with 11 layers\n",
      "Step 490: Loss = -7.990637120873548 for 9 qubits with 11 layers\n",
      "Running optimization for 9 qubits and 12 layers\n",
      "Step 0: Loss = -3.0135621708832523 for 9 qubits with 12 layers\n",
      "Step 10: Loss = -7.072279844646746 for 9 qubits with 12 layers\n",
      "Step 20: Loss = -7.710249932114024 for 9 qubits with 12 layers\n",
      "Step 30: Loss = -7.838778406629354 for 9 qubits with 12 layers\n",
      "Step 40: Loss = -7.884027529983562 for 9 qubits with 12 layers\n",
      "Step 50: Loss = -7.9247915226314305 for 9 qubits with 12 layers\n",
      "Step 60: Loss = -7.967335195661554 for 9 qubits with 12 layers\n",
      "Step 70: Loss = -7.989525927224642 for 9 qubits with 12 layers\n",
      "Step 80: Loss = -7.997987330561099 for 9 qubits with 12 layers\n",
      "Step 90: Loss = -7.998876243454467 for 9 qubits with 12 layers\n",
      "Step 100: Loss = -7.999114706763308 for 9 qubits with 12 layers\n",
      "Step 110: Loss = -7.999526724468055 for 9 qubits with 12 layers\n",
      "Step 120: Loss = -7.999794808952133 for 9 qubits with 12 layers\n",
      "Step 130: Loss = -7.999894469957287 for 9 qubits with 12 layers\n",
      "Step 140: Loss = -7.999938436353664 for 9 qubits with 12 layers\n",
      "Step 150: Loss = -7.999970109038297 for 9 qubits with 12 layers\n",
      "Step 160: Loss = -7.999986731594183 for 9 qubits with 12 layers\n",
      "Step 170: Loss = -7.999993959515985 for 9 qubits with 12 layers\n",
      "Step 180: Loss = -7.9999975263221685 for 9 qubits with 12 layers\n",
      "Step 190: Loss = -7.999999151325785 for 9 qubits with 12 layers\n",
      "Step 200: Loss = -7.999999743771377 for 9 qubits with 12 layers\n",
      "Step 210: Loss = -7.999999936586335 for 9 qubits with 12 layers\n",
      "Step 220: Loss = -7.999999990790134 for 9 qubits with 12 layers\n",
      "Step 230: Loss = -7.999999655418653 for 9 qubits with 12 layers\n",
      "Step 240: Loss = -7.977062059238962 for 9 qubits with 12 layers\n",
      "Step 250: Loss = -7.993912024874881 for 9 qubits with 12 layers\n",
      "Step 260: Loss = -7.996540478565653 for 9 qubits with 12 layers\n",
      "Step 270: Loss = -7.999391631432643 for 9 qubits with 12 layers\n",
      "Step 280: Loss = -7.999933190053234 for 9 qubits with 12 layers\n",
      "Step 290: Loss = -7.99988117488013 for 9 qubits with 12 layers\n",
      "Step 300: Loss = -7.9999621320285925 for 9 qubits with 12 layers\n",
      "Step 310: Loss = -7.999994202097824 for 9 qubits with 12 layers\n",
      "Step 320: Loss = -7.999990522964574 for 9 qubits with 12 layers\n",
      "Step 330: Loss = -7.999910493443991 for 9 qubits with 12 layers\n",
      "Step 340: Loss = -7.999075802829038 for 9 qubits with 12 layers\n",
      "Step 350: Loss = -7.995889541740123 for 9 qubits with 12 layers\n",
      "Step 360: Loss = -7.998042264672429 for 9 qubits with 12 layers\n",
      "Step 370: Loss = -7.999321880082231 for 9 qubits with 12 layers\n",
      "Step 380: Loss = -7.999830095455061 for 9 qubits with 12 layers\n",
      "Step 390: Loss = -7.999940502815722 for 9 qubits with 12 layers\n",
      "Step 400: Loss = -7.999911051572765 for 9 qubits with 12 layers\n",
      "Step 410: Loss = -7.994951176150929 for 9 qubits with 12 layers\n",
      "Step 420: Loss = -7.996956476726544 for 9 qubits with 12 layers\n",
      "Step 430: Loss = -7.998361162478729 for 9 qubits with 12 layers\n",
      "Step 440: Loss = -7.999427485665585 for 9 qubits with 12 layers\n",
      "Step 450: Loss = -7.998351769818134 for 9 qubits with 12 layers\n",
      "Step 460: Loss = -7.999794598470595 for 9 qubits with 12 layers\n",
      "Step 470: Loss = -7.999753307801389 for 9 qubits with 12 layers\n",
      "Step 480: Loss = -7.999786399313122 for 9 qubits with 12 layers\n",
      "Step 490: Loss = -7.999989995939869 for 9 qubits with 12 layers\n",
      "Running optimization for 9 qubits and 13 layers\n",
      "Step 0: Loss = -2.409704848791691 for 9 qubits with 13 layers\n",
      "Step 10: Loss = -7.378633416137699 for 9 qubits with 13 layers\n",
      "Step 20: Loss = -7.762593722894167 for 9 qubits with 13 layers\n",
      "Step 30: Loss = -7.924274530402068 for 9 qubits with 13 layers\n",
      "Step 40: Loss = -7.974968880205358 for 9 qubits with 13 layers\n",
      "Step 50: Loss = -7.993245537679613 for 9 qubits with 13 layers\n",
      "Step 60: Loss = -7.996134610403274 for 9 qubits with 13 layers\n",
      "Step 70: Loss = -7.998850664391279 for 9 qubits with 13 layers\n",
      "Step 80: Loss = -7.999609435563476 for 9 qubits with 13 layers\n",
      "Step 90: Loss = -7.999887094686157 for 9 qubits with 13 layers\n",
      "Step 100: Loss = -7.999951914774394 for 9 qubits with 13 layers\n",
      "Step 110: Loss = -7.999978261823866 for 9 qubits with 13 layers\n",
      "Step 120: Loss = -7.999990278362793 for 9 qubits with 13 layers\n",
      "Step 130: Loss = -7.999996793509204 for 9 qubits with 13 layers\n",
      "Step 140: Loss = -7.999998924905279 for 9 qubits with 13 layers\n",
      "Step 150: Loss = -7.999967110941287 for 9 qubits with 13 layers\n",
      "Step 160: Loss = -7.9861092691516795 for 9 qubits with 13 layers\n",
      "Step 170: Loss = -7.999854645003536 for 9 qubits with 13 layers\n",
      "Step 180: Loss = -7.995132300530705 for 9 qubits with 13 layers\n",
      "Step 190: Loss = -7.997887957400251 for 9 qubits with 13 layers\n",
      "Step 200: Loss = -7.999203483236207 for 9 qubits with 13 layers\n",
      "Step 210: Loss = -7.999968485106458 for 9 qubits with 13 layers\n",
      "Step 220: Loss = -7.999993748189293 for 9 qubits with 13 layers\n",
      "Step 230: Loss = -7.999916486865466 for 9 qubits with 13 layers\n",
      "Step 240: Loss = -7.999774869728654 for 9 qubits with 13 layers\n",
      "Step 250: Loss = -7.965706718455301 for 9 qubits with 13 layers\n",
      "Step 260: Loss = -7.99681713453643 for 9 qubits with 13 layers\n",
      "Step 270: Loss = -7.998232874475733 for 9 qubits with 13 layers\n",
      "Step 280: Loss = -7.998413256589161 for 9 qubits with 13 layers\n",
      "Step 290: Loss = -7.999002280297228 for 9 qubits with 13 layers\n",
      "Step 300: Loss = -7.999861717704204 for 9 qubits with 13 layers\n",
      "Step 310: Loss = -7.999879688398693 for 9 qubits with 13 layers\n",
      "Step 320: Loss = -7.982904740893618 for 9 qubits with 13 layers\n",
      "Step 330: Loss = -7.999314607147532 for 9 qubits with 13 layers\n",
      "Step 340: Loss = -7.999939360338201 for 9 qubits with 13 layers\n",
      "Step 350: Loss = -7.999966731727917 for 9 qubits with 13 layers\n",
      "Step 360: Loss = -7.9987334962179615 for 9 qubits with 13 layers\n",
      "Step 370: Loss = -7.998757467207399 for 9 qubits with 13 layers\n",
      "Step 380: Loss = -7.984348438974496 for 9 qubits with 13 layers\n",
      "Step 390: Loss = -7.9982175782922145 for 9 qubits with 13 layers\n",
      "Step 400: Loss = -7.999878936772213 for 9 qubits with 13 layers\n",
      "Step 410: Loss = -7.999103571713119 for 9 qubits with 13 layers\n",
      "Step 420: Loss = -7.999732171088348 for 9 qubits with 13 layers\n",
      "Step 430: Loss = -7.997149174427095 for 9 qubits with 13 layers\n",
      "Step 440: Loss = -7.988880623917396 for 9 qubits with 13 layers\n",
      "Step 450: Loss = -7.9962644264563405 for 9 qubits with 13 layers\n",
      "Step 460: Loss = -7.998232322057248 for 9 qubits with 13 layers\n",
      "Step 470: Loss = -7.99936070809007 for 9 qubits with 13 layers\n",
      "Step 480: Loss = -7.999796453777088 for 9 qubits with 13 layers\n",
      "Step 490: Loss = -7.999889765270965 for 9 qubits with 13 layers\n",
      "Running optimization for 9 qubits and 14 layers\n",
      "Step 0: Loss = -2.2162163798675967 for 9 qubits with 14 layers\n",
      "Step 10: Loss = -7.307866723186954 for 9 qubits with 14 layers\n",
      "Step 20: Loss = -7.863868957768856 for 9 qubits with 14 layers\n",
      "Step 30: Loss = -7.939245338008145 for 9 qubits with 14 layers\n",
      "Step 40: Loss = -7.973351897314931 for 9 qubits with 14 layers\n",
      "Step 50: Loss = -7.989354015454655 for 9 qubits with 14 layers\n",
      "Step 60: Loss = -7.996330639295451 for 9 qubits with 14 layers\n",
      "Step 70: Loss = -7.9991124300712695 for 9 qubits with 14 layers\n",
      "Step 80: Loss = -7.999599857988325 for 9 qubits with 14 layers\n",
      "Step 90: Loss = -7.999890015694335 for 9 qubits with 14 layers\n",
      "Step 100: Loss = -7.99994247962486 for 9 qubits with 14 layers\n",
      "Step 110: Loss = -7.999983170834612 for 9 qubits with 14 layers\n",
      "Step 120: Loss = -7.999994796334188 for 9 qubits with 14 layers\n",
      "Step 130: Loss = -7.999997936140256 for 9 qubits with 14 layers\n",
      "Step 140: Loss = -7.999999349410391 for 9 qubits with 14 layers\n",
      "Step 150: Loss = -7.999999677804702 for 9 qubits with 14 layers\n",
      "Step 160: Loss = -7.9999997868461055 for 9 qubits with 14 layers\n",
      "Step 170: Loss = -7.99999993967603 for 9 qubits with 14 layers\n",
      "Step 180: Loss = -7.999999993581538 for 9 qubits with 14 layers\n",
      "Step 190: Loss = -7.99999999343457 for 9 qubits with 14 layers\n",
      "Step 200: Loss = -7.999999996510931 for 9 qubits with 14 layers\n",
      "Step 210: Loss = -7.999999642836568 for 9 qubits with 14 layers\n",
      "Step 220: Loss = -7.969684320999884 for 9 qubits with 14 layers\n",
      "Step 230: Loss = -7.989419535948377 for 9 qubits with 14 layers\n",
      "Step 240: Loss = -7.999970212904929 for 9 qubits with 14 layers\n",
      "Step 250: Loss = -7.998874253947088 for 9 qubits with 14 layers\n",
      "Step 260: Loss = -7.998388460917185 for 9 qubits with 14 layers\n",
      "Step 270: Loss = -7.999970859546288 for 9 qubits with 14 layers\n",
      "Step 280: Loss = -7.999852997122827 for 9 qubits with 14 layers\n",
      "Step 290: Loss = -7.999967410020028 for 9 qubits with 14 layers\n",
      "Step 300: Loss = -7.999976990247537 for 9 qubits with 14 layers\n",
      "Step 310: Loss = -7.999995976222387 for 9 qubits with 14 layers\n",
      "Step 320: Loss = -7.999990552041515 for 9 qubits with 14 layers\n",
      "Step 330: Loss = -7.999997102972544 for 9 qubits with 14 layers\n",
      "Step 340: Loss = -7.997200270238335 for 9 qubits with 14 layers\n",
      "Step 350: Loss = -7.995128498915409 for 9 qubits with 14 layers\n",
      "Step 360: Loss = -7.99940831279763 for 9 qubits with 14 layers\n",
      "Step 370: Loss = -7.999497327401826 for 9 qubits with 14 layers\n",
      "Step 380: Loss = -7.999919516812878 for 9 qubits with 14 layers\n",
      "Step 390: Loss = -7.9998037892129075 for 9 qubits with 14 layers\n",
      "Step 400: Loss = -7.999894375019156 for 9 qubits with 14 layers\n",
      "Step 410: Loss = -7.99997755198276 for 9 qubits with 14 layers\n",
      "Step 420: Loss = -7.999929859807902 for 9 qubits with 14 layers\n",
      "Step 430: Loss = -7.999997546089266 for 9 qubits with 14 layers\n",
      "Step 440: Loss = -7.998688937184229 for 9 qubits with 14 layers\n",
      "Step 450: Loss = -7.989621049872384 for 9 qubits with 14 layers\n",
      "Step 460: Loss = -7.98703949561293 for 9 qubits with 14 layers\n",
      "Step 470: Loss = -7.999671568217728 for 9 qubits with 14 layers\n",
      "Step 480: Loss = -7.998583624837212 for 9 qubits with 14 layers\n",
      "Step 490: Loss = -7.999715893066757 for 9 qubits with 14 layers\n",
      "Running optimization for 9 qubits and 15 layers\n",
      "Step 0: Loss = -2.137974246187707 for 9 qubits with 15 layers\n",
      "Step 10: Loss = -7.064204027209228 for 9 qubits with 15 layers\n",
      "Step 20: Loss = -7.617445853856367 for 9 qubits with 15 layers\n",
      "Step 30: Loss = -7.937694194561768 for 9 qubits with 15 layers\n",
      "Step 40: Loss = -7.966409264970234 for 9 qubits with 15 layers\n",
      "Step 50: Loss = -7.986374505419577 for 9 qubits with 15 layers\n",
      "Step 60: Loss = -7.9967943454712636 for 9 qubits with 15 layers\n",
      "Step 70: Loss = -7.998603186592117 for 9 qubits with 15 layers\n",
      "Step 80: Loss = -7.999289214921328 for 9 qubits with 15 layers\n",
      "Step 90: Loss = -7.999802159449252 for 9 qubits with 15 layers\n",
      "Step 100: Loss = -7.999923231267051 for 9 qubits with 15 layers\n",
      "Step 110: Loss = -7.999965760700638 for 9 qubits with 15 layers\n",
      "Step 120: Loss = -7.999985104390243 for 9 qubits with 15 layers\n",
      "Step 130: Loss = -7.99999625752619 for 9 qubits with 15 layers\n",
      "Step 140: Loss = -7.999998231237451 for 9 qubits with 15 layers\n",
      "Step 150: Loss = -7.999999441830605 for 9 qubits with 15 layers\n",
      "Step 160: Loss = -7.999999814496255 for 9 qubits with 15 layers\n",
      "Step 170: Loss = -7.999999920295117 for 9 qubits with 15 layers\n",
      "Step 180: Loss = -7.99999997461615 for 9 qubits with 15 layers\n",
      "Step 190: Loss = -7.999999989147821 for 9 qubits with 15 layers\n",
      "Step 200: Loss = -7.999999996638902 for 9 qubits with 15 layers\n",
      "Step 210: Loss = -7.999999993491698 for 9 qubits with 15 layers\n",
      "Step 220: Loss = -7.9999618539512705 for 9 qubits with 15 layers\n",
      "Step 230: Loss = -7.976473393185355 for 9 qubits with 15 layers\n",
      "Step 240: Loss = -7.986475426785871 for 9 qubits with 15 layers\n",
      "Step 250: Loss = -7.9916253490190545 for 9 qubits with 15 layers\n",
      "Step 260: Loss = -7.998884695235888 for 9 qubits with 15 layers\n",
      "Step 270: Loss = -7.999293880402176 for 9 qubits with 15 layers\n",
      "Step 280: Loss = -7.99998941699586 for 9 qubits with 15 layers\n",
      "Step 290: Loss = -7.999994370585683 for 9 qubits with 15 layers\n",
      "Step 300: Loss = -7.999928857919137 for 9 qubits with 15 layers\n",
      "Step 310: Loss = -7.999983471676882 for 9 qubits with 15 layers\n",
      "Step 320: Loss = -7.999984528490931 for 9 qubits with 15 layers\n",
      "Step 330: Loss = -7.999987116488655 for 9 qubits with 15 layers\n",
      "Step 340: Loss = -7.996402805812743 for 9 qubits with 15 layers\n",
      "Step 350: Loss = -7.986117599355374 for 9 qubits with 15 layers\n",
      "Step 360: Loss = -7.990733785495907 for 9 qubits with 15 layers\n",
      "Step 370: Loss = -7.996072988029962 for 9 qubits with 15 layers\n",
      "Step 380: Loss = -7.999245869817304 for 9 qubits with 15 layers\n",
      "Step 390: Loss = -7.999530692391467 for 9 qubits with 15 layers\n",
      "Step 400: Loss = -7.999769869219378 for 9 qubits with 15 layers\n",
      "Step 410: Loss = -7.982933103380929 for 9 qubits with 15 layers\n",
      "Step 420: Loss = -7.9968697356517335 for 9 qubits with 15 layers\n",
      "Step 430: Loss = -7.998343340390469 for 9 qubits with 15 layers\n",
      "Step 440: Loss = -7.998982449352743 for 9 qubits with 15 layers\n",
      "Step 450: Loss = -7.987084022120823 for 9 qubits with 15 layers\n",
      "Step 460: Loss = -7.9959727976911665 for 9 qubits with 15 layers\n",
      "Step 470: Loss = -7.999077946147261 for 9 qubits with 15 layers\n",
      "Step 480: Loss = -7.999267901625572 for 9 qubits with 15 layers\n",
      "Step 490: Loss = -7.997827558718971 for 9 qubits with 15 layers\n",
      "Running optimization for 10 qubits and 1 layers\n",
      "Step 0: Loss = -7.424425537310263 for 10 qubits with 1 layers\n",
      "Step 10: Loss = -8.512201837402232 for 10 qubits with 1 layers\n",
      "Step 20: Loss = -8.962097040164018 for 10 qubits with 1 layers\n",
      "Step 30: Loss = -8.988082138554693 for 10 qubits with 1 layers\n",
      "Step 40: Loss = -8.976164461117046 for 10 qubits with 1 layers\n",
      "Step 50: Loss = -8.996830036848 for 10 qubits with 1 layers\n",
      "Step 60: Loss = -8.999228227026526 for 10 qubits with 1 layers\n",
      "Step 70: Loss = -8.99883448363964 for 10 qubits with 1 layers\n",
      "Step 80: Loss = -8.999992632258497 for 10 qubits with 1 layers\n",
      "Step 90: Loss = -8.9998610649923 for 10 qubits with 1 layers\n",
      "Step 100: Loss = -8.999986823876522 for 10 qubits with 1 layers\n",
      "Step 110: Loss = -8.999985860466024 for 10 qubits with 1 layers\n",
      "Step 120: Loss = -8.999997496672094 for 10 qubits with 1 layers\n",
      "Step 130: Loss = -8.999998151121982 for 10 qubits with 1 layers\n",
      "Step 140: Loss = -8.99999978550217 for 10 qubits with 1 layers\n",
      "Step 150: Loss = -8.999999692805762 for 10 qubits with 1 layers\n",
      "Step 160: Loss = -8.99999999821227 for 10 qubits with 1 layers\n",
      "Step 170: Loss = -8.99999995661765 for 10 qubits with 1 layers\n",
      "Step 180: Loss = -8.999999996373703 for 10 qubits with 1 layers\n",
      "Step 190: Loss = -8.999999997780105 for 10 qubits with 1 layers\n",
      "Step 200: Loss = -8.999999998035355 for 10 qubits with 1 layers\n",
      "Step 210: Loss = -8.999999999927129 for 10 qubits with 1 layers\n",
      "Step 220: Loss = -8.999999999901302 for 10 qubits with 1 layers\n",
      "Step 230: Loss = -8.999999999905546 for 10 qubits with 1 layers\n",
      "Step 240: Loss = -8.999999999985162 for 10 qubits with 1 layers\n",
      "Step 250: Loss = -8.99999999999989 for 10 qubits with 1 layers\n",
      "Step 260: Loss = -8.999999999997879 for 10 qubits with 1 layers\n",
      "Step 270: Loss = -8.999999999998419 for 10 qubits with 1 layers\n",
      "Step 280: Loss = -8.999999999999432 for 10 qubits with 1 layers\n",
      "Step 290: Loss = -8.999999999999886 for 10 qubits with 1 layers\n",
      "Step 300: Loss = -8.999999999999938 for 10 qubits with 1 layers\n",
      "Step 310: Loss = -8.999999999999961 for 10 qubits with 1 layers\n",
      "Step 320: Loss = -8.99999999999995 for 10 qubits with 1 layers\n",
      "Step 330: Loss = -8.999999999999918 for 10 qubits with 1 layers\n",
      "Step 340: Loss = -8.99999999999995 for 10 qubits with 1 layers\n",
      "Step 350: Loss = -8.999999999999966 for 10 qubits with 1 layers\n",
      "Step 360: Loss = -8.999999999999961 for 10 qubits with 1 layers\n",
      "Step 370: Loss = -8.999999999999938 for 10 qubits with 1 layers\n",
      "Step 380: Loss = -8.999999999999929 for 10 qubits with 1 layers\n",
      "Step 390: Loss = -8.999999999999929 for 10 qubits with 1 layers\n",
      "Step 400: Loss = -8.999999999999986 for 10 qubits with 1 layers\n",
      "Step 410: Loss = -8.99999999999996 for 10 qubits with 1 layers\n",
      "Step 420: Loss = -8.99999999999995 for 10 qubits with 1 layers\n",
      "Step 430: Loss = -8.999999999999911 for 10 qubits with 1 layers\n",
      "Step 440: Loss = -8.999999999999998 for 10 qubits with 1 layers\n",
      "Step 450: Loss = -8.99999999999996 for 10 qubits with 1 layers\n",
      "Step 460: Loss = -9.000000000000014 for 10 qubits with 1 layers\n",
      "Step 470: Loss = -8.999999999999982 for 10 qubits with 1 layers\n",
      "Step 480: Loss = -8.999999999999966 for 10 qubits with 1 layers\n",
      "Step 490: Loss = -8.999999999999888 for 10 qubits with 1 layers\n",
      "Running optimization for 10 qubits and 2 layers\n",
      "Step 0: Loss = -1.8964852327611301 for 10 qubits with 2 layers\n",
      "Step 10: Loss = -3.7101967427194538 for 10 qubits with 2 layers\n",
      "Step 20: Loss = -4.69612863060601 for 10 qubits with 2 layers\n",
      "Step 30: Loss = -5.3530388683408505 for 10 qubits with 2 layers\n",
      "Step 40: Loss = -6.884898294085734 for 10 qubits with 2 layers\n",
      "Step 50: Loss = -8.489783040048238 for 10 qubits with 2 layers\n",
      "Step 60: Loss = -8.968770559429897 for 10 qubits with 2 layers\n",
      "Step 70: Loss = -8.941608697590086 for 10 qubits with 2 layers\n",
      "Step 80: Loss = -8.974738019876742 for 10 qubits with 2 layers\n",
      "Step 90: Loss = -8.993556102634365 for 10 qubits with 2 layers\n",
      "Step 100: Loss = -8.998218010024983 for 10 qubits with 2 layers\n",
      "Step 110: Loss = -8.998986228784435 for 10 qubits with 2 layers\n",
      "Step 120: Loss = -8.99956605459964 for 10 qubits with 2 layers\n",
      "Step 130: Loss = -8.999979302470539 for 10 qubits with 2 layers\n",
      "Step 140: Loss = -8.999917620524487 for 10 qubits with 2 layers\n",
      "Step 150: Loss = -8.999998468327002 for 10 qubits with 2 layers\n",
      "Step 160: Loss = -8.999991286652149 for 10 qubits with 2 layers\n",
      "Step 170: Loss = -8.999998751442021 for 10 qubits with 2 layers\n",
      "Step 180: Loss = -8.999999288188377 for 10 qubits with 2 layers\n",
      "Step 190: Loss = -8.999999689913341 for 10 qubits with 2 layers\n",
      "Step 200: Loss = -8.999999964747463 for 10 qubits with 2 layers\n",
      "Step 210: Loss = -8.999999941864871 for 10 qubits with 2 layers\n",
      "Step 220: Loss = -8.999999998639016 for 10 qubits with 2 layers\n",
      "Step 230: Loss = -8.999999993778518 for 10 qubits with 2 layers\n",
      "Step 240: Loss = -8.999999998631033 for 10 qubits with 2 layers\n",
      "Step 250: Loss = -8.99999999956993 for 10 qubits with 2 layers\n",
      "Step 260: Loss = -8.999999999832145 for 10 qubits with 2 layers\n",
      "Step 270: Loss = -8.999999999927274 for 10 qubits with 2 layers\n",
      "Step 280: Loss = -8.999999999961632 for 10 qubits with 2 layers\n",
      "Step 290: Loss = -8.999999999999321 for 10 qubits with 2 layers\n",
      "Step 300: Loss = -8.999999999997172 for 10 qubits with 2 layers\n",
      "Step 310: Loss = -8.999999999998334 for 10 qubits with 2 layers\n",
      "Step 320: Loss = -8.999999999999625 for 10 qubits with 2 layers\n",
      "Step 330: Loss = -8.999999999999751 for 10 qubits with 2 layers\n",
      "Step 340: Loss = -8.999999999999849 for 10 qubits with 2 layers\n",
      "Step 350: Loss = -8.99999999999989 for 10 qubits with 2 layers\n",
      "Step 360: Loss = -8.999999999999954 for 10 qubits with 2 layers\n",
      "Step 370: Loss = -8.999999999999977 for 10 qubits with 2 layers\n",
      "Step 380: Loss = -8.999999999999954 for 10 qubits with 2 layers\n",
      "Step 390: Loss = -8.999999999999993 for 10 qubits with 2 layers\n",
      "Step 400: Loss = -8.999999999999922 for 10 qubits with 2 layers\n",
      "Step 410: Loss = -8.99999999999996 for 10 qubits with 2 layers\n",
      "Step 420: Loss = -8.99999999999995 for 10 qubits with 2 layers\n",
      "Step 430: Loss = -8.999999999999986 for 10 qubits with 2 layers\n",
      "Step 440: Loss = -8.999999999999952 for 10 qubits with 2 layers\n",
      "Step 450: Loss = -8.999999999999906 for 10 qubits with 2 layers\n",
      "Step 460: Loss = -8.999999999999913 for 10 qubits with 2 layers\n",
      "Step 470: Loss = -8.999999999999952 for 10 qubits with 2 layers\n",
      "Step 480: Loss = -8.999999999999943 for 10 qubits with 2 layers\n",
      "Step 490: Loss = -8.999999999999998 for 10 qubits with 2 layers\n",
      "Running optimization for 10 qubits and 3 layers\n",
      "Step 0: Loss = -2.1100268942081963 for 10 qubits with 3 layers\n",
      "Step 10: Loss = -4.286924221287575 for 10 qubits with 3 layers\n",
      "Step 20: Loss = -5.265701397895576 for 10 qubits with 3 layers\n",
      "Step 30: Loss = -7.003460034047379 for 10 qubits with 3 layers\n",
      "Step 40: Loss = -8.19088453719026 for 10 qubits with 3 layers\n",
      "Step 50: Loss = -8.816490467898332 for 10 qubits with 3 layers\n",
      "Step 60: Loss = -8.9581201797159 for 10 qubits with 3 layers\n",
      "Step 70: Loss = -8.976702974774835 for 10 qubits with 3 layers\n",
      "Step 80: Loss = -8.988984121098197 for 10 qubits with 3 layers\n",
      "Step 90: Loss = -8.993972180482599 for 10 qubits with 3 layers\n",
      "Step 100: Loss = -8.997289720787782 for 10 qubits with 3 layers\n",
      "Step 110: Loss = -8.998725986094765 for 10 qubits with 3 layers\n",
      "Step 120: Loss = -8.999531154316577 for 10 qubits with 3 layers\n",
      "Step 130: Loss = -8.99986066425398 for 10 qubits with 3 layers\n",
      "Step 140: Loss = -8.999967860438451 for 10 qubits with 3 layers\n",
      "Step 150: Loss = -8.999995661003092 for 10 qubits with 3 layers\n",
      "Step 160: Loss = -8.999999905937322 for 10 qubits with 3 layers\n",
      "Step 170: Loss = -8.999999658883794 for 10 qubits with 3 layers\n",
      "Step 180: Loss = -8.99999951591698 for 10 qubits with 3 layers\n",
      "Step 190: Loss = -8.99999972414198 for 10 qubits with 3 layers\n",
      "Step 200: Loss = -8.999999921191879 for 10 qubits with 3 layers\n",
      "Step 210: Loss = -8.999999989550174 for 10 qubits with 3 layers\n",
      "Step 220: Loss = -8.999999998750022 for 10 qubits with 3 layers\n",
      "Step 230: Loss = -8.999999998422476 for 10 qubits with 3 layers\n",
      "Step 240: Loss = -8.999999998601549 for 10 qubits with 3 layers\n",
      "Step 250: Loss = -8.999999999563062 for 10 qubits with 3 layers\n",
      "Step 260: Loss = -8.999999999972253 for 10 qubits with 3 layers\n",
      "Step 270: Loss = -8.999999999990933 for 10 qubits with 3 layers\n",
      "Step 280: Loss = -8.999999999981238 for 10 qubits with 3 layers\n",
      "Step 290: Loss = -8.999999999993284 for 10 qubits with 3 layers\n",
      "Step 300: Loss = -8.99999999999965 for 10 qubits with 3 layers\n",
      "Step 310: Loss = -8.999999999999627 for 10 qubits with 3 layers\n",
      "Step 320: Loss = -8.999999999999467 for 10 qubits with 3 layers\n",
      "Step 330: Loss = -8.999999999999892 for 10 qubits with 3 layers\n",
      "Step 340: Loss = -9.000000000000016 for 10 qubits with 3 layers\n",
      "Step 350: Loss = -8.999999999999906 for 10 qubits with 3 layers\n",
      "Step 360: Loss = -8.999999999999908 for 10 qubits with 3 layers\n",
      "Step 370: Loss = -8.999999999999874 for 10 qubits with 3 layers\n",
      "Step 380: Loss = -8.999999999999913 for 10 qubits with 3 layers\n",
      "Step 390: Loss = -8.99999999999987 for 10 qubits with 3 layers\n",
      "Step 400: Loss = -8.999999999999938 for 10 qubits with 3 layers\n",
      "Step 410: Loss = -8.999999999999881 for 10 qubits with 3 layers\n",
      "Step 420: Loss = -8.999999999999838 for 10 qubits with 3 layers\n",
      "Step 430: Loss = -8.999999999999762 for 10 qubits with 3 layers\n",
      "Step 440: Loss = -8.999999999999925 for 10 qubits with 3 layers\n",
      "Step 450: Loss = -8.999999999999872 for 10 qubits with 3 layers\n",
      "Step 460: Loss = -8.999999999999874 for 10 qubits with 3 layers\n",
      "Step 470: Loss = -8.999999999999893 for 10 qubits with 3 layers\n",
      "Step 480: Loss = -8.999999999999847 for 10 qubits with 3 layers\n",
      "Step 490: Loss = -8.999999999999817 for 10 qubits with 3 layers\n",
      "Running optimization for 10 qubits and 4 layers\n",
      "Step 0: Loss = -4.023099646124796 for 10 qubits with 4 layers\n",
      "Step 10: Loss = -5.767733116717025 for 10 qubits with 4 layers\n",
      "Step 20: Loss = -5.898145115975722 for 10 qubits with 4 layers\n",
      "Step 30: Loss = -6.204611244387294 for 10 qubits with 4 layers\n",
      "Step 40: Loss = -6.733244982910942 for 10 qubits with 4 layers\n",
      "Step 50: Loss = -8.040388831026153 for 10 qubits with 4 layers\n",
      "Step 60: Loss = -8.610770450873988 for 10 qubits with 4 layers\n",
      "Step 70: Loss = -8.903164797399244 for 10 qubits with 4 layers\n",
      "Step 80: Loss = -8.952409930889155 for 10 qubits with 4 layers\n",
      "Step 90: Loss = -8.965870241399568 for 10 qubits with 4 layers\n",
      "Step 100: Loss = -8.97859357418481 for 10 qubits with 4 layers\n",
      "Step 110: Loss = -8.98499831107859 for 10 qubits with 4 layers\n",
      "Step 120: Loss = -8.988689266162883 for 10 qubits with 4 layers\n",
      "Step 130: Loss = -8.990566926633912 for 10 qubits with 4 layers\n",
      "Step 140: Loss = -8.992087157292401 for 10 qubits with 4 layers\n",
      "Step 150: Loss = -8.993272590093543 for 10 qubits with 4 layers\n",
      "Step 160: Loss = -8.994221049776847 for 10 qubits with 4 layers\n",
      "Step 170: Loss = -8.99500066005856 for 10 qubits with 4 layers\n",
      "Step 180: Loss = -8.995648664521537 for 10 qubits with 4 layers\n",
      "Step 190: Loss = -8.996193260751994 for 10 qubits with 4 layers\n",
      "Step 200: Loss = -8.996654706869316 for 10 qubits with 4 layers\n",
      "Step 210: Loss = -8.99704853042792 for 10 qubits with 4 layers\n",
      "Step 220: Loss = -8.99738651603313 for 10 qubits with 4 layers\n",
      "Step 230: Loss = -8.997678092256521 for 10 qubits with 4 layers\n",
      "Step 240: Loss = -8.997930821070295 for 10 qubits with 4 layers\n",
      "Step 250: Loss = -8.998150846861401 for 10 qubits with 4 layers\n",
      "Step 260: Loss = -8.998343178996485 for 10 qubits with 4 layers\n",
      "Step 270: Loss = -8.998511937355499 for 10 qubits with 4 layers\n",
      "Step 280: Loss = -8.998660529384585 for 10 qubits with 4 layers\n",
      "Step 290: Loss = -8.998791791195304 for 10 qubits with 4 layers\n",
      "Step 300: Loss = -8.9989080959168 for 10 qubits with 4 layers\n",
      "Step 310: Loss = -8.999011439722146 for 10 qubits with 4 layers\n",
      "Step 320: Loss = -8.999103509937674 for 10 qubits with 4 layers\n",
      "Step 330: Loss = -8.999185739393434 for 10 qubits with 4 layers\n",
      "Step 340: Loss = -8.99925935013604 for 10 qubits with 4 layers\n",
      "Step 350: Loss = -8.999325388717017 for 10 qubits with 4 layers\n",
      "Step 360: Loss = -8.999384754897447 for 10 qubits with 4 layers\n",
      "Step 370: Loss = -8.99943822509564 for 10 qubits with 4 layers\n",
      "Step 380: Loss = -8.99948647163847 for 10 qubits with 4 layers\n",
      "Step 390: Loss = -8.999530078653281 for 10 qubits with 4 layers\n",
      "Step 400: Loss = -8.999569555237654 for 10 qubits with 4 layers\n",
      "Step 410: Loss = -8.999605346426396 for 10 qubits with 4 layers\n",
      "Step 420: Loss = -8.999637842362848 for 10 qubits with 4 layers\n",
      "Step 430: Loss = -8.999667385996169 for 10 qubits with 4 layers\n",
      "Step 440: Loss = -8.999694279569244 for 10 qubits with 4 layers\n",
      "Step 450: Loss = -8.999718790105058 for 10 qubits with 4 layers\n",
      "Step 460: Loss = -8.999741154064761 for 10 qubits with 4 layers\n",
      "Step 470: Loss = -8.9997615813136 for 10 qubits with 4 layers\n",
      "Step 480: Loss = -8.999780258510318 for 10 qubits with 4 layers\n",
      "Step 490: Loss = -8.99979735201089 for 10 qubits with 4 layers\n",
      "Running optimization for 10 qubits and 5 layers\n",
      "Step 0: Loss = -3.9223923413356845 for 10 qubits with 5 layers\n",
      "Step 10: Loss = -6.244740409327388 for 10 qubits with 5 layers\n",
      "Step 20: Loss = -6.931694180788799 for 10 qubits with 5 layers\n",
      "Step 30: Loss = -7.207769098100817 for 10 qubits with 5 layers\n",
      "Step 40: Loss = -7.223734594642167 for 10 qubits with 5 layers\n",
      "Step 50: Loss = -7.248531207590365 for 10 qubits with 5 layers\n",
      "Step 60: Loss = -7.255261478940002 for 10 qubits with 5 layers\n",
      "Step 70: Loss = -7.260666873673459 for 10 qubits with 5 layers\n",
      "Step 80: Loss = -7.261713074093367 for 10 qubits with 5 layers\n",
      "Step 90: Loss = -7.262167872633417 for 10 qubits with 5 layers\n",
      "Step 100: Loss = -7.262225273092651 for 10 qubits with 5 layers\n",
      "Step 110: Loss = -7.262255964412102 for 10 qubits with 5 layers\n",
      "Step 120: Loss = -7.26227157667427 for 10 qubits with 5 layers\n",
      "Step 130: Loss = -7.262274741005726 for 10 qubits with 5 layers\n",
      "Step 140: Loss = -7.262278185488754 for 10 qubits with 5 layers\n",
      "Step 150: Loss = -7.262279709986466 for 10 qubits with 5 layers\n",
      "Step 160: Loss = -7.262280041792874 for 10 qubits with 5 layers\n",
      "Step 170: Loss = -7.262280105470869 for 10 qubits with 5 layers\n",
      "Step 180: Loss = -7.26228015506187 for 10 qubits with 5 layers\n",
      "Step 190: Loss = -7.262280184213154 for 10 qubits with 5 layers\n",
      "Step 200: Loss = -7.26228018463577 for 10 qubits with 5 layers\n",
      "Step 210: Loss = -7.262280186738669 for 10 qubits with 5 layers\n",
      "Step 220: Loss = -7.262280187232386 for 10 qubits with 5 layers\n",
      "Step 230: Loss = -7.262280187614728 for 10 qubits with 5 layers\n",
      "Step 240: Loss = -7.26228018768006 for 10 qubits with 5 layers\n",
      "Step 250: Loss = -7.2622801877458425 for 10 qubits with 5 layers\n",
      "Step 260: Loss = -7.262280187744156 for 10 qubits with 5 layers\n",
      "Step 270: Loss = -7.262280184577681 for 10 qubits with 5 layers\n",
      "Step 280: Loss = -7.261996277863783 for 10 qubits with 5 layers\n",
      "Step 290: Loss = -7.257092298407667 for 10 qubits with 5 layers\n",
      "Step 300: Loss = -7.260570863613326 for 10 qubits with 5 layers\n",
      "Step 310: Loss = -7.261808015117447 for 10 qubits with 5 layers\n",
      "Step 320: Loss = -7.262071985555312 for 10 qubits with 5 layers\n",
      "Step 330: Loss = -7.262186486207744 for 10 qubits with 5 layers\n",
      "Step 340: Loss = -7.262278601559273 for 10 qubits with 5 layers\n",
      "Step 350: Loss = -7.262275777856853 for 10 qubits with 5 layers\n",
      "Step 360: Loss = -7.262279337299279 for 10 qubits with 5 layers\n",
      "Step 370: Loss = -7.262278683114756 for 10 qubits with 5 layers\n",
      "Step 380: Loss = -7.262280047310028 for 10 qubits with 5 layers\n",
      "Step 390: Loss = -7.2622796336520725 for 10 qubits with 5 layers\n",
      "Step 400: Loss = -7.262279542315211 for 10 qubits with 5 layers\n",
      "Step 410: Loss = -7.262139319265559 for 10 qubits with 5 layers\n",
      "Step 420: Loss = -7.258828179510662 for 10 qubits with 5 layers\n",
      "Step 430: Loss = -7.261083963126163 for 10 qubits with 5 layers\n",
      "Step 440: Loss = -7.261996089555326 for 10 qubits with 5 layers\n",
      "Step 450: Loss = -7.2622707132084745 for 10 qubits with 5 layers\n",
      "Step 460: Loss = -7.262233808471365 for 10 qubits with 5 layers\n",
      "Step 470: Loss = -7.2622700820982775 for 10 qubits with 5 layers\n",
      "Step 480: Loss = -7.262255453318029 for 10 qubits with 5 layers\n",
      "Step 490: Loss = -7.262264204122649 for 10 qubits with 5 layers\n",
      "Running optimization for 10 qubits and 6 layers\n",
      "Step 0: Loss = -4.46109577544958 for 10 qubits with 6 layers\n",
      "Step 10: Loss = -6.764093902526811 for 10 qubits with 6 layers\n",
      "Step 20: Loss = -7.615868857073697 for 10 qubits with 6 layers\n",
      "Step 30: Loss = -7.870344721596322 for 10 qubits with 6 layers\n",
      "Step 40: Loss = -8.123986278046685 for 10 qubits with 6 layers\n",
      "Step 50: Loss = -8.21270790882297 for 10 qubits with 6 layers\n",
      "Step 60: Loss = -8.334686523305924 for 10 qubits with 6 layers\n",
      "Step 70: Loss = -8.365197885710925 for 10 qubits with 6 layers\n",
      "Step 80: Loss = -8.390308139289205 for 10 qubits with 6 layers\n",
      "Step 90: Loss = -8.403492948837036 for 10 qubits with 6 layers\n",
      "Step 100: Loss = -8.41119695039421 for 10 qubits with 6 layers\n",
      "Step 110: Loss = -8.413349370466396 for 10 qubits with 6 layers\n",
      "Step 120: Loss = -8.413639596009324 for 10 qubits with 6 layers\n",
      "Step 130: Loss = -8.41357188743277 for 10 qubits with 6 layers\n",
      "Step 140: Loss = -8.413666372021275 for 10 qubits with 6 layers\n",
      "Step 150: Loss = -8.41371276724215 for 10 qubits with 6 layers\n",
      "Step 160: Loss = -8.413710852850334 for 10 qubits with 6 layers\n",
      "Step 170: Loss = -8.413711582262817 for 10 qubits with 6 layers\n",
      "Step 180: Loss = -8.413714096591507 for 10 qubits with 6 layers\n",
      "Step 190: Loss = -8.413714309161971 for 10 qubits with 6 layers\n",
      "Step 200: Loss = -8.413714289094633 for 10 qubits with 6 layers\n",
      "Step 210: Loss = -8.413714389362875 for 10 qubits with 6 layers\n",
      "Step 220: Loss = -8.413714394665586 for 10 qubits with 6 layers\n",
      "Step 230: Loss = -8.413714396826336 for 10 qubits with 6 layers\n",
      "Step 240: Loss = -8.413714401820778 for 10 qubits with 6 layers\n",
      "Step 250: Loss = -8.413714401642336 for 10 qubits with 6 layers\n",
      "Step 260: Loss = -8.41371440211603 for 10 qubits with 6 layers\n",
      "Step 270: Loss = -8.41371440221109 for 10 qubits with 6 layers\n",
      "Step 280: Loss = -8.41371440222244 for 10 qubits with 6 layers\n",
      "Step 290: Loss = -8.413714402249198 for 10 qubits with 6 layers\n",
      "Step 300: Loss = -8.413714402247166 for 10 qubits with 6 layers\n",
      "Step 310: Loss = -8.413714402251127 for 10 qubits with 6 layers\n",
      "Step 320: Loss = -8.413714402250868 for 10 qubits with 6 layers\n",
      "Step 330: Loss = -8.41371440225137 for 10 qubits with 6 layers\n",
      "Step 340: Loss = -8.413714402251445 for 10 qubits with 6 layers\n",
      "Step 350: Loss = -8.413714402251388 for 10 qubits with 6 layers\n",
      "Step 360: Loss = -8.413714402251419 for 10 qubits with 6 layers\n",
      "Step 370: Loss = -8.413714402251037 for 10 qubits with 6 layers\n",
      "Step 380: Loss = -8.413714384084686 for 10 qubits with 6 layers\n",
      "Step 390: Loss = -8.401975234645533 for 10 qubits with 6 layers\n",
      "Step 400: Loss = -8.408916557330938 for 10 qubits with 6 layers\n",
      "Step 410: Loss = -8.411815449474837 for 10 qubits with 6 layers\n",
      "Step 420: Loss = -8.410385870321273 for 10 qubits with 6 layers\n",
      "Step 430: Loss = -8.41272168038025 for 10 qubits with 6 layers\n",
      "Step 440: Loss = -8.413208973175637 for 10 qubits with 6 layers\n",
      "Step 450: Loss = -8.41366931453028 for 10 qubits with 6 layers\n",
      "Step 460: Loss = -8.413625581494781 for 10 qubits with 6 layers\n",
      "Step 470: Loss = -8.413011262355795 for 10 qubits with 6 layers\n",
      "Step 480: Loss = -8.413575377049751 for 10 qubits with 6 layers\n",
      "Step 490: Loss = -8.4136864647288 for 10 qubits with 6 layers\n",
      "Running optimization for 10 qubits and 7 layers\n",
      "Step 0: Loss = -1.8360693190647244 for 10 qubits with 7 layers\n",
      "Step 10: Loss = -5.694678036047116 for 10 qubits with 7 layers\n",
      "Step 20: Loss = -6.878086405525194 for 10 qubits with 7 layers\n",
      "Step 30: Loss = -8.04995913137144 for 10 qubits with 7 layers\n",
      "Step 40: Loss = -8.607211963000386 for 10 qubits with 7 layers\n",
      "Step 50: Loss = -8.586413509810333 for 10 qubits with 7 layers\n",
      "Step 60: Loss = -8.681652429262249 for 10 qubits with 7 layers\n",
      "Step 70: Loss = -8.691105300852602 for 10 qubits with 7 layers\n",
      "Step 80: Loss = -8.699270546747396 for 10 qubits with 7 layers\n",
      "Step 90: Loss = -8.69906499946207 for 10 qubits with 7 layers\n",
      "Step 100: Loss = -8.700574358994594 for 10 qubits with 7 layers\n",
      "Step 110: Loss = -8.700823462288673 for 10 qubits with 7 layers\n",
      "Step 120: Loss = -8.700987530957171 for 10 qubits with 7 layers\n",
      "Step 130: Loss = -8.701023801227418 for 10 qubits with 7 layers\n",
      "Step 140: Loss = -8.701050708218453 for 10 qubits with 7 layers\n",
      "Step 150: Loss = -8.701067271589263 for 10 qubits with 7 layers\n",
      "Step 160: Loss = -8.701076581303065 for 10 qubits with 7 layers\n",
      "Step 170: Loss = -8.667898642422657 for 10 qubits with 7 layers\n",
      "Step 180: Loss = -8.693962723551191 for 10 qubits with 7 layers\n",
      "Step 190: Loss = -8.696757761183916 for 10 qubits with 7 layers\n",
      "Step 200: Loss = -8.700093108873462 for 10 qubits with 7 layers\n",
      "Step 210: Loss = -8.700550533412468 for 10 qubits with 7 layers\n",
      "Step 220: Loss = -8.701117417388032 for 10 qubits with 7 layers\n",
      "Step 230: Loss = -8.701132929443423 for 10 qubits with 7 layers\n",
      "Step 240: Loss = -8.701183790710292 for 10 qubits with 7 layers\n",
      "Step 250: Loss = -8.701036271511798 for 10 qubits with 7 layers\n",
      "Step 260: Loss = -8.6868433166508 for 10 qubits with 7 layers\n",
      "Step 270: Loss = -8.699974035637753 for 10 qubits with 7 layers\n",
      "Step 280: Loss = -8.70080807669158 for 10 qubits with 7 layers\n",
      "Step 290: Loss = -8.700990092788205 for 10 qubits with 7 layers\n",
      "Step 300: Loss = -8.701140786942693 for 10 qubits with 7 layers\n",
      "Step 310: Loss = -8.700918011774275 for 10 qubits with 7 layers\n",
      "Step 320: Loss = -8.693034405213766 for 10 qubits with 7 layers\n",
      "Step 330: Loss = -8.699326286523526 for 10 qubits with 7 layers\n",
      "Step 340: Loss = -8.699755680141367 for 10 qubits with 7 layers\n",
      "Step 350: Loss = -8.700261020810636 for 10 qubits with 7 layers\n",
      "Step 360: Loss = -8.700604443737054 for 10 qubits with 7 layers\n",
      "Step 370: Loss = -8.701558561803434 for 10 qubits with 7 layers\n",
      "Step 380: Loss = -8.670768861535244 for 10 qubits with 7 layers\n",
      "Step 390: Loss = -8.696792300346631 for 10 qubits with 7 layers\n",
      "Step 400: Loss = -8.69786924461199 for 10 qubits with 7 layers\n",
      "Step 410: Loss = -8.700313723469398 for 10 qubits with 7 layers\n",
      "Step 420: Loss = -8.701738675764815 for 10 qubits with 7 layers\n",
      "Step 430: Loss = -8.701958514045051 for 10 qubits with 7 layers\n",
      "Step 440: Loss = -8.701888098533638 for 10 qubits with 7 layers\n",
      "Step 450: Loss = -8.701947373833228 for 10 qubits with 7 layers\n",
      "Step 460: Loss = -8.69394238613052 for 10 qubits with 7 layers\n",
      "Step 470: Loss = -8.700840070800965 for 10 qubits with 7 layers\n",
      "Step 480: Loss = -8.702416849998324 for 10 qubits with 7 layers\n",
      "Step 490: Loss = -8.702453212381368 for 10 qubits with 7 layers\n",
      "Running optimization for 10 qubits and 8 layers\n",
      "Step 0: Loss = -1.9901344063488795 for 10 qubits with 8 layers\n",
      "Step 10: Loss = -7.598079509770912 for 10 qubits with 8 layers\n",
      "Step 20: Loss = -8.040024719872523 for 10 qubits with 8 layers\n",
      "Step 30: Loss = -8.337981112072507 for 10 qubits with 8 layers\n",
      "Step 40: Loss = -8.52407355135174 for 10 qubits with 8 layers\n",
      "Step 50: Loss = -8.634037511131833 for 10 qubits with 8 layers\n",
      "Step 60: Loss = -8.671597617760543 for 10 qubits with 8 layers\n",
      "Step 70: Loss = -8.692911817208577 for 10 qubits with 8 layers\n",
      "Step 80: Loss = -8.768067702784236 for 10 qubits with 8 layers\n",
      "Step 90: Loss = -8.859391622303136 for 10 qubits with 8 layers\n",
      "Step 100: Loss = -8.889483866067081 for 10 qubits with 8 layers\n",
      "Step 110: Loss = -8.895933132935884 for 10 qubits with 8 layers\n",
      "Step 120: Loss = -8.898986253385683 for 10 qubits with 8 layers\n",
      "Step 130: Loss = -8.900805644825995 for 10 qubits with 8 layers\n",
      "Step 140: Loss = -8.900797578055073 for 10 qubits with 8 layers\n",
      "Step 150: Loss = -8.901030852969392 for 10 qubits with 8 layers\n",
      "Step 160: Loss = -8.901041866924336 for 10 qubits with 8 layers\n",
      "Step 170: Loss = -8.90106892249278 for 10 qubits with 8 layers\n",
      "Step 180: Loss = -8.901073310626657 for 10 qubits with 8 layers\n",
      "Step 190: Loss = -8.90107720710939 for 10 qubits with 8 layers\n",
      "Step 200: Loss = -8.901077037020103 for 10 qubits with 8 layers\n",
      "Step 210: Loss = -8.90107761883178 for 10 qubits with 8 layers\n",
      "Step 220: Loss = -8.90107765271911 for 10 qubits with 8 layers\n",
      "Step 230: Loss = -8.901077713672281 for 10 qubits with 8 layers\n",
      "Step 240: Loss = -8.90107770906954 for 10 qubits with 8 layers\n",
      "Step 250: Loss = -8.901077717890063 for 10 qubits with 8 layers\n",
      "Step 260: Loss = -8.901077719408399 for 10 qubits with 8 layers\n",
      "Step 270: Loss = -8.90107665309287 for 10 qubits with 8 layers\n",
      "Step 280: Loss = -8.867345075721916 for 10 qubits with 8 layers\n",
      "Step 290: Loss = -8.899804394640357 for 10 qubits with 8 layers\n",
      "Step 300: Loss = -8.900029355838102 for 10 qubits with 8 layers\n",
      "Step 310: Loss = -8.899279446745334 for 10 qubits with 8 layers\n",
      "Step 320: Loss = -8.901076860370827 for 10 qubits with 8 layers\n",
      "Step 330: Loss = -8.90097586519127 for 10 qubits with 8 layers\n",
      "Step 340: Loss = -8.901062334005859 for 10 qubits with 8 layers\n",
      "Step 350: Loss = -8.901047004369648 for 10 qubits with 8 layers\n",
      "Step 360: Loss = -8.901072251329106 for 10 qubits with 8 layers\n",
      "Step 370: Loss = -8.901068045375542 for 10 qubits with 8 layers\n",
      "Step 380: Loss = -8.901071415331216 for 10 qubits with 8 layers\n",
      "Step 390: Loss = -8.898251454072124 for 10 qubits with 8 layers\n",
      "Step 400: Loss = -8.900710185976964 for 10 qubits with 8 layers\n",
      "Step 410: Loss = -8.89875852525908 for 10 qubits with 8 layers\n",
      "Step 420: Loss = -8.899181188286706 for 10 qubits with 8 layers\n",
      "Step 430: Loss = -8.90104401143943 for 10 qubits with 8 layers\n",
      "Step 440: Loss = -8.900923242061932 for 10 qubits with 8 layers\n",
      "Step 450: Loss = -8.90106990205986 for 10 qubits with 8 layers\n",
      "Step 460: Loss = -8.901069167946837 for 10 qubits with 8 layers\n",
      "Step 470: Loss = -8.900383448851242 for 10 qubits with 8 layers\n",
      "Step 480: Loss = -8.88451686964327 for 10 qubits with 8 layers\n",
      "Step 490: Loss = -8.899984561172632 for 10 qubits with 8 layers\n",
      "Running optimization for 10 qubits and 9 layers\n",
      "Step 0: Loss = -2.8030923530045966 for 10 qubits with 9 layers\n",
      "Step 10: Loss = -7.392283477384333 for 10 qubits with 9 layers\n",
      "Step 20: Loss = -8.126118354763626 for 10 qubits with 9 layers\n",
      "Step 30: Loss = -8.281169067271165 for 10 qubits with 9 layers\n",
      "Step 40: Loss = -8.559725596097875 for 10 qubits with 9 layers\n",
      "Step 50: Loss = -8.773837271653463 for 10 qubits with 9 layers\n",
      "Step 60: Loss = -8.783090920590706 for 10 qubits with 9 layers\n",
      "Step 70: Loss = -8.80498426708293 for 10 qubits with 9 layers\n",
      "Step 80: Loss = -8.810966054966007 for 10 qubits with 9 layers\n",
      "Step 90: Loss = -8.81886281083862 for 10 qubits with 9 layers\n",
      "Step 100: Loss = -8.82610910440436 for 10 qubits with 9 layers\n",
      "Step 110: Loss = -8.829292032483979 for 10 qubits with 9 layers\n",
      "Step 120: Loss = -8.829362658942896 for 10 qubits with 9 layers\n",
      "Step 130: Loss = -8.829598465030516 for 10 qubits with 9 layers\n",
      "Step 140: Loss = -8.829682670523633 for 10 qubits with 9 layers\n",
      "Step 150: Loss = -8.829729449336105 for 10 qubits with 9 layers\n",
      "Step 160: Loss = -8.829744978929359 for 10 qubits with 9 layers\n",
      "Step 170: Loss = -8.829360637192798 for 10 qubits with 9 layers\n",
      "Step 180: Loss = -8.814977357993326 for 10 qubits with 9 layers\n",
      "Step 190: Loss = -8.829496665374762 for 10 qubits with 9 layers\n",
      "Step 200: Loss = -8.826425890273406 for 10 qubits with 9 layers\n",
      "Step 210: Loss = -8.828579961254851 for 10 qubits with 9 layers\n",
      "Step 220: Loss = -8.829263427302124 for 10 qubits with 9 layers\n",
      "Step 230: Loss = -8.829819227248594 for 10 qubits with 9 layers\n",
      "Step 240: Loss = -8.829813726859808 for 10 qubits with 9 layers\n",
      "Step 250: Loss = -8.829793603230081 for 10 qubits with 9 layers\n",
      "Step 260: Loss = -8.829745254028047 for 10 qubits with 9 layers\n",
      "Step 270: Loss = -8.800343878829574 for 10 qubits with 9 layers\n",
      "Step 280: Loss = -8.825134599465592 for 10 qubits with 9 layers\n",
      "Step 290: Loss = -8.827880320196973 for 10 qubits with 9 layers\n",
      "Step 300: Loss = -8.828394108477681 for 10 qubits with 9 layers\n",
      "Step 310: Loss = -8.829187749139919 for 10 qubits with 9 layers\n",
      "Step 320: Loss = -8.829885687022909 for 10 qubits with 9 layers\n",
      "Step 330: Loss = -8.828694487842291 for 10 qubits with 9 layers\n",
      "Step 340: Loss = -8.827950590975355 for 10 qubits with 9 layers\n",
      "Step 350: Loss = -8.825352429609714 for 10 qubits with 9 layers\n",
      "Step 360: Loss = -8.82989515880692 for 10 qubits with 9 layers\n",
      "Step 370: Loss = -8.829905910671187 for 10 qubits with 9 layers\n",
      "Step 380: Loss = -8.82095106629835 for 10 qubits with 9 layers\n",
      "Step 390: Loss = -8.824562103634744 for 10 qubits with 9 layers\n",
      "Step 400: Loss = -8.827594226147047 for 10 qubits with 9 layers\n",
      "Step 410: Loss = -8.828590799354625 for 10 qubits with 9 layers\n",
      "Step 420: Loss = -8.827568224734764 for 10 qubits with 9 layers\n",
      "Step 430: Loss = -8.828822806695362 for 10 qubits with 9 layers\n",
      "Step 440: Loss = -8.824533189482365 for 10 qubits with 9 layers\n",
      "Step 450: Loss = -8.827428040021971 for 10 qubits with 9 layers\n",
      "Step 460: Loss = -8.829977294896311 for 10 qubits with 9 layers\n",
      "Step 470: Loss = -8.828079470333293 for 10 qubits with 9 layers\n",
      "Step 480: Loss = -8.82462974604955 for 10 qubits with 9 layers\n",
      "Step 490: Loss = -8.826314277973934 for 10 qubits with 9 layers\n",
      "Running optimization for 10 qubits and 10 layers\n",
      "Step 0: Loss = -2.4086559502386438 for 10 qubits with 10 layers\n",
      "Step 10: Loss = -8.261598447708163 for 10 qubits with 10 layers\n",
      "Step 20: Loss = -8.739418384876263 for 10 qubits with 10 layers\n",
      "Step 30: Loss = -8.851020219226049 for 10 qubits with 10 layers\n",
      "Step 40: Loss = -8.964051891292257 for 10 qubits with 10 layers\n",
      "Step 50: Loss = -8.977687308556039 for 10 qubits with 10 layers\n",
      "Step 60: Loss = -8.995498404744293 for 10 qubits with 10 layers\n",
      "Step 70: Loss = -8.999158249261217 for 10 qubits with 10 layers\n",
      "Step 80: Loss = -8.999256104120391 for 10 qubits with 10 layers\n",
      "Step 90: Loss = -8.999623446298735 for 10 qubits with 10 layers\n",
      "Step 100: Loss = -8.999831430858821 for 10 qubits with 10 layers\n",
      "Step 110: Loss = -8.999905094846271 for 10 qubits with 10 layers\n",
      "Step 120: Loss = -8.99994068244484 for 10 qubits with 10 layers\n",
      "Step 130: Loss = -8.999959330059756 for 10 qubits with 10 layers\n",
      "Step 140: Loss = -8.999969011431 for 10 qubits with 10 layers\n",
      "Step 150: Loss = -8.999977569546939 for 10 qubits with 10 layers\n",
      "Step 160: Loss = -8.999983982425366 for 10 qubits with 10 layers\n",
      "Step 170: Loss = -8.999972244097451 for 10 qubits with 10 layers\n",
      "Step 180: Loss = -8.999863668504789 for 10 qubits with 10 layers\n",
      "Step 190: Loss = -8.983690911837156 for 10 qubits with 10 layers\n",
      "Step 200: Loss = -8.99469991744411 for 10 qubits with 10 layers\n",
      "Step 210: Loss = -8.99974012450888 for 10 qubits with 10 layers\n",
      "Step 220: Loss = -8.999230513666921 for 10 qubits with 10 layers\n",
      "Step 230: Loss = -8.9999184231348 for 10 qubits with 10 layers\n",
      "Step 240: Loss = -8.999925847639055 for 10 qubits with 10 layers\n",
      "Step 250: Loss = -8.999968505328635 for 10 qubits with 10 layers\n",
      "Step 260: Loss = -8.999980249002688 for 10 qubits with 10 layers\n",
      "Step 270: Loss = -8.999994106341418 for 10 qubits with 10 layers\n",
      "Step 280: Loss = -8.999949827767322 for 10 qubits with 10 layers\n",
      "Step 290: Loss = -8.966874401054698 for 10 qubits with 10 layers\n",
      "Step 300: Loss = -8.995473589168268 for 10 qubits with 10 layers\n",
      "Step 310: Loss = -8.995524334992579 for 10 qubits with 10 layers\n",
      "Step 320: Loss = -8.999644027477654 for 10 qubits with 10 layers\n",
      "Step 330: Loss = -8.999328999358761 for 10 qubits with 10 layers\n",
      "Step 340: Loss = -8.999830141334959 for 10 qubits with 10 layers\n",
      "Step 350: Loss = -8.999756708687348 for 10 qubits with 10 layers\n",
      "Step 360: Loss = -8.99795051442368 for 10 qubits with 10 layers\n",
      "Step 370: Loss = -8.991190495819199 for 10 qubits with 10 layers\n",
      "Step 380: Loss = -8.995135017095604 for 10 qubits with 10 layers\n",
      "Step 390: Loss = -8.99793241948307 for 10 qubits with 10 layers\n",
      "Step 400: Loss = -8.998577833092725 for 10 qubits with 10 layers\n",
      "Step 410: Loss = -8.998854179614783 for 10 qubits with 10 layers\n",
      "Step 420: Loss = -8.998753067717955 for 10 qubits with 10 layers\n",
      "Step 430: Loss = -8.981299835937591 for 10 qubits with 10 layers\n",
      "Step 440: Loss = -8.986066121362605 for 10 qubits with 10 layers\n",
      "Step 450: Loss = -8.995646303652478 for 10 qubits with 10 layers\n",
      "Step 460: Loss = -8.99416882460385 for 10 qubits with 10 layers\n",
      "Step 470: Loss = -8.999271853494491 for 10 qubits with 10 layers\n",
      "Step 480: Loss = -8.999472454227933 for 10 qubits with 10 layers\n",
      "Step 490: Loss = -8.999582372121175 for 10 qubits with 10 layers\n",
      "Running optimization for 10 qubits and 11 layers\n",
      "Step 0: Loss = -4.0859710917062735 for 10 qubits with 11 layers\n",
      "Step 10: Loss = -8.343668441957007 for 10 qubits with 11 layers\n",
      "Step 20: Loss = -8.811415687557325 for 10 qubits with 11 layers\n",
      "Step 30: Loss = -8.861176935810459 for 10 qubits with 11 layers\n",
      "Step 40: Loss = -8.913006433828427 for 10 qubits with 11 layers\n",
      "Step 50: Loss = -8.937772721628548 for 10 qubits with 11 layers\n",
      "Step 60: Loss = -8.950563257979136 for 10 qubits with 11 layers\n",
      "Step 70: Loss = -8.95348279395009 for 10 qubits with 11 layers\n",
      "Step 80: Loss = -8.95612656931377 for 10 qubits with 11 layers\n",
      "Step 90: Loss = -8.957105364939135 for 10 qubits with 11 layers\n",
      "Step 100: Loss = -8.957856795327217 for 10 qubits with 11 layers\n",
      "Step 110: Loss = -8.958652566061552 for 10 qubits with 11 layers\n",
      "Step 120: Loss = -8.959515470699076 for 10 qubits with 11 layers\n",
      "Step 130: Loss = -8.960387263969473 for 10 qubits with 11 layers\n",
      "Step 140: Loss = -8.961178654826096 for 10 qubits with 11 layers\n",
      "Step 150: Loss = -8.961833789021465 for 10 qubits with 11 layers\n",
      "Step 160: Loss = -8.962215425564002 for 10 qubits with 11 layers\n",
      "Step 170: Loss = -8.961613314919909 for 10 qubits with 11 layers\n",
      "Step 180: Loss = -8.963563935299744 for 10 qubits with 11 layers\n",
      "Step 190: Loss = -8.964000242377637 for 10 qubits with 11 layers\n",
      "Step 200: Loss = -8.965763830795378 for 10 qubits with 11 layers\n",
      "Step 210: Loss = -8.968080854361576 for 10 qubits with 11 layers\n",
      "Step 220: Loss = -8.971016902905962 for 10 qubits with 11 layers\n",
      "Step 230: Loss = -8.975249383653402 for 10 qubits with 11 layers\n",
      "Step 240: Loss = -8.980653730047155 for 10 qubits with 11 layers\n",
      "Step 250: Loss = -8.98632779596733 for 10 qubits with 11 layers\n",
      "Step 260: Loss = -8.979667593896025 for 10 qubits with 11 layers\n",
      "Step 270: Loss = -8.993616569034767 for 10 qubits with 11 layers\n",
      "Step 280: Loss = -8.992894805516595 for 10 qubits with 11 layers\n",
      "Step 290: Loss = -8.995990043028618 for 10 qubits with 11 layers\n",
      "Step 300: Loss = -8.998545638406316 for 10 qubits with 11 layers\n",
      "Step 310: Loss = -8.998938888155806 for 10 qubits with 11 layers\n",
      "Step 320: Loss = -8.999536530119837 for 10 qubits with 11 layers\n",
      "Step 330: Loss = -8.999712134151448 for 10 qubits with 11 layers\n",
      "Step 340: Loss = -8.996703587342632 for 10 qubits with 11 layers\n",
      "Step 350: Loss = -8.9854903405334 for 10 qubits with 11 layers\n",
      "Step 360: Loss = -8.99859320586659 for 10 qubits with 11 layers\n",
      "Step 370: Loss = -8.999900102555511 for 10 qubits with 11 layers\n",
      "Step 380: Loss = -8.999231164876814 for 10 qubits with 11 layers\n",
      "Step 390: Loss = -8.99994365295841 for 10 qubits with 11 layers\n",
      "Step 400: Loss = -8.9960238916223 for 10 qubits with 11 layers\n",
      "Step 410: Loss = -8.991815148195078 for 10 qubits with 11 layers\n",
      "Step 420: Loss = -8.99964905699878 for 10 qubits with 11 layers\n",
      "Step 430: Loss = -8.99771214683374 for 10 qubits with 11 layers\n",
      "Step 440: Loss = -8.997904257385025 for 10 qubits with 11 layers\n",
      "Step 450: Loss = -8.996871355201137 for 10 qubits with 11 layers\n",
      "Step 460: Loss = -8.997686432193772 for 10 qubits with 11 layers\n",
      "Step 470: Loss = -8.992711637694567 for 10 qubits with 11 layers\n",
      "Step 480: Loss = -8.993682822581123 for 10 qubits with 11 layers\n",
      "Step 490: Loss = -8.997549631233563 for 10 qubits with 11 layers\n",
      "Running optimization for 10 qubits and 12 layers\n",
      "Step 0: Loss = -4.725053949718802 for 10 qubits with 12 layers\n",
      "Step 10: Loss = -8.019063139331259 for 10 qubits with 12 layers\n",
      "Step 20: Loss = -8.604423751459404 for 10 qubits with 12 layers\n",
      "Step 30: Loss = -8.810621996804514 for 10 qubits with 12 layers\n",
      "Step 40: Loss = -8.934186587777353 for 10 qubits with 12 layers\n",
      "Step 50: Loss = -8.975654723942048 for 10 qubits with 12 layers\n",
      "Step 60: Loss = -8.990716053632678 for 10 qubits with 12 layers\n",
      "Step 70: Loss = -8.996500742357393 for 10 qubits with 12 layers\n",
      "Step 80: Loss = -8.99860651038088 for 10 qubits with 12 layers\n",
      "Step 90: Loss = -8.999746162520195 for 10 qubits with 12 layers\n",
      "Step 100: Loss = -8.999821110335768 for 10 qubits with 12 layers\n",
      "Step 110: Loss = -8.999939199613738 for 10 qubits with 12 layers\n",
      "Step 120: Loss = -8.999984379861491 for 10 qubits with 12 layers\n",
      "Step 130: Loss = -8.999992726832096 for 10 qubits with 12 layers\n",
      "Step 140: Loss = -8.999997779930343 for 10 qubits with 12 layers\n",
      "Step 150: Loss = -8.999998986107187 for 10 qubits with 12 layers\n",
      "Step 160: Loss = -8.999999734303266 for 10 qubits with 12 layers\n",
      "Step 170: Loss = -8.999999893774458 for 10 qubits with 12 layers\n",
      "Step 180: Loss = -8.999999958575817 for 10 qubits with 12 layers\n",
      "Step 190: Loss = -8.999999985741256 for 10 qubits with 12 layers\n",
      "Step 200: Loss = -8.999999993505556 for 10 qubits with 12 layers\n",
      "Step 210: Loss = -8.999999998240202 for 10 qubits with 12 layers\n",
      "Step 220: Loss = -8.999999997386563 for 10 qubits with 12 layers\n",
      "Step 230: Loss = -8.999998864467658 for 10 qubits with 12 layers\n",
      "Step 240: Loss = -8.92229096719673 for 10 qubits with 12 layers\n",
      "Step 250: Loss = -8.957966313066164 for 10 qubits with 12 layers\n",
      "Step 260: Loss = -8.99535371936338 for 10 qubits with 12 layers\n",
      "Step 270: Loss = -8.999502539859233 for 10 qubits with 12 layers\n",
      "Step 280: Loss = -8.997799640651305 for 10 qubits with 12 layers\n",
      "Step 290: Loss = -8.999784660398172 for 10 qubits with 12 layers\n",
      "Step 300: Loss = -8.999963501332934 for 10 qubits with 12 layers\n",
      "Step 310: Loss = -8.999880271305148 for 10 qubits with 12 layers\n",
      "Step 320: Loss = -8.999996891266026 for 10 qubits with 12 layers\n",
      "Step 330: Loss = -8.999986780267689 for 10 qubits with 12 layers\n",
      "Step 340: Loss = -8.999998165276295 for 10 qubits with 12 layers\n",
      "Step 350: Loss = -8.999871265786798 for 10 qubits with 12 layers\n",
      "Step 360: Loss = -8.96491031720948 for 10 qubits with 12 layers\n",
      "Step 370: Loss = -8.978048879495663 for 10 qubits with 12 layers\n",
      "Step 380: Loss = -8.991948939176613 for 10 qubits with 12 layers\n",
      "Step 390: Loss = -8.99690635702743 for 10 qubits with 12 layers\n",
      "Step 400: Loss = -8.99990127391722 for 10 qubits with 12 layers\n",
      "Step 410: Loss = -8.999411562886337 for 10 qubits with 12 layers\n",
      "Step 420: Loss = -8.99263996743182 for 10 qubits with 12 layers\n",
      "Step 430: Loss = -8.999021086631537 for 10 qubits with 12 layers\n",
      "Step 440: Loss = -8.999670162557141 for 10 qubits with 12 layers\n",
      "Step 450: Loss = -8.982956971601439 for 10 qubits with 12 layers\n",
      "Step 460: Loss = -8.996196451883115 for 10 qubits with 12 layers\n",
      "Step 470: Loss = -8.9971727206306 for 10 qubits with 12 layers\n",
      "Step 480: Loss = -8.997297030971795 for 10 qubits with 12 layers\n",
      "Step 490: Loss = -8.998768986671143 for 10 qubits with 12 layers\n",
      "Running optimization for 10 qubits and 13 layers\n",
      "Step 0: Loss = -3.4837389925653874 for 10 qubits with 13 layers\n",
      "Step 10: Loss = -8.273336376053102 for 10 qubits with 13 layers\n",
      "Step 20: Loss = -8.695279301159646 for 10 qubits with 13 layers\n",
      "Step 30: Loss = -8.879824813338802 for 10 qubits with 13 layers\n",
      "Step 40: Loss = -8.947371672738578 for 10 qubits with 13 layers\n",
      "Step 50: Loss = -8.984937606986655 for 10 qubits with 13 layers\n",
      "Step 60: Loss = -8.995397453236041 for 10 qubits with 13 layers\n",
      "Step 70: Loss = -8.997566374587954 for 10 qubits with 13 layers\n",
      "Step 80: Loss = -8.999648892710633 for 10 qubits with 13 layers\n",
      "Step 90: Loss = -8.99972774938896 for 10 qubits with 13 layers\n",
      "Step 100: Loss = -8.999897581214098 for 10 qubits with 13 layers\n",
      "Step 110: Loss = -8.99995961979857 for 10 qubits with 13 layers\n",
      "Step 120: Loss = -8.999987382840436 for 10 qubits with 13 layers\n",
      "Step 130: Loss = -8.999996565287613 for 10 qubits with 13 layers\n",
      "Step 140: Loss = -8.999998400652057 for 10 qubits with 13 layers\n",
      "Step 150: Loss = -8.99999943237174 for 10 qubits with 13 layers\n",
      "Step 160: Loss = -8.999999771793664 for 10 qubits with 13 layers\n",
      "Step 170: Loss = -8.99999991600251 for 10 qubits with 13 layers\n",
      "Step 180: Loss = -8.999999979497815 for 10 qubits with 13 layers\n",
      "Step 190: Loss = -8.999999987622614 for 10 qubits with 13 layers\n",
      "Step 200: Loss = -8.999999996052152 for 10 qubits with 13 layers\n",
      "Step 210: Loss = -8.99999999819352 for 10 qubits with 13 layers\n",
      "Step 220: Loss = -8.999999999338511 for 10 qubits with 13 layers\n",
      "Step 230: Loss = -8.999999999784874 for 10 qubits with 13 layers\n",
      "Step 240: Loss = -8.999999998790281 for 10 qubits with 13 layers\n",
      "Step 250: Loss = -8.999988321156732 for 10 qubits with 13 layers\n",
      "Step 260: Loss = -8.992755495811933 for 10 qubits with 13 layers\n",
      "Step 270: Loss = -8.959794776321031 for 10 qubits with 13 layers\n",
      "Step 280: Loss = -8.98592206816945 for 10 qubits with 13 layers\n",
      "Step 290: Loss = -8.995316717636825 for 10 qubits with 13 layers\n",
      "Step 300: Loss = -8.999520522368082 for 10 qubits with 13 layers\n",
      "Step 310: Loss = -8.999562009679483 for 10 qubits with 13 layers\n",
      "Step 320: Loss = -8.999995409444269 for 10 qubits with 13 layers\n",
      "Step 330: Loss = -8.999998667369344 for 10 qubits with 13 layers\n",
      "Step 340: Loss = -8.999962441384374 for 10 qubits with 13 layers\n",
      "Step 350: Loss = -8.999996423098155 for 10 qubits with 13 layers\n",
      "Step 360: Loss = -8.993046359490062 for 10 qubits with 13 layers\n",
      "Step 370: Loss = -8.989003836269385 for 10 qubits with 13 layers\n",
      "Step 380: Loss = -8.996299991908106 for 10 qubits with 13 layers\n",
      "Step 390: Loss = -8.996664875792836 for 10 qubits with 13 layers\n",
      "Step 400: Loss = -8.992013257576598 for 10 qubits with 13 layers\n",
      "Step 410: Loss = -8.995592131363834 for 10 qubits with 13 layers\n",
      "Step 420: Loss = -8.998201405042956 for 10 qubits with 13 layers\n",
      "Step 430: Loss = -8.999894619872357 for 10 qubits with 13 layers\n",
      "Step 440: Loss = -8.999956385188117 for 10 qubits with 13 layers\n",
      "Step 450: Loss = -8.990354173472573 for 10 qubits with 13 layers\n",
      "Step 460: Loss = -8.989196838797662 for 10 qubits with 13 layers\n",
      "Step 470: Loss = -8.99944322665025 for 10 qubits with 13 layers\n",
      "Step 480: Loss = -8.99529003325458 for 10 qubits with 13 layers\n",
      "Step 490: Loss = -8.994861182504756 for 10 qubits with 13 layers\n",
      "Running optimization for 10 qubits and 14 layers\n",
      "Step 0: Loss = -3.147549310439835 for 10 qubits with 14 layers\n",
      "Step 10: Loss = -7.965134306698708 for 10 qubits with 14 layers\n",
      "Step 20: Loss = -8.586001020936312 for 10 qubits with 14 layers\n",
      "Step 30: Loss = -8.84407681729152 for 10 qubits with 14 layers\n",
      "Step 40: Loss = -8.95937786622461 for 10 qubits with 14 layers\n",
      "Step 50: Loss = -8.986028796910988 for 10 qubits with 14 layers\n",
      "Step 60: Loss = -8.993837303800841 for 10 qubits with 14 layers\n",
      "Step 70: Loss = -8.998310075451721 for 10 qubits with 14 layers\n",
      "Step 80: Loss = -8.99912955127575 for 10 qubits with 14 layers\n",
      "Step 90: Loss = -8.999698939856088 for 10 qubits with 14 layers\n",
      "Step 100: Loss = -8.999918042168822 for 10 qubits with 14 layers\n",
      "Step 110: Loss = -8.999968221235996 for 10 qubits with 14 layers\n",
      "Step 120: Loss = -8.999990671230314 for 10 qubits with 14 layers\n",
      "Step 130: Loss = -8.99999422552404 for 10 qubits with 14 layers\n",
      "Step 140: Loss = -8.999998656604694 for 10 qubits with 14 layers\n",
      "Step 150: Loss = -8.99999947558015 for 10 qubits with 14 layers\n",
      "Step 160: Loss = -8.999999740687826 for 10 qubits with 14 layers\n",
      "Step 170: Loss = -8.999999850883274 for 10 qubits with 14 layers\n",
      "Step 180: Loss = -8.999996652274332 for 10 qubits with 14 layers\n",
      "Step 190: Loss = -8.94233566035662 for 10 qubits with 14 layers\n",
      "Step 200: Loss = -8.985566914626688 for 10 qubits with 14 layers\n",
      "Step 210: Loss = -8.99273847407154 for 10 qubits with 14 layers\n",
      "Step 220: Loss = -8.998506270932344 for 10 qubits with 14 layers\n",
      "Step 230: Loss = -8.999685048802327 for 10 qubits with 14 layers\n",
      "Step 240: Loss = -8.999864748621535 for 10 qubits with 14 layers\n",
      "Step 250: Loss = -8.999997994079921 for 10 qubits with 14 layers\n",
      "Step 260: Loss = -8.999968765410596 for 10 qubits with 14 layers\n",
      "Step 270: Loss = -8.997468203147431 for 10 qubits with 14 layers\n",
      "Step 280: Loss = -8.988949386833578 for 10 qubits with 14 layers\n",
      "Step 290: Loss = -8.995213116551685 for 10 qubits with 14 layers\n",
      "Step 300: Loss = -8.998959676503778 for 10 qubits with 14 layers\n",
      "Step 310: Loss = -8.999694159599526 for 10 qubits with 14 layers\n",
      "Step 320: Loss = -8.978633557419913 for 10 qubits with 14 layers\n",
      "Step 330: Loss = -8.99541588691263 for 10 qubits with 14 layers\n",
      "Step 340: Loss = -8.997495883038757 for 10 qubits with 14 layers\n",
      "Step 350: Loss = -8.998138696101806 for 10 qubits with 14 layers\n",
      "Step 360: Loss = -8.988812108534038 for 10 qubits with 14 layers\n",
      "Step 370: Loss = -8.99402189205114 for 10 qubits with 14 layers\n",
      "Step 380: Loss = -8.996220427790744 for 10 qubits with 14 layers\n",
      "Step 390: Loss = -8.990507127181395 for 10 qubits with 14 layers\n",
      "Step 400: Loss = -8.995228864071056 for 10 qubits with 14 layers\n",
      "Step 410: Loss = -8.988795858723046 for 10 qubits with 14 layers\n",
      "Step 420: Loss = -8.98726461143649 for 10 qubits with 14 layers\n",
      "Step 430: Loss = -8.994820115363522 for 10 qubits with 14 layers\n",
      "Step 440: Loss = -8.998995975718229 for 10 qubits with 14 layers\n",
      "Step 450: Loss = -8.98638445742475 for 10 qubits with 14 layers\n",
      "Step 460: Loss = -8.9942619195431 for 10 qubits with 14 layers\n",
      "Step 470: Loss = -8.995815982657824 for 10 qubits with 14 layers\n",
      "Step 480: Loss = -8.997756151738212 for 10 qubits with 14 layers\n",
      "Step 490: Loss = -8.994281530074506 for 10 qubits with 14 layers\n",
      "Running optimization for 10 qubits and 15 layers\n",
      "Step 0: Loss = -4.424652379409175 for 10 qubits with 15 layers\n",
      "Step 10: Loss = -7.66523207157907 for 10 qubits with 15 layers\n",
      "Step 20: Loss = -8.646190442500398 for 10 qubits with 15 layers\n",
      "Step 30: Loss = -8.886153078927961 for 10 qubits with 15 layers\n",
      "Step 40: Loss = -8.94719506619137 for 10 qubits with 15 layers\n",
      "Step 50: Loss = -8.973419127832294 for 10 qubits with 15 layers\n",
      "Step 60: Loss = -8.990670828263617 for 10 qubits with 15 layers\n",
      "Step 70: Loss = -8.995594168796698 for 10 qubits with 15 layers\n",
      "Step 80: Loss = -8.998433786004915 for 10 qubits with 15 layers\n",
      "Step 90: Loss = -8.999451536652955 for 10 qubits with 15 layers\n",
      "Step 100: Loss = -8.999873193932773 for 10 qubits with 15 layers\n",
      "Step 110: Loss = -8.999963553649607 for 10 qubits with 15 layers\n",
      "Step 120: Loss = -8.999979907201087 for 10 qubits with 15 layers\n",
      "Step 130: Loss = -8.999989100463806 for 10 qubits with 15 layers\n",
      "Step 140: Loss = -8.999996651010518 for 10 qubits with 15 layers\n",
      "Step 150: Loss = -8.999999100794875 for 10 qubits with 15 layers\n",
      "Step 160: Loss = -8.999999697121606 for 10 qubits with 15 layers\n",
      "Step 170: Loss = -8.999999899867603 for 10 qubits with 15 layers\n",
      "Step 180: Loss = -8.999999950950558 for 10 qubits with 15 layers\n",
      "Step 190: Loss = -8.999999985582534 for 10 qubits with 15 layers\n",
      "Step 200: Loss = -8.999999993485083 for 10 qubits with 15 layers\n",
      "Step 210: Loss = -8.999999997336708 for 10 qubits with 15 layers\n",
      "Step 220: Loss = -8.999999999096676 for 10 qubits with 15 layers\n",
      "Step 230: Loss = -8.99999999974881 for 10 qubits with 15 layers\n",
      "Step 240: Loss = -8.999999999835545 for 10 qubits with 15 layers\n",
      "Step 250: Loss = -8.999999999893127 for 10 qubits with 15 layers\n",
      "Step 260: Loss = -8.999999998290004 for 10 qubits with 15 layers\n",
      "Step 270: Loss = -8.999969156623333 for 10 qubits with 15 layers\n",
      "Step 280: Loss = -8.956338276198075 for 10 qubits with 15 layers\n",
      "Step 290: Loss = -8.984857157231678 for 10 qubits with 15 layers\n",
      "Step 300: Loss = -8.994387625425391 for 10 qubits with 15 layers\n",
      "Step 310: Loss = -8.996082610392738 for 10 qubits with 15 layers\n",
      "Step 320: Loss = -8.999747066182126 for 10 qubits with 15 layers\n",
      "Step 330: Loss = -8.999633789413796 for 10 qubits with 15 layers\n",
      "Step 340: Loss = -8.999778462019115 for 10 qubits with 15 layers\n",
      "Step 350: Loss = -8.999929703830984 for 10 qubits with 15 layers\n",
      "Step 360: Loss = -8.99999591995286 for 10 qubits with 15 layers\n",
      "Step 370: Loss = -8.999987585487528 for 10 qubits with 15 layers\n",
      "Step 380: Loss = -8.999996179884995 for 10 qubits with 15 layers\n",
      "Step 390: Loss = -8.999997962970427 for 10 qubits with 15 layers\n",
      "Step 400: Loss = -8.999999902570396 for 10 qubits with 15 layers\n",
      "Step 410: Loss = -8.999999989807458 for 10 qubits with 15 layers\n",
      "Step 420: Loss = -8.99999963896856 for 10 qubits with 15 layers\n",
      "Step 430: Loss = -8.999284907737326 for 10 qubits with 15 layers\n",
      "Step 440: Loss = -8.98750874154558 for 10 qubits with 15 layers\n",
      "Step 450: Loss = -8.993904784021211 for 10 qubits with 15 layers\n",
      "Step 460: Loss = -8.995142211434146 for 10 qubits with 15 layers\n",
      "Step 470: Loss = -8.996539234732213 for 10 qubits with 15 layers\n",
      "Step 480: Loss = -8.99009922637042 for 10 qubits with 15 layers\n",
      "Step 490: Loss = -8.996456629002951 for 10 qubits with 15 layers\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "def run_optimization(num_qubits, P):\n",
    "    edges = [(i, j) for i in range(num_qubits) for j in range(i + 1, num_qubits)]\n",
    "    nodes = range(num_qubits)\n",
    "\n",
    "    dev = qml.device('default.qubit', wires=num_qubits)\n",
    "\n",
    "    # Define the Hamiltonians\n",
    "    h1 = qml.Hamiltonian([1.0 for _ in edges], [qml.PauliZ(i) @ qml.PauliZ(j) for (i, j) in edges])\n",
    "    h2 = qml.Hamiltonian([1.0 for _ in nodes], [qml.PauliX(i) for i in nodes])\n",
    "    stab1 = qml.Hamiltonian([1], [qml.operation.Tensor(*(qml.PauliX(i) for i in nodes))])\n",
    "    stab2 = qml.Hamiltonian([1 for _ in range(len(nodes) - 1)], [qml.PauliZ(i) @ qml.PauliZ(i + 1) for i in range(len(nodes) - 1)])\n",
    "    stab = stab1 + stab2\n",
    "\n",
    "    @qml.qnode(dev)\n",
    "    def circuit(params):\n",
    "        for p in range(P):\n",
    "            qml.templates.ApproxTimeEvolution(h1, params[2 * p], 1)\n",
    "            qml.templates.ApproxTimeEvolution(h2, params[2 * p + 1], 1)\n",
    "        return qml.expval(stab)\n",
    "\n",
    "    params = np.random.random(P * 2)  # 2 parameters per layer\n",
    "\n",
    "    def loss(params):\n",
    "        return -circuit(params)\n",
    "\n",
    "    opt = qml.AdamOptimizer(stepsize=0.01)\n",
    "    for i in range(500):\n",
    "        params, val = opt.step_and_cost(loss, params)\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Step {i}: Loss = {val} for {num_qubits} qubits with {P} layers\")\n",
    "\n",
    "\n",
    "for n in range(2, 11):\n",
    "    for p in range(1, 16):\n",
    "        print(f\"Running optimization for {n} qubits and {p} layers\")\n",
    "        run_optimization(n, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimization for 7 qubits and 20 layers\n",
      "Step 0: Stabilizer Expectation = 1.4831751890270886, Loss = -1.0014782386148213 for 7 qubits with 20 layers\n",
      "Step 10: Stabilizer Expectation = 3.850780358518238, Loss = -3.758420810946242 for 7 qubits with 20 layers\n",
      "Step 20: Stabilizer Expectation = 4.461119572445193, Loss = -4.411874890918706 for 7 qubits with 20 layers\n",
      "Step 30: Stabilizer Expectation = 4.64959234673835, Loss = -4.6305342205170215 for 7 qubits with 20 layers\n",
      "Step 40: Stabilizer Expectation = 4.807619141329301, Loss = -4.795363411923552 for 7 qubits with 20 layers\n",
      "Step 50: Stabilizer Expectation = 4.865062604786247, Loss = -4.862841954410565 for 7 qubits with 20 layers\n",
      "Step 60: Stabilizer Expectation = 4.881174830732158, Loss = -4.87913128760042 for 7 qubits with 20 layers\n",
      "Step 70: Stabilizer Expectation = 4.90019433651732, Loss = -4.898258620049857 for 7 qubits with 20 layers\n",
      "Step 80: Stabilizer Expectation = 4.919321574689619, Loss = -4.917574377447392 for 7 qubits with 20 layers\n",
      "Step 90: Stabilizer Expectation = 4.936827381988069, Loss = -4.935129083195768 for 7 qubits with 20 layers\n",
      "Step 100: Stabilizer Expectation = 4.952619581440535, Loss = -4.951086742939213 for 7 qubits with 20 layers\n",
      "Step 110: Stabilizer Expectation = 4.967153849996765, Loss = -4.965772859791164 for 7 qubits with 20 layers\n",
      "Step 120: Stabilizer Expectation = 4.979981035792271, Loss = -4.978774361706641 for 7 qubits with 20 layers\n",
      "Step 130: Stabilizer Expectation = 4.991220590562081, Loss = -4.990152817009075 for 7 qubits with 20 layers\n",
      "Step 140: Stabilizer Expectation = 5.001383490290399, Loss = -5.00040237514602 for 7 qubits with 20 layers\n",
      "Step 150: Stabilizer Expectation = 5.0107808244584024, Loss = -5.009875510733903 for 7 qubits with 20 layers\n",
      "Step 160: Stabilizer Expectation = 5.019443388177959, Loss = -5.018605786512812 for 7 qubits with 20 layers\n",
      "Step 170: Stabilizer Expectation = 5.027607232401686, Loss = -5.026797318774725 for 7 qubits with 20 layers\n",
      "Step 180: Stabilizer Expectation = 5.036145441217561, Loss = -5.035226442237968 for 7 qubits with 20 layers\n",
      "Step 190: Stabilizer Expectation = 5.046804382956413, Loss = -5.045600305199024 for 7 qubits with 20 layers\n",
      "Step 200: Stabilizer Expectation = 5.060496975604132, Loss = -5.059014768611198 for 7 qubits with 20 layers\n",
      "Step 210: Stabilizer Expectation = 5.075694010300243, Loss = -5.07419350336763 for 7 qubits with 20 layers\n",
      "Step 220: Stabilizer Expectation = 5.0891608011038745, Loss = -5.088000029337954 for 7 qubits with 20 layers\n",
      "Step 230: Stabilizer Expectation = 5.0975316714213665, Loss = -5.0969512773634955 for 7 qubits with 20 layers\n",
      "Step 240: Stabilizer Expectation = 5.101404317858981, Loss = -5.1011204496065945 for 7 qubits with 20 layers\n",
      "Step 250: Stabilizer Expectation = 5.103861922006248, Loss = -5.103624158364913 for 7 qubits with 20 layers\n",
      "Step 260: Stabilizer Expectation = 5.106271981296096, Loss = -5.106034922398812 for 7 qubits with 20 layers\n",
      "Step 270: Stabilizer Expectation = 5.10445080800688, Loss = -5.1046464844711 for 7 qubits with 20 layers\n",
      "Step 280: Stabilizer Expectation = 5.109937286803156, Loss = -5.10871567314458 for 7 qubits with 20 layers\n",
      "Step 290: Stabilizer Expectation = 5.1115984211150165, Loss = -5.111811862229025 for 7 qubits with 20 layers\n",
      "Step 300: Stabilizer Expectation = 5.113440425249271, Loss = -5.113171419483495 for 7 qubits with 20 layers\n",
      "Step 310: Stabilizer Expectation = 5.114961417181937, Loss = -5.114772045861662 for 7 qubits with 20 layers\n",
      "Step 320: Stabilizer Expectation = 5.11634737387829, Loss = -5.116192142217068 for 7 qubits with 20 layers\n",
      "Step 330: Stabilizer Expectation = 5.117669002266918, Loss = -5.117536223228056 for 7 qubits with 20 layers\n",
      "Step 340: Stabilizer Expectation = 5.118956511428072, Loss = -5.1188285464766805 for 7 qubits with 20 layers\n",
      "Step 350: Stabilizer Expectation = 5.120190274737719, Loss = -5.120067726427161 for 7 qubits with 20 layers\n",
      "Step 360: Stabilizer Expectation = 5.121383541921002, Loss = -5.121266212570439 for 7 qubits with 20 layers\n",
      "Step 370: Stabilizer Expectation = 5.122541577666426, Loss = -5.122427316770692 for 7 qubits with 20 layers\n",
      "Step 380: Stabilizer Expectation = 5.123565007304412, Loss = -5.123512206813932 for 7 qubits with 20 layers\n",
      "Step 390: Stabilizer Expectation = 5.122587224940051, Loss = -5.122926298519263 for 7 qubits with 20 layers\n",
      "Step 400: Stabilizer Expectation = 5.125088598513125, Loss = -5.124781899219441 for 7 qubits with 20 layers\n",
      "Step 410: Stabilizer Expectation = 5.126095580624441, Loss = -5.126044273857155 for 7 qubits with 20 layers\n",
      "Step 420: Stabilizer Expectation = 5.127014658768521, Loss = -5.126993248376626 for 7 qubits with 20 layers\n",
      "Step 430: Stabilizer Expectation = 5.127981721838527, Loss = -5.127870449600272 for 7 qubits with 20 layers\n",
      "Step 440: Stabilizer Expectation = 5.128940596588338, Loss = -5.1288350255398445 for 7 qubits with 20 layers\n",
      "Step 450: Stabilizer Expectation = 5.129850149482253, Loss = -5.129864224531582 for 7 qubits with 20 layers\n",
      "Step 460: Stabilizer Expectation = 5.1307704714137135, Loss = -5.129852988809695 for 7 qubits with 20 layers\n",
      "Step 470: Stabilizer Expectation = 5.133241582025462, Loss = -5.132436771687674 for 7 qubits with 20 layers\n",
      "Step 480: Stabilizer Expectation = 5.136170870428035, Loss = -5.135892537595107 for 7 qubits with 20 layers\n",
      "Step 490: Stabilizer Expectation = 5.140242939300912, Loss = -5.1398101537812 for 7 qubits with 20 layers\n",
      "Step 500: Stabilizer Expectation = 5.144673318860055, Loss = -5.14422333975774 for 7 qubits with 20 layers\n",
      "Step 510: Stabilizer Expectation = 5.150030083140814, Loss = -5.1494161401888405 for 7 qubits with 20 layers\n",
      "Step 520: Stabilizer Expectation = 5.15989350472374, Loss = -5.159105993744936 for 7 qubits with 20 layers\n",
      "Step 530: Stabilizer Expectation = 5.197165046992619, Loss = -5.190355692814922 for 7 qubits with 20 layers\n",
      "Step 540: Stabilizer Expectation = 5.317766269049391, Loss = -5.3050121117826 for 7 qubits with 20 layers\n",
      "Step 550: Stabilizer Expectation = 5.427683316664195, Loss = -5.413201825237829 for 7 qubits with 20 layers\n",
      "Step 560: Stabilizer Expectation = 5.481624685473685, Loss = -5.478787731588758 for 7 qubits with 20 layers\n",
      "Step 570: Stabilizer Expectation = 5.504991234103018, Loss = -5.504219480250019 for 7 qubits with 20 layers\n",
      "Step 580: Stabilizer Expectation = 5.508162656886024, Loss = -5.507888942238134 for 7 qubits with 20 layers\n",
      "Step 590: Stabilizer Expectation = 5.510194964270172, Loss = -5.510022905735276 for 7 qubits with 20 layers\n",
      "Step 600: Stabilizer Expectation = 5.51116597752221, Loss = -5.511278344146275 for 7 qubits with 20 layers\n",
      "Step 610: Stabilizer Expectation = 5.510828536062025, Loss = -5.508687090276314 for 7 qubits with 20 layers\n",
      "Step 620: Stabilizer Expectation = 5.511376439564731, Loss = -5.511646717777938 for 7 qubits with 20 layers\n",
      "Step 630: Stabilizer Expectation = 5.512209070646816, Loss = -5.512495538177523 for 7 qubits with 20 layers\n",
      "Step 640: Stabilizer Expectation = 5.512658152081528, Loss = -5.512526176139459 for 7 qubits with 20 layers\n",
      "Step 650: Stabilizer Expectation = 5.512757990311182, Loss = -5.512746961976971 for 7 qubits with 20 layers\n",
      "Step 660: Stabilizer Expectation = 5.512819957238733, Loss = -5.512798520486539 for 7 qubits with 20 layers\n",
      "Step 670: Stabilizer Expectation = 5.511956436100781, Loss = -5.5115737958156865 for 7 qubits with 20 layers\n",
      "Step 680: Stabilizer Expectation = 5.512873949800919, Loss = -5.512574195020018 for 7 qubits with 20 layers\n",
      "Step 690: Stabilizer Expectation = 5.507910268199592, Loss = -5.508439110159286 for 7 qubits with 20 layers\n",
      "Step 700: Stabilizer Expectation = 5.512832374798208, Loss = -5.51173595275237 for 7 qubits with 20 layers\n",
      "Step 710: Stabilizer Expectation = 5.512730161377783, Loss = -5.51252441507736 for 7 qubits with 20 layers\n",
      "Step 720: Stabilizer Expectation = 5.51282534232178, Loss = -5.512963767921449 for 7 qubits with 20 layers\n",
      "Step 730: Stabilizer Expectation = 5.513054389468012, Loss = -5.512990817672761 for 7 qubits with 20 layers\n",
      "Step 740: Stabilizer Expectation = 5.51308479350979, Loss = -5.513067718011966 for 7 qubits with 20 layers\n",
      "Step 750: Stabilizer Expectation = 5.513048176842245, Loss = -5.5130611481206735 for 7 qubits with 20 layers\n",
      "Step 760: Stabilizer Expectation = 5.512625332071445, Loss = -5.509189598676718 for 7 qubits with 20 layers\n",
      "Step 770: Stabilizer Expectation = 5.511521554102137, Loss = -5.512469721002207 for 7 qubits with 20 layers\n",
      "Step 780: Stabilizer Expectation = 5.512947351083291, Loss = -5.513083656129861 for 7 qubits with 20 layers\n",
      "Step 790: Stabilizer Expectation = 5.512978488874021, Loss = -5.512868944693503 for 7 qubits with 20 layers\n",
      "Step 800: Stabilizer Expectation = 5.513136318135193, Loss = -5.513069768314781 for 7 qubits with 20 layers\n",
      "Step 810: Stabilizer Expectation = 5.513110468960045, Loss = -5.513094459462054 for 7 qubits with 20 layers\n",
      "Step 820: Stabilizer Expectation = 5.513136169250532, Loss = -5.5131276110600265 for 7 qubits with 20 layers\n",
      "Step 830: Stabilizer Expectation = 5.5110426262913705, Loss = -5.512294197110721 for 7 qubits with 20 layers\n",
      "Step 840: Stabilizer Expectation = 5.513077359996377, Loss = -5.510678931189188 for 7 qubits with 20 layers\n",
      "Step 850: Stabilizer Expectation = 5.512382154733913, Loss = -5.51286785340847 for 7 qubits with 20 layers\n",
      "Step 860: Stabilizer Expectation = 5.512921979215083, Loss = -5.513053507835448 for 7 qubits with 20 layers\n",
      "Step 870: Stabilizer Expectation = 5.513089139503236, Loss = -5.513082959454291 for 7 qubits with 20 layers\n",
      "Step 880: Stabilizer Expectation = 5.513126795077485, Loss = -5.513110348891077 for 7 qubits with 20 layers\n",
      "Step 890: Stabilizer Expectation = 5.512686505753099, Loss = -5.51284578812767 for 7 qubits with 20 layers\n",
      "Step 900: Stabilizer Expectation = 5.512184885528676, Loss = -5.512868158680098 for 7 qubits with 20 layers\n",
      "Step 910: Stabilizer Expectation = 5.513134779609324, Loss = -5.5131088213633435 for 7 qubits with 20 layers\n",
      "Step 920: Stabilizer Expectation = 5.512923636226896, Loss = -5.512857844000464 for 7 qubits with 20 layers\n",
      "Step 930: Stabilizer Expectation = 5.513014785001537, Loss = -5.5129746205725105 for 7 qubits with 20 layers\n",
      "Step 940: Stabilizer Expectation = 5.513172293759266, Loss = -5.513173091329328 for 7 qubits with 20 layers\n",
      "Step 950: Stabilizer Expectation = 5.5081900739418455, Loss = -5.51090999806352 for 7 qubits with 20 layers\n",
      "Step 960: Stabilizer Expectation = 5.51261681948018, Loss = -5.509786933346659 for 7 qubits with 20 layers\n",
      "Step 970: Stabilizer Expectation = 5.513109534804983, Loss = -5.512537853200144 for 7 qubits with 20 layers\n",
      "Step 980: Stabilizer Expectation = 5.512975929663856, Loss = -5.513128378588801 for 7 qubits with 20 layers\n",
      "Step 990: Stabilizer Expectation = 5.513119399253126, Loss = -5.513156284250597 for 7 qubits with 20 layers\n",
      "Running optimization for 7 qubits and 50 layers\n",
      "Step 0: Stabilizer Expectation = 1.7761091805850584, Loss = -0.8337722793889084 for 7 qubits with 50 layers\n",
      "Step 10: Stabilizer Expectation = 4.446018580670423, Loss = -4.35142640504573 for 7 qubits with 50 layers\n",
      "Step 20: Stabilizer Expectation = 5.0009810806110515, Loss = -4.940260225435581 for 7 qubits with 50 layers\n",
      "Step 30: Stabilizer Expectation = 5.385412358726982, Loss = -5.364495511899452 for 7 qubits with 50 layers\n",
      "Step 40: Stabilizer Expectation = 5.5489639117880225, Loss = -5.5338446979175036 for 7 qubits with 50 layers\n",
      "Step 50: Stabilizer Expectation = 5.698352136139538, Loss = -5.684786431040869 for 7 qubits with 50 layers\n",
      "Step 60: Stabilizer Expectation = 5.770148747035516, Loss = -5.766157778318952 for 7 qubits with 50 layers\n",
      "Step 70: Stabilizer Expectation = 5.823645913341038, Loss = -5.817818178011725 for 7 qubits with 50 layers\n",
      "Step 80: Stabilizer Expectation = 5.877712526495887, Loss = -5.872886479792297 for 7 qubits with 50 layers\n",
      "Step 90: Stabilizer Expectation = 5.913876870188016, Loss = -5.910946433842776 for 7 qubits with 50 layers\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "def run_optimization(num_qubits, P,num_iterations):\n",
    "    edges = [(0, 1), (1, 2), (2, 0),  # First triangle (1-2-3)\n",
    "             (3, 4), (4, 5), (5, 3),  # Second triangle (4-5-6)\n",
    "             (2, 6), (3, 6)]          # Connecting edges (3-7 and 4-7)\n",
    "    \n",
    "    dev = qml.device('default.qubit', wires=num_qubits)\n",
    "\n",
    "    \n",
    "    h1 = qml.Hamiltonian([1.0 for _ in edges], [qml.PauliZ(i) @ qml.PauliZ(j) for (i, j) in edges])\n",
    "    h2 = qml.Hamiltonian([1.0 for _ in range(num_qubits)], [qml.PauliX(i) for i in range(num_qubits)])\n",
    "    stab1 = qml.Hamiltonian([1], [qml.operation.Tensor(*(qml.PauliX(i) for i in nodes))])\n",
    "    stab2 = qml.Hamiltonian([1 for _ in range(len(nodes) - 1)], [qml.PauliZ(i) @ qml.PauliZ(i + 1) for i in range(len(nodes) - 1)])\n",
    "    stab = stab1 + stab2\n",
    "    \n",
    "    @qml.qnode(dev)\n",
    "    def circuit(params):\n",
    "        for p in range(P):\n",
    "            qml.templates.ApproxTimeEvolution(h1, params[2 * p], 1)\n",
    "            qml.templates.ApproxTimeEvolution(h2, params[2 * p + 1], 1)\n",
    "        return qml.expval(stab)\n",
    "\n",
    "    params = np.random.random(2 * P)  # 2 parameters per layer\n",
    "\n",
    "    def loss(params):\n",
    "        return -circuit(params)\n",
    "\n",
    "    opt = qml.AdamOptimizer(stepsize=0.01)\n",
    "    for i in range(num_iterations):\n",
    "        params, val = opt.step_and_cost(loss, params)\n",
    "        if i % 10 == 0:\n",
    "            final_expval = circuit(params)  \n",
    "            print(f\"Step {i}: Stabilizer Expectation = {final_expval}, Loss = {val} for {num_qubits} qubits with {P} layers\")\n",
    "\n",
    "\n",
    "num_qubits = 7\n",
    "print(f\"Running optimization for {num_qubits} qubits and 20 layers\")\n",
    "run_optimization(num_qubits, 20,1000)\n",
    "print(f\"Running optimization for {num_qubits} qubits and 50 layers\")\n",
    "run_optimization(num_qubits, 50,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimization for 7 qubits and 20 layers\n",
      "Step 0: Fidelity = 0.02745370337888798, Loss = -0.8935973089106568 for 7 qubits with 20 layers\n",
      "Step 10: Fidelity = 0.31247400117703455, Loss = -4.2332585719751314 for 7 qubits with 20 layers\n",
      "Step 20: Fidelity = 0.35901585344841375, Loss = -4.9933157114114515 for 7 qubits with 20 layers\n",
      "Step 30: Fidelity = 0.4107105141519517, Loss = -5.341289350441483 for 7 qubits with 20 layers\n",
      "Step 40: Fidelity = 0.43301109815089156, Loss = -5.51911972859743 for 7 qubits with 20 layers\n",
      "Step 50: Fidelity = 0.44736429147820606, Loss = -5.644716769367432 for 7 qubits with 20 layers\n",
      "Step 60: Fidelity = 0.46131828923381774, Loss = -5.760456997475771 for 7 qubits with 20 layers\n",
      "Step 70: Fidelity = 0.4781571703617105, Loss = -5.853127231046866 for 7 qubits with 20 layers\n",
      "Step 80: Fidelity = 0.48789061060303657, Loss = -5.909701963228739 for 7 qubits with 20 layers\n",
      "Step 90: Fidelity = 0.4926924982920004, Loss = -5.936588742783313 for 7 qubits with 20 layers\n",
      "Step 100: Fidelity = 0.49466413248051366, Loss = -5.950835289083544 for 7 qubits with 20 layers\n",
      "Step 110: Fidelity = 0.4958179059467351, Loss = -5.960343941402662 for 7 qubits with 20 layers\n",
      "Step 120: Fidelity = 0.4965824961847831, Loss = -5.967406847132414 for 7 qubits with 20 layers\n",
      "Step 130: Fidelity = 0.497095988377524, Loss = -5.972748124891042 for 7 qubits with 20 layers\n",
      "Step 140: Fidelity = 0.4975085180578285, Loss = -5.976818744128792 for 7 qubits with 20 layers\n",
      "Step 150: Fidelity = 0.4978330279868296, Loss = -5.979972939552083 for 7 qubits with 20 layers\n",
      "Step 160: Fidelity = 0.49808260663630954, Loss = -5.982482766353019 for 7 qubits with 20 layers\n",
      "Step 170: Fidelity = 0.49829125920054007, Loss = -5.984538997639038 for 7 qubits with 20 layers\n",
      "Step 180: Fidelity = 0.4984680323716845, Loss = -5.986272351736665 for 7 qubits with 20 layers\n",
      "Step 190: Fidelity = 0.4986218425690565, Loss = -5.987768978389434 for 7 qubits with 20 layers\n",
      "Step 200: Fidelity = 0.49875932561004704, Loss = -5.989085874857842 for 7 qubits with 20 layers\n",
      "Step 210: Fidelity = 0.4988839401475595, Loss = -5.990260223577051 for 7 qubits with 20 layers\n",
      "Step 220: Fidelity = 0.49899765201070256, Loss = -5.991316190388462 for 7 qubits with 20 layers\n",
      "Step 230: Fidelity = 0.49910155508582876, Loss = -5.992269597414421 for 7 qubits with 20 layers\n",
      "Step 240: Fidelity = 0.49919652520746854, Loss = -5.99313109217958 for 7 qubits with 20 layers\n",
      "Step 250: Fidelity = 0.4992831414495473, Loss = -5.99390828132843 for 7 qubits with 20 layers\n",
      "Step 260: Fidelity = 0.4993629659766329, Loss = -5.994603824308028 for 7 qubits with 20 layers\n",
      "Step 270: Fidelity = 0.498725675902359, Loss = -5.986416165348123 for 7 qubits with 20 layers\n",
      "Step 280: Fidelity = 0.4991497357979663, Loss = -5.995294311289407 for 7 qubits with 20 layers\n",
      "Step 290: Fidelity = 0.49940485243558974, Loss = -5.996029938547765 for 7 qubits with 20 layers\n",
      "Step 300: Fidelity = 0.4995409047875935, Loss = -5.996420101784722 for 7 qubits with 20 layers\n",
      "Step 310: Fidelity = 0.4996136974317056, Loss = -5.996610687430264 for 7 qubits with 20 layers\n",
      "Step 320: Fidelity = 0.4996454422854058, Loss = -5.997100520366962 for 7 qubits with 20 layers\n",
      "Step 330: Fidelity = 0.49967997710581114, Loss = -5.997378574882802 for 7 qubits with 20 layers\n",
      "Step 340: Fidelity = 0.4997090820084394, Loss = -5.997644531381888 for 7 qubits with 20 layers\n",
      "Step 350: Fidelity = 0.49973800453545686, Loss = -5.997874545230372 for 7 qubits with 20 layers\n",
      "Step 360: Fidelity = 0.49976339380316526, Loss = -5.9980850346307095 for 7 qubits with 20 layers\n",
      "Step 370: Fidelity = 0.4997844872365374, Loss = -5.998268307635479 for 7 qubits with 20 layers\n",
      "Step 380: Fidelity = 0.49943316334197196, Loss = -5.996659114338372 for 7 qubits with 20 layers\n",
      "Step 390: Fidelity = 0.49965556794332144, Loss = -5.996135120427222 for 7 qubits with 20 layers\n",
      "Step 400: Fidelity = 0.4997637799833177, Loss = -5.997965653524039 for 7 qubits with 20 layers\n",
      "Step 410: Fidelity = 0.4998104051036246, Loss = -5.998703436661936 for 7 qubits with 20 layers\n",
      "Step 420: Fidelity = 0.4998559070098345, Loss = -5.998794546989822 for 7 qubits with 20 layers\n",
      "Step 430: Fidelity = 0.49970728877888876, Loss = -5.9979112729316 for 7 qubits with 20 layers\n",
      "Step 440: Fidelity = 0.49981876168953676, Loss = -5.997035965379898 for 7 qubits with 20 layers\n",
      "Step 450: Fidelity = 0.49986838317990134, Loss = -5.998334674647216 for 7 qubits with 20 layers\n",
      "Step 460: Fidelity = 0.4998738381226934, Loss = -5.998772548937016 for 7 qubits with 20 layers\n",
      "Step 470: Fidelity = 0.49989555038402383, Loss = -5.99916456022763 for 7 qubits with 20 layers\n",
      "Step 480: Fidelity = 0.4998930305259364, Loss = -5.999107909895598 for 7 qubits with 20 layers\n",
      "Step 490: Fidelity = 0.499897076662706, Loss = -5.998511051639118 for 7 qubits with 20 layers\n",
      "Step 500: Fidelity = 0.49973598986863066, Loss = -5.996274188474586 for 7 qubits with 20 layers\n",
      "Step 510: Fidelity = 0.4999222989691685, Loss = -5.998959778652217 for 7 qubits with 20 layers\n",
      "Step 520: Fidelity = 0.499914389464103, Loss = -5.999277752306218 for 7 qubits with 20 layers\n",
      "Step 530: Fidelity = 0.499923631376496, Loss = -5.9992627805283 for 7 qubits with 20 layers\n",
      "Step 540: Fidelity = 0.49986420045901936, Loss = -5.9991015347776555 for 7 qubits with 20 layers\n",
      "Step 550: Fidelity = 0.4997743774284772, Loss = -5.996008584712468 for 7 qubits with 20 layers\n",
      "Step 560: Fidelity = 0.49987049604199635, Loss = -5.999491761975215 for 7 qubits with 20 layers\n",
      "Step 570: Fidelity = 0.4999348340645357, Loss = -5.999082293655116 for 7 qubits with 20 layers\n",
      "Step 580: Fidelity = 0.4999398707462606, Loss = -5.9995515808568625 for 7 qubits with 20 layers\n",
      "Step 590: Fidelity = 0.49981567858111253, Loss = -5.998778689105816 for 7 qubits with 20 layers\n",
      "Step 600: Fidelity = 0.4998631251606459, Loss = -5.99836116872352 for 7 qubits with 20 layers\n",
      "Step 610: Fidelity = 0.4999121883776673, Loss = -5.999412139828571 for 7 qubits with 20 layers\n",
      "Step 620: Fidelity = 0.49995409680123876, Loss = -5.999570366222971 for 7 qubits with 20 layers\n",
      "Step 630: Fidelity = 0.4998996454664417, Loss = -5.999118232476178 for 7 qubits with 20 layers\n",
      "Step 640: Fidelity = 0.4998788622331282, Loss = -5.999616677996833 for 7 qubits with 20 layers\n",
      "Step 650: Fidelity = 0.49994903180256717, Loss = -5.999271083211022 for 7 qubits with 20 layers\n",
      "Step 660: Fidelity = 0.49992098438992455, Loss = -5.9993965733900465 for 7 qubits with 20 layers\n",
      "Step 670: Fidelity = 0.49989811253944705, Loss = -5.998571175185671 for 7 qubits with 20 layers\n",
      "Step 680: Fidelity = 0.4998902702477448, Loss = -5.9990114770656415 for 7 qubits with 20 layers\n",
      "Step 690: Fidelity = 0.49994521946504905, Loss = -5.99952919645851 for 7 qubits with 20 layers\n",
      "Step 700: Fidelity = 0.49986415205298423, Loss = -5.997387270617388 for 7 qubits with 20 layers\n",
      "Step 710: Fidelity = 0.4999287786008348, Loss = -5.999117981928595 for 7 qubits with 20 layers\n",
      "Step 720: Fidelity = 0.4999294351575316, Loss = -5.999398264977034 for 7 qubits with 20 layers\n",
      "Step 730: Fidelity = 0.49996467727201127, Loss = -5.999749288888351 for 7 qubits with 20 layers\n",
      "Step 740: Fidelity = 0.49969453606307074, Loss = -5.998127356596805 for 7 qubits with 20 layers\n",
      "Step 750: Fidelity = 0.49980778751928273, Loss = -5.9994327909732466 for 7 qubits with 20 layers\n",
      "Step 760: Fidelity = 0.49996042607204305, Loss = -5.998989624707353 for 7 qubits with 20 layers\n",
      "Step 770: Fidelity = 0.4999486999053017, Loss = -5.999658692637364 for 7 qubits with 20 layers\n",
      "Step 780: Fidelity = 0.4999648646266388, Loss = -5.999620739276616 for 7 qubits with 20 layers\n",
      "Step 790: Fidelity = 0.4999532069516231, Loss = -5.998466875949464 for 7 qubits with 20 layers\n",
      "Step 800: Fidelity = 0.49992102662873766, Loss = -5.999135615528067 for 7 qubits with 20 layers\n",
      "Step 810: Fidelity = 0.4999596234199341, Loss = -5.999743147051898 for 7 qubits with 20 layers\n",
      "Step 820: Fidelity = 0.4999676809956909, Loss = -5.999764438191247 for 7 qubits with 20 layers\n",
      "Step 830: Fidelity = 0.4998631329747996, Loss = -5.998992382389789 for 7 qubits with 20 layers\n",
      "Step 840: Fidelity = 0.499928815627973, Loss = -5.999487151947111 for 7 qubits with 20 layers\n",
      "Step 850: Fidelity = 0.4999443594412234, Loss = -5.99968578925539 for 7 qubits with 20 layers\n",
      "Step 860: Fidelity = 0.4999398596965464, Loss = -5.999417210874316 for 7 qubits with 20 layers\n",
      "Step 870: Fidelity = 0.4999469279779341, Loss = -5.99891421956933 for 7 qubits with 20 layers\n",
      "Step 880: Fidelity = 0.4999435335858826, Loss = -5.999017000184729 for 7 qubits with 20 layers\n",
      "Step 890: Fidelity = 0.49990139680368334, Loss = -5.9992681525392895 for 7 qubits with 20 layers\n",
      "Step 900: Fidelity = 0.4998972815310622, Loss = -5.999052042337215 for 7 qubits with 20 layers\n",
      "Step 910: Fidelity = 0.4999838241660054, Loss = -5.999795921422028 for 7 qubits with 20 layers\n",
      "Step 920: Fidelity = 0.4996775995324892, Loss = -5.997734908980237 for 7 qubits with 20 layers\n",
      "Step 930: Fidelity = 0.4999663116757088, Loss = -5.999494485534348 for 7 qubits with 20 layers\n",
      "Step 940: Fidelity = 0.4999814328836849, Loss = -5.999704233210748 for 7 qubits with 20 layers\n",
      "Step 950: Fidelity = 0.4999779550649898, Loss = -5.9997095101772375 for 7 qubits with 20 layers\n",
      "Step 960: Fidelity = 0.499953400585943, Loss = -5.9980625393341525 for 7 qubits with 20 layers\n",
      "Step 970: Fidelity = 0.49991621573074735, Loss = -5.999599430141716 for 7 qubits with 20 layers\n",
      "Step 980: Fidelity = 0.49998942167537375, Loss = -5.999720734737328 for 7 qubits with 20 layers\n",
      "Step 990: Fidelity = 0.49996331692741636, Loss = -5.99982837130135 for 7 qubits with 20 layers\n",
      "Running optimization for 7 qubits and 50 layers\n",
      "Step 0: Fidelity = 0.007757241808115629, Loss = -0.41048002493862673 for 7 qubits with 50 layers\n",
      "Step 10: Fidelity = 0.15593125374368688, Loss = -3.6172486480748436 for 7 qubits with 50 layers\n",
      "Step 20: Fidelity = 0.23221574262070055, Loss = -4.514673722910028 for 7 qubits with 50 layers\n",
      "Step 30: Fidelity = 0.3680520485600388, Loss = -5.055036934146442 for 7 qubits with 50 layers\n",
      "Step 40: Fidelity = 0.4401411274773263, Loss = -5.462328894376738 for 7 qubits with 50 layers\n",
      "Step 50: Fidelity = 0.46583033618272995, Loss = -5.632035466224076 for 7 qubits with 50 layers\n",
      "Step 60: Fidelity = 0.47141443014981427, Loss = -5.715962976477627 for 7 qubits with 50 layers\n",
      "Step 70: Fidelity = 0.47688662128666454, Loss = -5.777108110846551 for 7 qubits with 50 layers\n",
      "Step 80: Fidelity = 0.4800477342088975, Loss = -5.832169220664351 for 7 qubits with 50 layers\n",
      "Step 90: Fidelity = 0.48190421075660056, Loss = -5.867154594816366 for 7 qubits with 50 layers\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "def ghz_state(num_qubits):\n",
    "    \"\"\"Create a GHZ state for the given number of qubits.\"\"\"\n",
    "    ghz = np.zeros(2**num_qubits)\n",
    "    ghz[0] = 1 / np.sqrt(2)\n",
    "    ghz[-1] = 1 / np.sqrt(2)\n",
    "    return ghz\n",
    "\n",
    "def fidelity(state1, state2):\n",
    "    \"\"\"Calculate the fidelity between two states.\"\"\"\n",
    "    return np.abs(np.dot(np.conj(state1), state2))**2\n",
    "\n",
    "def run_optimization(num_qubits, P, num_iterations):\n",
    "    edges = [(0, 1), (1, 2), (2, 0),  # First triangle (1-2-3)\n",
    "             (3, 4), (4, 5), (5, 3),  # Second triangle (4-5-6)\n",
    "             (2, 6), (3, 6)]          # Connecting edges (3-7 and 4-7)\n",
    "    \n",
    "    \n",
    "    dev = qml.device('default.qubit', wires=num_qubits)\n",
    "\n",
    "    \n",
    "    h1 = qml.Hamiltonian([1.0 for _ in edges], [qml.PauliZ(i) @ qml.PauliZ(j) for (i, j) in edges])\n",
    "    h2 = qml.Hamiltonian([1.0 for _ in range(num_qubits)], [qml.PauliX(i) for i in range(num_qubits)])\n",
    "    stab1 = qml.Hamiltonian([1], [qml.operation.Tensor(*(qml.PauliX(i) for i in range(num_qubits)))])\n",
    "    stab2 = qml.Hamiltonian([1 for _ in range(num_qubits - 1)], [qml.PauliZ(i) @ qml.PauliZ(i + 1) for i in range(num_qubits - 1)])\n",
    "    stab = stab1 + stab2\n",
    "\n",
    "    @qml.qnode(dev)\n",
    "    def combined_circuit(params):\n",
    "        for p in range(P):\n",
    "            qml.templates.ApproxTimeEvolution(h1, params[2 * p], 1)\n",
    "            qml.templates.ApproxTimeEvolution(h2, params[2 * p + 1], 1)\n",
    "        return qml.state(), qml.expval(stab)\n",
    "\n",
    "    params = np.random.random(2 * P)  # 2 parameters per layer\n",
    "\n",
    "    def loss(params):\n",
    "        state, expectation = combined_circuit(params)\n",
    "        return -expectation\n",
    "\n",
    "    opt = qml.AdamOptimizer(stepsize=0.01)\n",
    "    reference_state = ghz_state(num_qubits)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        params, val = opt.step_and_cost(loss, params)\n",
    "        if i % 10 == 0:\n",
    "            final_state, _ = combined_circuit(params)\n",
    "            fidelity_value = fidelity(reference_state, final_state)\n",
    "            print(f\"Step {i}: Fidelity = {fidelity_value}, Loss = {val} for {num_qubits} qubits with {P} layers\")\n",
    "\n",
    "num_qubits = 7\n",
    "print(f\"Running optimization for {num_qubits} qubits and 20 layers\")\n",
    "run_optimization(num_qubits, 20, 1000)\n",
    "print(f\"Running optimization for {num_qubits} qubits and 50 layers\")\n",
    "run_optimization(num_qubits, 50, 100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

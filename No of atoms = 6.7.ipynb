{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No of atoms = 6,7 (Learning rate decay used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from rdkit import RDLogger\n",
    "from chainer_chemistry import datasets\n",
    "\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_filepath = C:\\Users\\riakh\\.chainer\\dataset\\pfnet/chainer/qm9\\qm9.csv\n"
     ]
    }
   ],
   "source": [
    "dataset_filepath = datasets.get_qm9_filepath()\n",
    "\n",
    "print('dataset_filepath =', dataset_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133885/133885 [00:26<00:00, 5055.44it/s]\n",
      "INFO:chainer_chemistry.dataset.parsers.data_frame_parser:Preprocess finished. FAIL 0, SUCCESS 133885, TOTAL 133885\n"
     ]
    }
   ],
   "source": [
    "from chainer_chemistry.dataset.preprocessors.ggnn_preprocessor import \\\n",
    "    GGNNPreprocessor\n",
    "    \n",
    "preprocessor = GGNNPreprocessor()\n",
    "dataset, dataset_smiles = datasets.get_qm9(preprocessor, labels=None, return_smiles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset information...\n",
      "dataset <class 'chainer_chemistry.datasets.numpy_tuple_dataset.NumpyTupleDataset'> 133885\n",
      "smiles information...\n",
      "dataset_smiles <class 'numpy.ndarray'> 133885\n"
     ]
    }
   ],
   "source": [
    "print('dataset information...')\n",
    "print('dataset', type(dataset), len(dataset))\n",
    "\n",
    "print('smiles information...')\n",
    "print('dataset_smiles', type(dataset_smiles), len(dataset_smiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of atoms in each graph in the data: [7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 7, 7, 6, 7, 7, 7, 6, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 6, 7, 7, 6, 7, 7, 7, 7, 6, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 6, 7, 7, 6, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 6, 7, 6, 7, 7, 7, 7, 6, 6, 7, 6, 6, 7, 6, 7, 6, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 6, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 6, 7, 7, 6, 7, 7, 7, 7, 7, 6, 7, 7, 6, 7, 7, 7, 7, 6, 7, 7, 6, 7, 7, 6, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 6, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 6, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 6, 6, 7, 7, 7, 7, 6, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 6, 6, 7, 7, 6, 7, 7, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 7, 7, 6, 7, 6, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 6, 7, 6, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 6, 7, 6, 7, 6, 6, 7, 6, 6, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 6, 6, 7, 6, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 6, 7, 7, 6, 6, 7, 7, 7, 7, 6, 7, 7, 7, 7, 6, 7, 7, 7, 7, 6, 7, 7, 6, 7, 7, 7, 7, 6, 7, 7, 7, 6, 7, 7, 7, 6, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 6, 7, 7, 6, 6, 6, 6, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 6, 7, 7, 6, 7, 6, 7, 6, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 6, 7, 7, 6, 6, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 6, 7, 6, 7, 7, 7, 6, 7, 7, 6, 6, 7, 6, 6, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 6, 7, 6, 6, 7, 7, 7, 7, 7, 6, 7, 6, 7, 7, 7, 7, 6, 7, 7, 7, 7, 6, 6, 6, 7, 7, 6, 6, 7, 7, 6, 7, 7, 6, 6, 7, 7, 6, 7, 7, 7, 6, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 6, 7, 6, 6, 7, 7, 7, 7, 6, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 6, 7, 7, 6, 7, 6, 7, 7, 6, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 6, 6, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 6, 7, 7, 7, 6, 6, 6, 7, 7, 7, 7, 7, 7, 6, 6, 7, 7, 7, 6, 7, 7, 7, 7, 7, 6, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 6, 6, 7, 6, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 6, 7, 7, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 6, 7, 7, 6, 7, 7, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 7, 7, 7, 6, 6, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 6, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 6, 7, 6, 6, 7, 7, 7, 6, 6, 6, 6, 6, 7, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 6, 7, 7, 7, 7, 6, 7, 6, 6, 6, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 7, 6, 7, 7, 6, 7, 7, 7, 7, 7, 7, 6, 6, 7, 7, 7, 6, 7, 7, 6, 6, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 7, 6, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 6, 7, 7, 7, 6, 7, 7, 6, 7, 7, 6, 6, 6, 7, 6, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 6, 7, 6, 7, 7, 7, 6, 7, 7, 7, 7, 6, 7, 7, 6, 7, 7, 6, 7, 7, 7, 6, 7, 7, 7, 7, 7, 6, 7, 7, 6, 7, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 7, 6, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 6, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 6, 7, 7, 6, 7, 6, 7, 6, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 7, 6, 7, 7, 6, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 6, 7, 7, 6, 6, 6, 7, 6, 7, 7, 6, 6, 7, 7, 6, 7, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 6, 7, 7, 7, 6, 7, 6, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 6, 7, 6, 7, 7, 7, 7, 7, 6, 7, 7, 7, 6, 6, 7, 7, 7, 7, 6, 7, 7, 7, 7, 6, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 6, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 6, 6, 7, 6, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 6, 7, 7, 6, 7, 7, 6, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 6, 6, 7, 7, 6, 7, 6, 7, 7, 7, 7, 7, 7, 7, 6, 6, 7, 6, 6, 7, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 6, 7, 7, 7, 6, 6, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 6, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 6, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 6, 7, 7, 6, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 6, 6, 7, 6, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 6, 6, 7, 7, 6, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 6, 7, 7, 6, 7, 7, 7, 7, 6, 7, 7, 7, 6, 7, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 7, 7, 7, 6, 6, 6, 7, 7, 7, 6, 7, 7, 6, 7, 7, 7, 7, 6, 7, 7, 6, 6, 7, 6, 6, 7, 6, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 6, 7, 6, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 6, 7, 6, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 6, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 7, 6, 7, 7, 6, 7, 7, 7, 7, 6, 7, 6, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\riakh\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.006076067335171891\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from rdkit import Chem\n",
    "import pennylane as qml\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "def smiles_to_graph_and_targets(smiles_list, dataset):\n",
    "    data_list = []\n",
    "    targets = []\n",
    "    for smiles, data in zip(smiles_list, dataset):\n",
    "        properties = data if len(data) > 0 else None\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            continue\n",
    "\n",
    "        atomic_nums = [atom.GetAtomicNum() for atom in mol.GetAtoms()]\n",
    "        aromatics = [atom.GetIsAromatic() for atom in mol.GetAtoms()]\n",
    "        hybridizations = [int(atom.GetHybridization()) for atom in mol.GetAtoms()]\n",
    "        num_hydrogens = [atom.GetTotalNumHs() for atom in mol.GetAtoms()]\n",
    "\n",
    "        x = np.column_stack([atomic_nums, aromatics, hybridizations, num_hydrogens])\n",
    "        edge_index = []\n",
    "        edge_attr = []\n",
    "        bond_info = []\n",
    "\n",
    "        for bond in mol.GetBonds():\n",
    "            start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "            bond_type = bond.GetBondType()\n",
    "            bond_label = {Chem.rdchem.BondType.SINGLE: 'single',\n",
    "                          Chem.rdchem.BondType.DOUBLE: 'double',\n",
    "                          Chem.rdchem.BondType.TRIPLE: 'triple',\n",
    "                          Chem.rdchem.BondType.AROMATIC: 'aromatic'}.get(bond_type, 'single')\n",
    "            bond_info.append((start, end, bond_label))\n",
    "            edge_attr.append([1 if bond_label == bl else 0 for bl in ['single', 'double', 'triple', 'aromatic']])\n",
    "            edge_index.append([start, end])\n",
    "\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t()\n",
    "        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, bond_info=bond_info)\n",
    "        data_list.append(data)\n",
    "\n",
    "        if properties is not None:\n",
    "            target_value = properties[2][7]  # the target is in the 7th column of the third block\n",
    "            targets.append(target_value)\n",
    "\n",
    "    targets = torch.tensor(targets, dtype=torch.float)\n",
    "    return data_list, targets\n",
    "\n",
    "def split_data(graph_data, targets, train_ratio=0.7):\n",
    "    num_train = int(len(graph_data) * train_ratio)\n",
    "    indices = np.random.permutation(len(graph_data))\n",
    "    train_indices = indices[:num_train]\n",
    "    val_indices = indices[num_train:]\n",
    "\n",
    "    train_data = [graph_data[i] for i in train_indices]\n",
    "    train_targets = targets[train_indices]\n",
    "    val_data = [graph_data[i] for i in val_indices]\n",
    "    val_targets = targets[val_indices]\n",
    "\n",
    "    return (train_data, train_targets), (val_data, val_targets)\n",
    "\n",
    "def filter_graphs_by_num_atoms(graph_data, targets, num_atoms=[6, 7]):\n",
    "    filtered_graphs = []\n",
    "    filtered_targets = []\n",
    "    for i, graph in enumerate(graph_data):\n",
    "        if graph.num_nodes in num_atoms:\n",
    "            filtered_graphs.append(graph)\n",
    "            filtered_targets.append(targets[i])\n",
    "    return filtered_graphs, torch.tensor(filtered_targets, dtype=torch.float)\n",
    "\n",
    "def batch_processing(graph_data, targets, batch_size):\n",
    "    num_batches = len(graph_data) // batch_size\n",
    "    for i in range(num_batches):\n",
    "        batch_data = graph_data[i * batch_size: (i + 1) * batch_size]\n",
    "        batch_targets = targets[i * batch_size: (i + 1) * batch_size]\n",
    "        yield batch_data, batch_targets\n",
    "\n",
    "def quantum_encode(features):\n",
    "    encoded_params = []\n",
    "    for feature in features:\n",
    "        z, nh, aromatic, hybridization = feature.tolist()\n",
    "        ry_angle = (2 * z - 7) * np.pi / 4 if z is not None else 0\n",
    "        rz_angle = (-1)**(2 * hybridization - 1) * np.pi / 6 if hybridization is not None else 0\n",
    "        encoded_params.append([ry_angle, rz_angle])\n",
    "    return np.array(encoded_params)\n",
    "\n",
    "def default_EDU(bond_params, rzz_param, wires):\n",
    "    qml.RZ(bond_params[0], wires=wires[0])\n",
    "    qml.RX(bond_params[1], wires=wires[0])\n",
    "    qml.RZ(bond_params[0], wires=wires[1])\n",
    "    qml.RX(bond_params[1], wires=wires[1])\n",
    "    qml.CNOT(wires=wires)\n",
    "    qml.RZ(rzz_param, wires=wires[1])\n",
    "    qml.CNOT(wires=wires)\n",
    "    qml.RX(bond_params[0], wires=wires[0])\n",
    "    qml.RZ(bond_params[1], wires=wires[0])\n",
    "    qml.RX(bond_params[0], wires=wires[1])\n",
    "    qml.RZ(bond_params[1], wires=wires[1])\n",
    "\n",
    "def quantum_circuit_single(graph, layer_params):\n",
    "    num_layers = 3\n",
    "    num_atoms = graph.num_nodes\n",
    "    dev = qml.device('default.qubit', wires=num_atoms)\n",
    "\n",
    "    @qml.qnode(dev, interface='torch')\n",
    "    def circuit():\n",
    "        encoded_features = quantum_encode(graph.x)\n",
    "        for i in range(num_atoms):\n",
    "            qml.RY(encoded_features[i][0], wires=i)\n",
    "            qml.RZ(encoded_features[i][1], wires=i)\n",
    "\n",
    "        for layer in range(num_layers):\n",
    "            rzz_param = layer_params['rzz_params'][layer]\n",
    "            for i in range(num_atoms):\n",
    "                qml.U3(layer_params['theta'][layer], layer_params['phi'][layer], layer_params['omega'][layer], wires=i)\n",
    "            bond_order = ['single', 'aromatic', 'double', 'triple']\n",
    "            sorted_bonds = sorted(graph.bond_info, key=lambda x: bond_order.index(x[2]))\n",
    "            for start, end, bond_type in sorted_bonds:\n",
    "                bond_edu_params = layer_params['edu'][bond_type]\n",
    "                default_EDU(bond_edu_params, rzz_param, [start, end])\n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(num_atoms)]\n",
    "\n",
    "    qml_results = circuit()\n",
    "    return torch.stack(qml_results).float()\n",
    "\n",
    "def quantum_circuit_batch(batch_data, params):\n",
    "    results = []\n",
    "    max_atoms = max(graph.num_nodes for graph in batch_data)  \n",
    "    max_features = 1 \n",
    "\n",
    "    for graph in batch_data:\n",
    "        result = quantum_circuit_single(graph, params)\n",
    "        if result.size(0) < max_atoms:\n",
    "            result = F.pad(result, (0, 0, 0, max_atoms - result.size(0)), \"constant\", 0)  \n",
    "        if result.size(1) < max_features:\n",
    "            result = F.pad(result, (0, max_features - result.size(1), 0, 0), \"constant\", 0)  \n",
    "        results.append(result)\n",
    "\n",
    "    # Stack results into a single tensor for the batch\n",
    "    return torch.stack(results, dim=0)\n",
    "\n",
    "def initialize_params(layers):\n",
    "    params = {\n",
    "        'theta': [torch.nn.Parameter(torch.rand(1), requires_grad=True) for _ in range(layers)],\n",
    "        'phi': [torch.nn.Parameter(torch.rand(1), requires_grad=True) for _ in range(layers)],\n",
    "        'omega': [torch.nn.Parameter(torch.rand(1), requires_grad=True) for _ in range(layers)],\n",
    "        'rzz_params': [torch.nn.Parameter(torch.rand(1), requires_grad=True) for _ in range(layers)],\n",
    "        'edu': {\n",
    "            'single': torch.nn.Parameter(torch.rand(2), requires_grad=True),\n",
    "            'aromatic': torch.nn.Parameter(torch.rand(2), requires_grad=True),\n",
    "            'double': torch.nn.Parameter(torch.rand(2), requires_grad=True),\n",
    "            'triple': torch.nn.Parameter(torch.rand(2), requires_grad=True)\n",
    "        }\n",
    "    }\n",
    "    return params\n",
    "\n",
    "def train_and_plot(train_data, train_targets, val_data, val_targets, batch_size, total_epochs=10, validate_every=1):\n",
    "    layers = 3\n",
    "    params = initialize_params(layers)\n",
    "\n",
    "    all_params = []\n",
    "    for key in params:\n",
    "        if isinstance(params[key], dict):\n",
    "            for subkey in params[key]:\n",
    "                all_params.append(params[key][subkey])\n",
    "        else:\n",
    "            all_params.extend(params[key])\n",
    "\n",
    "    r0 = torch.nn.Parameter(torch.tensor(0.1, requires_grad=True))\n",
    "    r1 = torch.nn.Parameter(torch.tensor(0.2, requires_grad=True))\n",
    "    all_params.extend([r0, r1])\n",
    "\n",
    "    optimizer = torch.optim.Adam(all_params, lr=0.01)\n",
    "    scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(total_epochs):\n",
    "        total_train_loss = 0\n",
    "        train_batches = list(batch_processing(train_data, train_targets, batch_size))\n",
    "        for batch_data, batch_targets in train_batches:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = quantum_circuit_batch(batch_data, params)\n",
    "            adjusted_predictions = r0 + r1 * torch.sum(predictions, dim=1) / len(predictions[0])\n",
    "            loss = loss_func(adjusted_predictions, batch_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        scheduler.step()  # Update the learning rate\n",
    "        epoch_train_loss = total_train_loss / len(train_batches)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {epoch_train_loss}\")\n",
    "\n",
    "        if (epoch + 1) % validate_every == 0:\n",
    "            total_val_loss = 0\n",
    "            val_batches = list(batch_processing(val_data, val_targets, batch_size))\n",
    "            for batch_data, batch_targets in val_batches:\n",
    "                predictions = quantum_circuit_batch(batch_data, params)\n",
    "                adjusted_predictions = r0 + r1 * torch.sum(predictions, dim=1) / len(predictions[0])\n",
    "                loss = loss_func(adjusted_predictions, batch_targets)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "            epoch_val_loss = total_val_loss / len(val_batches)\n",
    "            val_losses.append(epoch_val_loss)\n",
    "            print(f\"Validation Loss after Epoch {epoch+1}: {epoch_val_loss}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "    plt.plot(range(validate_every, len(val_losses) * validate_every + 1, validate_every), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def print_num_atoms_in_data(data):\n",
    "    num_atoms_list = [graph.num_nodes for graph in data]\n",
    "    print(f\"Number of atoms in each graph in the data: {num_atoms_list}\")\n",
    "\n",
    "\n",
    "graph_data, targets = smiles_to_graph_and_targets(dataset_smiles, dataset)\n",
    "filtered_graphs, filtered_targets = filter_graphs_by_num_atoms(graph_data, targets, num_atoms=[6, 7])\n",
    "assert len(filtered_graphs) >= 3000, \"Not enough samples with 6 atoms\"\n",
    "selected_graphs = filtered_graphs[:3000]\n",
    "selected_targets = filtered_targets[:3000]\n",
    "(train_data, train_targets), (val_data, val_targets) = split_data(selected_graphs, selected_targets, train_ratio=0.7)\n",
    "\n",
    "\n",
    "print_num_atoms_in_data(train_data)\n",
    "train_and_plot(train_data, train_targets, val_data, val_targets, batch_size=30, total_epochs=150, validate_every=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of graphs with 6 atoms: 618\n",
      "Total number of graphs with 7 atoms: 3197\n"
     ]
    }
   ],
   "source": [
    "# Filter graphs by the number of atoms (6 and 7)\n",
    "filtered_graphs_for_6, _ = filter_graphs_by_num_atoms(graph_data, targets, num_atoms=[6])\n",
    "filtered_graphs_for_7, _ = filter_graphs_by_num_atoms(graph_data, targets, num_atoms=[7])\n",
    "\n",
    "# Print the total number of graphs for each atom count\n",
    "print(f\"Total number of graphs with 6 atoms: {len(filtered_graphs_for_6)}\")\n",
    "print(f\"Total number of graphs with 7 atoms: {len(filtered_graphs_for_7)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
